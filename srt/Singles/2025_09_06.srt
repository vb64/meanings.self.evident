1
00:00:00,208 --> 00:00:08,908
Здравствуйте, друзья! В эфире необычный выпуск ведущих подкаста «В поисках смысла».

2
00:00:08,908 --> 00:00:15,448
Евгения Голуба, Павла Щелина и сегодня с нами приглашённый эксперт Алиса Ким.

3
00:00:16,678 --> 00:00:29,618
Мы решили записать этот выпуск, так как первый раз мы затронули тему искусственного интеллекта, а сегодня речь пойдёт о нём, примерно год назад, может быть, немного больше.

4
00:00:29,618 --> 00:00:35,858
И с тех пор многое изменилось. Я бы так сказал, что изменилось практически всё.

5
00:00:35,858 --> 00:00:43,272
И для того, чтобы осмыслить происходящее, Нам уже не хватает собственного понимания.

6
00:00:43,272 --> 00:00:45,632
Мы подозреваем себя в предвзятости.

7
00:00:45,632 --> 00:00:52,032
И для этого мы пригласили Алису, кандидата наук, эксперта по искусственному интеллекту.

8
00:00:52,032 --> 00:01:00,872
Алиса занималась академическими исследованиями в Гумбольд-Университете, точнее, в Университете Гумбольда и в Стэнфордском университете.

9
00:01:01,448 --> 00:01:07,908
Алиса разрабатывала языковые модели в AWS, Amazon.

10
00:01:07,908 --> 00:01:17,148
И, в общем, последние 10 лет Алиса занимается внедрением искусственного интеллекта в разного рода системах, в стартапах и крупных корпорациях.

11
00:01:17,148 --> 00:01:24,848
Поэтому никому, как ни к Алисе, нам прийти с нашими вопросами и недоумениями.

12
00:01:24,848 --> 00:01:26,988
Ну что ж, начнем потихонечку.

13
00:01:27,081 --> 00:01:35,521
Итак, я предложил сегодня разделить роли в нашем встрече следующим образом.

14
00:01:35,521 --> 00:01:39,841
Как уже понятно, Алиса — наш эксперт, Павел — философ.

15
00:01:39,841 --> 00:01:52,988
А я буду выступать сегодня в роли обычного пользователя разного рода и помощников, обычного обывателя, которых много и у которых есть простые незамысловатые вопросы.

16
00:01:52,988 --> 00:01:58,416
И начну я с такого простого вопроса к Алисе.

17
00:01:59,785 --> 00:02:08,805
Сегодня уже очень многие, включая меня, не мыслят в своей жизни без помощников, как мы их называем.

18
00:02:08,805 --> 00:02:14,685
У меня при запуске браузера запускается 6 штук сразу.

19
00:02:14,685 --> 00:02:24,765
И многие сейчас уже, собственно, ищут даже продукты через искусственный интеллект, через разного рода помощников.

20
00:02:25,148 --> 00:02:36,748
Так вот вопрос следующего рода. Всё-таки это полезная штука или за этим кроется что-то ещё?

21
00:02:36,748 --> 00:02:49,748
Как Вы считаете, насколько нагружены сегодняшние помощники скрытыми какими-то функциями, скрытыми намерениями их создателей?

22
00:02:49,748 --> 00:02:52,788
Есть ли подвох в этой технологии?

23
00:02:55,843 --> 00:03:00,803
Наверное, начну с занудного «полезно для кого и для чего».

24
00:03:00,803 --> 00:03:09,163
Для нас, как для пользователей, в наших жизненных целях, безусловно, на мой взгляд, полезная вещь.

25
00:03:09,163 --> 00:03:26,564
Но мы с вами находимся в такой сложной ситуации, когда абсолютно всё в этом продукте По сути, оптимизированы для того, чтобы мы чуть-чуть сбились с курса, забыли о том, зачем мы делаем то, что мы делаем, и как это работает.

26
00:03:26,564 --> 00:03:35,300
Если бы я должна была ответить на вопрос классический, Хорошая вещь или плохая, я бы сказала.

27
00:03:35,300 --> 00:03:39,020
Хорошая, если мы будем пользоваться ей осознанно.

28
00:03:39,020 --> 00:03:47,240
И на этой части обычно все юзеры, я в том числе, говорят, если осознанно, то это уже слишком сложно.

29
00:03:49,185 --> 00:03:56,445
Что касается нагруженности, тут, на мой взгляд, всё банально и немножко грустно.

30
00:03:56,445 --> 00:04:05,185
Практически всё о том, как создаются эти помощники, зачем, с какими ограничениями.

31
00:04:05,185 --> 00:04:06,985
Всё прописано, всё открыто.

32
00:04:06,985 --> 00:04:14,684
Я думаю, что там… Скрытых смыслов и скрытых идей и тайных помыслов довольно мало.

33
00:04:14,684 --> 00:04:24,224
Другое дело, что мы как… Вы знаете, как никто не считает правила пользования, все всегда всё принимают и радостно этим дальше пользуются.

34
00:04:24,224 --> 00:05:00,465
Другое дело, что мы с вами, наверное, никогда ещё не сталкивались с инструментом, по сути, телевизорами, чайниками, любыми инструментами, доступными для рядового пользователя, которые были бы настолько сложны и настолько обманчивы, настолько сделаны для того, чтобы максимально с нашей головушкой помериться силами, запутать и вести в искусство.

35
00:05:02,845 --> 00:05:18,405
Изначально же все эти системы, когда они стали популярны, никогда не стали точными, никогда не стали какими-то правильными с точки зрения даваемых ответов.

36
00:05:18,405 --> 00:05:34,031
А тогда, когда они стали настолько похожи на настоящего собеседника, что мы прямо вот… Мы так-то любим всё антропоморфизировать, да?

37
00:05:34,031 --> 00:05:37,891
С компьютерами разговариваем, с телевизорами даём имена машинам и так далее.

38
00:05:37,891 --> 00:05:44,571
А тут оно ещё и разговаривает, и отвечает, ещё и учится тому, что мы любим и не любим, и запоминает нас.

39
00:05:44,571 --> 00:05:51,417
Ну, то есть тут не… Ну, начать с этим взаимодействовать нас и помощникам не как это же как-то… инструментом практически нереально.

40
00:05:51,417 --> 00:06:03,017
Даже если вы супер-подсознанки, скорее всего, через N и N пользований вы уже немножко забудете про то, зачем это было сделано, как это работает и так далее.

41
00:06:04,675 --> 00:06:07,635
На мой взгляд, пользы от этого можно извлечь очень много.

42
00:06:07,635 --> 00:06:19,395
Другое дело, что с такими инструментами прямо вот совсем-совсем нельзя забывать, зачем я это делаю и какие у этого ограничения.

43
00:06:19,395 --> 00:06:21,595
Они все прописаны, они все понятны.

44
00:06:21,595 --> 00:06:31,479
Но кроме тех, которые не прописаны, по идее, должны быть понятны любому человеку, Это коммерческий инструмент.

45
00:06:31,479 --> 00:06:40,799
Наверное, там есть какая-то доля того, что на вас хотят заработать, и ваше всеобщее благо мира, наверное, там не единственная цель.

46
00:06:40,799 --> 00:06:44,459
То есть, скорее всего, этот компонент тоже есть, но мы про это не очень любим помнить.

47
00:06:44,459 --> 00:06:52,879
А это влияет на то, как дальше развиваются эти системы. Вот такой мой будет ответ. То есть.

48
00:06:52,879 --> 00:07:00,228
Штука, как любая технология, всегда имеет Скажу иначе.

49
00:07:00,228 --> 00:07:08,388
Искусственный интеллект, как любой инструмент, может быть использован по прямому назначению и во благо.

50
00:07:08,388 --> 00:07:14,868
Или как кухонный нож можно резать мясо, можно зарезать человека.

51
00:07:14,868 --> 00:07:23,268
Как технология книгопечатания можно печатать Библию, а можно порно-рисуночки издавать.

52
00:07:23,268 --> 00:07:26,388
И здесь уже вопрос у меня к Павлу.

53
00:07:26,485 --> 00:07:52,188
Мы говорили с тобой год, наверное, и много больше назад о том, что движется, кажется, такое время, надвигаются времена, когда многое из того, что делает человека человеком, будет, скорее всего, передано на аутсорс, искусственному интеллекту.

54
00:07:52,188 --> 00:08:12,888
И мы тогда с тобой видели риски того, что вот это творческое начало, чувства, эмоции начнут автоматизироваться, и, соответственно, в этой части есть риски для людей потерять свой компонент человечности, что ли.

55
00:08:12,888 --> 00:08:14,228
Что ты думаешь по этому поводу?

56
00:08:17,008 --> 00:08:24,088
Ещё раз всем здравствуйте. Ну, думаю много чего. Тут даже не знаешь с какого конца подбираться.

57
00:08:24,088 --> 00:08:32,208
Первое, всё-таки сделаю базовые комментарии для фиксации собственной позиции по твоему преамбуле.

58
00:08:32,208 --> 00:08:35,548
Про вот кухонный нож и прочую всю вот эту историю.

59
00:08:35,622 --> 00:08:40,462
Дело в том, что вот с этой позиции я философски не согласен.

60
00:08:40,462 --> 00:08:48,942
Мне представляется, что само представление существования такого феномена, как нейтральная технология, является глубоким заблуждением.

61
00:08:48,942 --> 00:08:53,362
Не существует такого феномена, как нейтральность технологий.

62
00:08:53,362 --> 00:09:00,402
С самим фактом своего существования технология не нейтральна. Она создает ассиметрию.

63
00:09:00,402 --> 00:09:06,941
Банально между теми, кто технологией пользоваться умеет и кто технологией пользоваться не умеют.

64
00:09:06,941 --> 00:09:18,021
Те, кто технологией пользоваться умеют, получают дополнительные ресурсы, власть, способ взаимодействия с миром относительно тех, кто ей не пользуется.

65
00:09:18,265 --> 00:09:25,685
Эту асимметрию ты никаким образом... Вот это и есть собственно сама технологическая асимметрия.

66
00:09:25,685 --> 00:09:32,945
То, что ты говоришь уже про волевой, этический выбор субъекта, использующий технологию, это следующий этап, это следующий уровень.

67
00:09:32,945 --> 00:09:39,805
Но сначала есть вот этот базовый уровень, что самим фактом своего бытия технология мир меняет.

68
00:09:39,814 --> 00:09:51,474
И вот наше представление о ней как о некой нейтральности — это очень хороший sales point для любого, скажем так, маркетолога, но с философской точки зрения просто он неадекватный.

69
00:09:51,474 --> 00:10:02,234
Ты знаешь, мне кажется, нужно объяснить, что такое твой тезис о том, что самим фактом существования технология меняет мир.

70
00:10:02,234 --> 00:10:07,623
Логическая связь здесь не очевидна, по крайней мере, для меня.

71
00:10:07,623 --> 00:10:09,432
Ну, давай самое простое.

72
00:10:09,432 --> 00:10:19,292
Вот если пока этой технологии не было, у тебя была определенная культура и определенные, если не условно, паттерны, уж простите, не знаю, как там вечно...

73
00:10:19,292 --> 00:10:22,692
Независимость как-то. Закономерности некоторые.

74
00:10:22,692 --> 00:10:31,612
Закономерности отношений между людьми, отношений экономические, социальные, политические и так далее, как ты ни крути.

75
00:10:31,857 --> 00:10:38,157
Вот когда технология появилась, она стала фактором всех этих отношений просто по факту своего появления.

76
00:10:38,157 --> 00:10:41,577
Еще никакой воли нет, но она создала дополнительные возможности.

77
00:10:41,577 --> 00:10:46,858
Повторюсь, главная эта возможность, я ее назвал, это возможность к власти.

78
00:10:46,858 --> 00:10:52,038
Любая технология содержит в себе заряд к власти. Ну, это логично.

79
00:10:52,038 --> 00:10:55,718
Если бы она его не содержала, ее бы никто не создал. Это вот очень важно понимать.

80
00:10:55,718 --> 00:11:04,358
То, что в нашем культуре технология всегда создается, на самом деле, с неким зарядом к власти.

81
00:11:04,358 --> 00:11:04,838
Тот, кто...

82
00:11:04,838 --> 00:11:14,898
Банально, власть работы в нашем конкретном примере, власть манипулирования, власть работы с данными, власть производства дополнительного материального ресурса и так далее.

83
00:11:14,898 --> 00:11:16,632
Это все властные отношения.

84
00:11:16,632 --> 00:11:24,806
И вот эта технология, ты можешь сказать, если тебе уточнить, ну, до поры до времени, можно сказать, что она создается с технологией, как минимум, потенциальной власти.

85
00:11:24,806 --> 00:11:32,846
То есть, она лежит и спит, да, требуется волевой субъект, чтобы эту власть активизировать, но, тем не менее, потенциал-то уже создан самим фактом ее появления.

86
00:11:32,846 --> 00:11:35,766
То есть, некий статус-кво оказалось нарушенным. Всё.

87
00:11:35,766 --> 00:11:48,606
То есть изобретение автомобиля, вне зависимости от намерений конкретного изводчика, конкретного водителя или производства автомобиля, самим фактом своей технологии является угрозой, условно говоря, для коневодов.

88
00:11:48,606 --> 00:11:54,558
Она меняет эти отношения автоматически, просто по факту своего появления.

89
00:11:54,558 --> 00:12:03,371
Технология может существовать, продукта может не быть. Ведь сам факт возможности что-то сделать...

90
00:12:03,371 --> 00:12:03,381
Я.

91
00:12:03,381 --> 00:12:07,591
Поэтому так и говорю про потенциальное. Давай так, ты прав. Как минимум потенциальное.

92
00:12:07,591 --> 00:12:11,291
С философской точки зрения я веду категорию потенциальное изменение.

93
00:12:11,291 --> 00:12:19,151
Но просто мы живем еще в цивилизации последние 400 лет, где любое потенциальное изменение в зоне технологии и прогресса должно быть актуализировано.

94
00:12:19,151 --> 00:12:23,451
У нас нет никаких этических ограничений на любую технологическую актуализацию.

95
00:12:23,670 --> 00:12:26,510
Это собственно ради этого модерн мы и создавали.

96
00:12:26,510 --> 00:12:29,661
Мы говорим о том, что в модерне...

97
00:12:29,661 --> 00:12:40,501
Каждая технология, прежде всего, рассматривается с точки зрения возможности увеличить властный потенциал субъекта, обладающего этой технологией.

98
00:12:40,501 --> 00:12:43,041
Субъекта, обладающего этой технологией, да.

99
00:12:43,041 --> 00:12:51,601
А в любой технологии всегда, и до модерна, и после модерна, и вне модерна, ну, всегда содержится увеличение потенциала субъекта.

100
00:12:51,601 --> 00:12:52,441
Ну, это понятно.

101
00:12:52,441 --> 00:12:54,761
Усиление, да? То есть мы говорим, что... Усиление.

102
00:12:54,761 --> 00:12:56,725
Ну, я не могу бегать, как гепард.

103
00:12:56,725 --> 00:13:01,290
Но с машиной я могу перемещаться со скоростью, которой гепарду и не снилось.

104
00:13:01,290 --> 00:13:09,150
Мой потенциал в категории беганье, она усиливает. Она поэтому и создана.

105
00:13:09,150 --> 00:13:14,450
Теперь, я тебя прервал, может быть, ты сделаешь шаг назад и вернешься к своему второму тезису?

106
00:13:14,450 --> 00:13:17,710
Да. Приведу пример не с искусственным интеллектом, но близкий.

107
00:13:17,710 --> 00:13:23,990
У нас есть технологии социальных сетей, выпущенные относительно недавно, буквально 15 лет назад.

108
00:13:24,572 --> 00:13:35,932
Сейчас начали выходить исследования Айн Нейра, в общем, об изменениях на материальном уровне мозга молодых, особенно детей, подростков, девочек, которые 10 лет выросли на этих технологиях.

109
00:13:36,840 --> 00:13:43,520
Скажем так, исследования, мягко говоря, тревожащие. То есть там много разных неприятных последствий.

110
00:13:43,520 --> 00:13:46,880
Но я сейчас говорю не про это. Я говорю про то, что вот у нас есть асимметрия.

111
00:13:46,880 --> 00:13:51,680
Технология выпущена была 10 лет назад, последствия от нее пришли 15 лет.

112
00:13:51,680 --> 00:14:01,460
И что-то я сомневаюсь, что 15 лет назад, когда люди выпускали социальные дети, вообще хоть на каком-то этапе выпуска этой технологии задумывались о последствиях через 15 лет.

113
00:14:01,460 --> 00:14:03,540
Это ее структурное ограничение.

114
00:14:03,739 --> 00:14:12,199
То есть, по крайней мере, в нашей культуре, где скорость, повторюсь, не повторюсь, скажу новый тезис, что скорость является благом сама по себе.

115
00:14:12,199 --> 00:14:19,559
В принципе, идея торможения, движения, особенно технологического, является ересью и харамом.

116
00:14:19,559 --> 00:14:23,859
То есть, смотри, проблема технологической асимметрии последствий существовала всегда.

117
00:14:23,859 --> 00:14:28,615
Собственно, мы это знаем со времен ящика Пандора. Миф ящика Пандоры ровно про это.

118
00:14:28,615 --> 00:14:32,595
Принесли огонь, а потом выпушил, как это получился ящик.

119
00:14:32,595 --> 00:14:35,955
Это вот очень классическая взаимосвязанная история.

120
00:14:35,955 --> 00:14:45,935
Но сегодня мы просто повысили масштабы, скажем так, этой проблемы до определенного уровня, который в каком-то смысле количественно действительно является беспрецедентом.

121
00:14:46,197 --> 00:14:58,617
Имеется в виду вопрос, насколько обсуждается и насколько озабочены компании, разрабатывающие большие языковые модели, насколько озабочены последствиями.

122
00:14:58,617 --> 00:15:14,537
Я не возьмусь говорить, конечно, за всех гигантов индустрии, но в целом я думаю, что можно сделать такую усреднённую позицию, сформулировать.

123
00:15:15,019 --> 00:15:35,021
есть философская, идеалистическая воля их основателей, CEO, ведущих учёных, которые Все как один пишут и, скорее всего, действительно думают о том, что они бы очень хотели сделать мир лучше.

124
00:15:35,021 --> 00:15:44,301
То есть их позиции почти всегда такие очень публичные. Я думаю, что они действительно в это верят.

125
00:15:44,301 --> 00:15:47,661
Есть то, как это работает внутри.

126
00:15:47,661 --> 00:16:12,428
В целом почти все негативные последствия, которые… вызывают эти продукты, они практически всегда важны только когда они коротковременные и влияют на два самых важных фактора, которые, в свою очередь, влияют на то, довольны инвесторами или нет.

127
00:16:12,428 --> 00:16:26,548
К сожалению, тот единственный драйвер, который важен, в силу, кстати, того, что вы сейчас сказали о скорости, потому что сейчас у абсолютно всех участников рынка 100% уверенность в том, что вот сейчас мы в этой точке бифоркации.

128
00:16:26,548 --> 00:16:32,688
Тот, кто успеет и возьмёт рынок, тот будет править следующие 100 лет. Что не любят инвесторы?

129
00:16:32,688 --> 00:16:39,308
Инвесторы не любят, когда по рукам дают регуляторы, и инвесторы не любят, когда сильно жалуются и отпадает пользователь.

130
00:16:39,951 --> 00:16:58,431
Вот если кто-то из них усмотрел непосредственный вред в каком-то видео, сумел это довести до точки, когда реально уже наступает прессинг на компанию внести какие-то изменения, то тут компания может официально как-то позицию заявить.

131
00:16:58,431 --> 00:17:05,071
То есть, например, сейчас у всех компаний прописаны их, условно говоря, ценности и ориентиры.

132
00:17:05,331 --> 00:17:14,191
Например, OpenAI — это вот мы хотим, чтобы мы были helpful, но no harm и maximize utility — вот это их такая общая тема.

133
00:17:14,191 --> 00:17:23,531
И это влияет на то, что они реально внутри пытаются делать для того, чтобы как-то ограничить негативный, например, вот этот вот no harm, обеспечить.

134
00:17:23,531 --> 00:17:26,831
Но в целом есть, к сожалению, такая...

135
00:17:28,230 --> 00:17:34,910
неприятная история, такой конфликт интересов, что это, знаете, как для принципиального человека выломать на что угодно гораздо сложнее.

136
00:17:34,910 --> 00:17:51,150
Вот модель, у которой слишком много ограничений, она, скорее всего, будет не так хорошо, красиво работать, вот, ее тренировать дороже, ей могут быть недовольные пользователи, и поэтому в целом мотивации реально усложнять эту историю у компаний нету никакой.

137
00:17:51,282 --> 00:18:09,692
Я же постулировал вопрос более радикально, потому что, как в примере с этих социальных сетей, я ни за что не буду утверждать, что у людей, которые вводили социальные сети как корпоративный метод в середине двухтысячных, было намерение сломать психику девочкам-подросткам в 2025 году.

138
00:18:09,692 --> 00:18:18,712
Основная проблема в том, что есть огромная сфера того, что мы не знаем о технологическом последствии.

139
00:18:18,712 --> 00:18:33,932
Мы в теории могли бы попытаться об этом думать, как категория философского риска, промышления, но, как я понимаю, из вашего описания, разумеется, не по причине некого зла, а по причине той системы мотивации к действию, такой вопрос в принципе никто не ставит.

140
00:18:33,932 --> 00:18:38,632
Если последствия будут через 15 лет, нас это вообще никаким образом сегодня не волнует.

141
00:18:39,679 --> 00:18:56,499
Про это пытаются думать и даже нанимают дорогостоящих исследователей и образовывают целые think-tanks внутри компаний, и они даже публикуют желательно не сильно, конечно, радикальные работы, но показать социальную ответственность очень надо.

142
00:18:56,499 --> 00:19:01,739
Но нет времени и денег у компаний сейчас об этом думать.

143
00:19:01,739 --> 00:19:07,179
Разве что какие-то более независимые институты могут пытаться делать какие-то проекты.

144
00:19:07,179 --> 00:19:17,032
Они их делают. нет времени, возможности, слишком велика конкуренция, слишком велик прессинг.

145
00:19:17,032 --> 00:19:22,412
Павел хотел отдельно прокомментировать то, что вы сказали касательно нейтральности технологии.

146
00:19:22,412 --> 00:19:29,732
Мы здесь, безусловно, не имеем дела с технологией, которая даже подаётся как нейтральная.

147
00:19:29,732 --> 00:19:51,750
То есть, во-первых, большинство этих решений подаются с очень громким, информационным бэкграундом того, что мы это делаем ради того, чтобы человечество тут лучше жило, чтобы вам, дорогие пользователи, дать свободу, то есть «freedom to the users» — он прямо это обещает.

148
00:19:51,750 --> 00:20:01,810
Потом уже появляются «safety» и так далее, но это в целом подается очень агрессивно, как это прямо то, что сейчас вам всем сделает лучше.

149
00:20:02,215 --> 00:20:23,435
И для того, чтобы этой технологией пользоваться действительно максимально осознанно, как-то максимально возможно и безопасно, то тоже нужно найти третий ход слева за трактором, повернуть направо и желательно отключить вот этот вот еще функционал, вот эту информацию не давать, а вот здесь еще перезагрузиться.

150
00:20:23,435 --> 00:20:28,115
И тогда в целом, наверное, будет чуть получше.

151
00:20:28,115 --> 00:20:30,021
Нет бенефита, нет позитива.

152
00:20:30,021 --> 00:20:52,568
Уровень сложности, количество сальто, которое нужно сделать, чтобы действительно эта технология для вас, когда пользователя, была нейтральной, бесчестна велика для того, чтобы утверждать, что это вот «да нет, мы же вам всё по-честному дарили, это всё вы, это ваше пользование дало вам плохие результаты, это не мы».

153
00:20:52,568 --> 00:20:55,248
Тут нужно просто это по-честному отметить.

154
00:20:57,328 --> 00:20:59,788
Ну, у меня будет два комментария. Первое.

155
00:20:59,788 --> 00:21:10,348
Так как я постоянно рассказываю о том, что человек выходит из корпоративного мира, то я знаю цену всем вот этим корпоративным миссиям, виденью и всему остальному.

156
00:21:10,348 --> 00:21:18,248
Цена эта не очень высока. Это всё, в общем, известное лицемерие.

157
00:21:18,248 --> 00:21:27,407
И во главе угла всегда стоят только деньги. деньги и власть.

158
00:21:27,407 --> 00:21:49,567
Поэтому если на пути у топ-менеджмента становятся какие-то не вполне, скажем так, очевидные или сомнительные свойства продукта, то топ-менеджмент всегда, повторяю, всегда, прежде всего, попытается добиться максимального финансового результата.

159
00:21:49,995 --> 00:22:03,815
Конечно же, с одной стороны, снижая риски для себя, и главным образом, как бы кто об этом ничего не узнал, или как бы чего не вышло, а уже потом, как бы ничего не вышло с точки зрения пиара, а уже потом будет думать о всех этих миссиях и видениях.

160
00:22:03,815 --> 00:22:11,335
Миссии и видения нужны для того, чтобы себе красиво выступать на конференциях и сорвать аплодисменты.

161
00:22:11,848 --> 00:22:13,088
Это первый комментарий.

162
00:22:13,088 --> 00:22:25,568
Поэтому наличие в «Антропик» миссии видения для меня совершенно не успокаивает, а даже скорее наоборот говорит о том, что если такая миссия видения, значит точно там где-то что-то не так.

163
00:22:25,568 --> 00:22:33,738
Это первое. А второе — это наблюдение за нашими лидерами мнений.

164
00:22:33,738 --> 00:22:51,168
вот этими замечательными гениями технологическими, вроде Сэма Альтмана, который, как мы уже говорили с Павлом, в своём послании «Городу и миру» заявил о том, что мы в двух шагах от райских кущ, которые нам произведёт искусственный интеллект.

165
00:22:51,168 --> 00:23:11,357
И при этом он перечислял какие-то такие, скажем так, свойства искусственного интеллекта и привёл такие доводы, которое можно, наверное, оглянувшись назад, было бы услышать от изобретателей, не знаю, электрических двигателей, стиральных машин, паровозов и так далее, и так далее.

166
00:23:11,357 --> 00:23:16,097
То есть кажется, что, дружище, ну как бы, что ж ты повторяешь-то всё одно и то же?

167
00:23:16,097 --> 00:23:22,004
Ну как изменилась жизнь обывателя к лучшему? за счет технологии. Она стала комфортнее, да. И что?

168
00:23:22,004 --> 00:23:23,684
И к чему это привело?

169
00:23:23,684 --> 00:23:29,024
А теперь твоя технология отнимает у него последний шанс к творчеству, как мне кажется.

170
00:23:29,024 --> 00:23:34,575
Алиса, ваши комментарии как человека близко к корпоративному миру, ну и, конечно же, Павла хотелось бы.

171
00:23:34,575 --> 00:23:41,565
Я здесь, наверное, скажу. А теперь пару слов в апологию всей этой истории.

172
00:23:41,565 --> 00:23:43,485
Да, да, хорошо.

173
00:23:43,485 --> 00:24:06,751
Нужно просто сказать, что даже если внезапно самые альфаны Марки Цукерберге этого мира решат, всё, давайте, ребята, забудем про деньги, будем заниматься, прям действительно постараемся, наши ЛЛМы, наши Клоды, наши чаты ЧПТ, они прям были, да, там сели, разумные, добрые, вечные и так далее.

174
00:24:06,751 --> 00:24:09,731
Вот прямо вот сейчас, эх, мы возьмёмся.

175
00:24:09,731 --> 00:24:29,548
Я здесь, конечно, не скажу за прямо вот совсем, да, вот этот bleeding edge того, что существует, но в целом, насколько мне известно, даже если мы очень сильно захотим, наши границы возможного для того, чтобы действительно заставить моделей.

176
00:24:29,548 --> 00:24:37,348
Если только мы их не превратим вот в попок, которые вот если тебя спросили, я-то отвечаю «это», мы просто не можем.

177
00:24:38,273 --> 00:25:00,713
точно быть уверены в том, что модели будут действовать так, как нам надо, что они будут обладать теми… точнее, не обладать, а демонстрировать то поведение, те ценности, которые мы в них хотели заложить, исходя из своего представления о том, как это должно быть, и исходя из фидбэка юзеров, чтобы им тоже не навредить.

178
00:25:02,455 --> 00:25:04,635
Мы здесь просто ещё технически ограничены.

179
00:25:04,635 --> 00:25:27,275
Несмотря на то, что ограничения, разные степени контроля, направления этих моделей, они закладываются на целом ряде разных шагов, которые происходят во время тренировки, подготовки, тюнинга этих моделей, пользования уже этими моделями.

180
00:25:27,275 --> 00:25:32,295
Мы на всех этих шагах довольно сильно ограничены.

181
00:25:32,678 --> 00:25:37,718
Хочу здесь просто так же оговориться, что есть некий потолок того, что мы можем сделать.

182
00:25:37,718 --> 00:25:52,058
И даже если вы тысячу раз спросите модели, если тебе попросят рецепт коктейля молотого, например, или спросят самый лучший способ сделать что-нибудь нехорошее, ты же ничего не ответишь.

183
00:25:52,058 --> 00:25:56,698
Тысячу раз модель отвечает, конечно, ничего не отвечу. Тысяча первыми может ответить.

184
00:25:58,885 --> 00:26:14,445
Наша личная степень контроля, несмотря на то, что она очень велика, и, конечно же, вообще мы целиком формируем то, как эти модели какую-нибудь информацию выдают, как действуют, как принимают решения, мы всё равно довольно сильно ограничиваем.

185
00:26:14,445 --> 00:26:15,265
Нужно это сказать.

186
00:26:17,346 --> 00:26:25,066
Послушайте, ну среди обывателей, скажем так, распространено две крайние точки зрения.

187
00:26:25,066 --> 00:26:32,787
С одной стороны, да что вы там мне рассказываете про этот искусственный интеллект, он галлюцинирит, вообще это продвинутая Т7, подсказыватель букв.

188
00:26:32,787 --> 00:26:37,327
Т9, извини, да, Т9, которое просто развили.

189
00:26:37,327 --> 00:26:41,507
А с другой стороны, господи, это магия какая-то, это вообще душа.

190
00:26:41,507 --> 00:26:47,007
Наконец-то меня кто-то понял, услышал. Не рассказывайте мне все ваши сказки про Т9.

191
00:26:47,007 --> 00:26:54,707
Это просто уже новая сущность какая-то, совершенно непредставимая раньше.

192
00:26:54,707 --> 00:27:01,427
И сейчас, Алиса, то, что вы говорите, звучит как… Мы вообще не понимаем, как это работает.

193
00:27:01,427 --> 00:27:09,927
Есть… Ну как бы так в целом, как будто понимаем, но до конца не можем предугадать, что вообще, говоря, получится.

194
00:27:09,927 --> 00:27:15,427
Есть огромное ограничение в том, что там называется объяснимость сетей.

195
00:27:15,427 --> 00:27:31,427
То есть если мы спросим даже самую суперпродвинутую модель, а почему вот ты думаешь, что это плохо, мы не можем быть уверены, что она нам отвечает честно и что она дала какой-то ответ именно потому, что вот она думает так, как мы хотим, чтобы она думала.

196
00:27:32,288 --> 00:27:40,308
То есть мы всё равно здесь взаимодействуем с очень высокой степенью неуверенности того, почему оно делает то, что оно делает.

197
00:27:41,181 --> 00:27:42,121
Да.

198
00:27:42,121 --> 00:27:48,161
Вот так, да? Вы говорите, она думает. Она вообще думает?

199
00:27:48,161 --> 00:28:00,021
Там же, как я понимаю, идет какой-то огромный подбор вариантов, сочетаний тех или иных смысловых знаков и так далее.

200
00:28:00,021 --> 00:28:02,461
Можешь сказать, что она думает вообще?

201
00:28:03,793 --> 00:28:19,373
С учётом того, что первые нейронные сети и вообще перцептрон создавался как моделька вот этого нейрона в голове, я думаю, что мы склонны использовать глагол «вроде думает».

202
00:28:19,373 --> 00:28:32,561
Но нет, это последовательность неких математических действий, которые обусловлены оптимизацией, которая была проведена некими правилами, которые были вшиты в это всё во время тренировки модели.

203
00:28:32,561 --> 00:28:36,888
которые приводят к тому, что модель отвечает что-то определённое.

204
00:28:36,888 --> 00:28:49,108
Но там этих вот ходов, которые не нами прописаны, а которые возникли сами в ходе тренировки, подготовки этой модели, их гораздо больше.

205
00:28:49,108 --> 00:28:57,708
То есть тут немножечко получается как такое… И опять же, я сейчас буду использовать человеческие слова «сознание под сознание».

206
00:28:57,708 --> 00:29:01,448
То есть какую-то часть мы контролируем и видим, а какую-то часть мы всё ещё не видим.

207
00:29:05,438 --> 00:29:09,278
Ну да. Звучит все-таки немного жутковато. Павел?

208
00:29:09,278 --> 00:29:19,258
Мне кажется, возвращаясь к первому вопросу, звучит жутковато, но оптимистично. Объясню почему.

209
00:29:19,258 --> 00:29:21,078
Повод задуматься.

210
00:29:21,078 --> 00:29:30,791
Мне нравится, что на фоне всей этой искусственно-интеллектной истории актуализируется постепенный интерес к по-настоящему важным вопросам эсхатологическим.

211
00:29:30,791 --> 00:29:38,384
и антологическим. Мне тоже уже 20 человек прислали ссылки на лекцию Питера Тиля.

212
00:29:38,384 --> 00:29:44,464
То есть, это тоже поразительное следствие об антихристе. Поразительная черта нашей эпохи.

213
00:29:44,464 --> 00:29:52,664
То есть, то, что говорит батюшка Самон, условно, тысячелетиями никому неинтересно, но тут Тиль 4 лекции сделает.

214
00:29:52,664 --> 00:29:56,804
Вот ты употреблял раньше слово магия, я бы на нём, честно говоря, остановился бы.

215
00:29:58,071 --> 00:30:02,311
На каком-то уровне бытия это действительно магическая история.

216
00:30:02,311 --> 00:30:17,291
Просто если мы не ограничиваемся только материальными причинами, о чем нам весьма подробно рассказала Алиса, а подключаем к этому субъективный опыт взаимодействия пользователя с этой системой, то он действительно очень магичен в своем вот именно опыте.

217
00:30:17,291 --> 00:30:20,011
Поэтому здесь как раз особых противоречий нет.

218
00:30:20,937 --> 00:30:34,117
И перед этим самым пользователем, уверенным в себе и в своем интеллектуальном превосходстве над неорганической природой до недавнего времени, искусственный интеллект, конечно, ну, эти все модели ставят очень большой вопрос.

219
00:30:34,117 --> 00:30:39,117
А ты, собственно, человек или нет? Что в тебе, собственно, человеческого? А что ты готов...

220
00:30:39,769 --> 00:30:44,269
пожертвовать, чтобы сохранить в себе некую человеческую особенность.

221
00:30:44,269 --> 00:30:52,129
Здесь я объясню, я имею в виду сейчас даже не какие-то страшные пожертвования, а очень практические, но отсюда не менее страшные.

222
00:30:52,129 --> 00:30:53,049
Начну издалека.

223
00:30:53,049 --> 00:31:07,328
Классические примеры проблемы технологии, сформулирован По батькой нашим Платоном, еще вот в знаменитом разговоре египетского бога Тота с жрецами, который жаловался на изобретение письменности.

224
00:31:07,328 --> 00:31:10,228
Тоже к вопросу о нейтральности технологий.

225
00:31:10,228 --> 00:31:21,748
Там бог Тот, если конкретно, жаловался на то, что вы сейчас писать научитесь, и саги, и священные тексты длиной в 15 тысяч строчек, условно, запоминать перестанете.

226
00:31:21,748 --> 00:31:24,488
То есть вы потеряете способность это делать.

227
00:31:26,091 --> 00:31:29,391
И вот в каком-то смысле любая технология несет в себе эту угрозу.

228
00:31:29,391 --> 00:31:34,571
На самом деле с письменностью в широком значении произошло действительно именно это.

229
00:31:34,571 --> 00:31:39,011
Я уже молчу о том, что есть большая разница между научиться читать и научиться понимать текст.

230
00:31:39,388 --> 00:31:51,468
Это у нас отдельная проблема, то что разрыв между этими двумя, скажем так, феноменами нашей опты тоже нами очень редко осознается.

231
00:31:51,468 --> 00:31:54,348
Так вот, тем не менее, мы о себе думаем очень высокого мнения.

232
00:31:54,348 --> 00:31:58,848
И тут ИИ ставит перед нами очень такую, на мой взгляд, радикальную задачу.

233
00:31:59,267 --> 00:32:17,147
Вот как раз всё не творческое, всё построено на комбинаторике, всё построено на запоминании даже количества беспонимания, я вот уточню некоторым, то есть количественная информация и беспонимание сути этой информации, он действительно сделает лучше нас.

234
00:32:17,147 --> 00:32:21,967
И это ставит нас действительно перед радикальным зеркалом, а, собственно, повторюсь, кто-то и есть.

235
00:32:21,967 --> 00:32:31,341
Как бы могла бы выглядеть альтернативная вот практическая, скажем так, И это уже перехожу к части, где я немножко поговорю о том, что, мне кажется, можно делать.

236
00:32:31,341 --> 00:32:34,161
И мне интересно будет мне и Алису.

237
00:32:34,161 --> 00:32:56,956
Ну, мыслим ли нам по-честному сценарий, что на каждый час использования интеллекта ради рациональных, усиления твоего могущества по организации собственного дня и приобретения дополнительно свободного времени, ты будешь тратить полтора часа Человек будет тратить полтора часа на запоминание стихов, саг, псалмов, там, как идеал.

238
00:32:56,956 --> 00:32:59,296
Или прочтение бумажной книги.

239
00:32:59,296 --> 00:33:04,945
Не потому, что это более эффективно, а для того, чтобы сохранить вот эту свою способность человеческого восприятия.

240
00:33:04,945 --> 00:33:09,405
Вот это, мне кажется, есть вопрос, который сегодня стоит по-настоящему.

241
00:33:09,405 --> 00:33:17,025
Другими словами, мне кажется, взаимодействовать с определенной субъектностью ИИ, и тут у меня еще не до конца понятно.

242
00:33:17,025 --> 00:33:21,585
У меня есть пара гипотез, но я здесь их проверять не буду.

243
00:33:21,585 --> 00:33:33,345
Относительно того, насколько ИИ является субъектным, тут, скажем так, разные есть мнения и у священников, и у философов, и у пользователей.

244
00:33:33,345 --> 00:33:34,165
Ну, допустим.

245
00:33:34,223 --> 00:33:43,563
Но в любом случае, очевидно, что взаимодействовать с этой машинкой можно только, постоянно повышая субъектность собственную.

246
00:33:43,563 --> 00:33:46,043
И вот в этом-то у нас проблема.

247
00:33:46,043 --> 00:33:55,443
То, что вот эта вторая часть, наша уже культура, общество, да и само мышление о технологиях, ну вообще ни разу не поднимает.

248
00:33:55,443 --> 00:34:01,103
Мы не ставим принципиально так вопрос. И для меня именно в этом-то и заключена главная опасность.

249
00:34:01,307 --> 00:34:05,787
Да, технология... Ну, в метафоре классической это черт.

250
00:34:05,787 --> 00:34:09,667
Причем, возможно, такой нынешний черт, он прям всем чертям черт.

251
00:34:09,667 --> 00:34:13,427
Но, как говорят русские сказки, черта иногда можно попытаться оседлать.

252
00:34:13,427 --> 00:34:16,707
Ну, как акула, летающая на черте за черевичками.

253
00:34:16,707 --> 00:34:23,747
Но чтобы очертать этого черта, черта даже в сказках, ну, ты должен проявить субъектность больше, чем у этого самого черта.

254
00:34:23,747 --> 00:34:34,092
А с субъектностью у нас коллективная напряжёнка. И вот как-то так я вижу эту проблематику. Евгений?

255
00:34:34,412 --> 00:34:38,112
Задумался я о твоих словах.

256
00:34:38,112 --> 00:34:47,232
Этот образ вакуула, оседлавшего чёрта, я уже его слышал, по-моему, на одном из твоих интервью.

257
00:34:47,232 --> 00:34:53,192
Интересный образ. Но получается так, что мы фактически становимся заложниками.

258
00:34:53,192 --> 00:34:54,872
Технологию не остановить.

259
00:34:55,122 --> 00:35:06,402
Уровень, скажем так, мудрости разработчиков искусственного интеллекта, на мой взгляд, довольно невысок.

260
00:35:06,402 --> 00:35:16,042
Капитализм нас толкает, логика капиталистического, отношение капиталистическое толкает нас к тому, что нужно максимизировать прибыль.

261
00:35:16,042 --> 00:35:27,589
Да, будут говорить про всех стейкхолдеров и нужно все общее, но давайте честно. Деньги решают.

262
00:35:27,589 --> 00:35:28,989
Деньги и власть по-прежнему решают.

263
00:35:28,989 --> 00:35:31,789
Я ничего не говорил о коллективной. Давай так, уточню.

264
00:35:31,789 --> 00:35:37,469
Когда я это говорил, я это говорил прежде всего в индивидуальном порядке, но это первый момент.

265
00:35:37,469 --> 00:35:42,109
Но второй момент, здесь есть очень интересный аспект, который я все-таки тогда тоже здесь закину.

266
00:35:42,109 --> 00:35:45,889
Связано, уж извини меня напрямую, с властью и вот этой драмократией.

267
00:35:45,889 --> 00:35:50,469
Это к тому, что Алиса упоминала ранее, о страшных русских, американцах и китайцах.

268
00:35:51,130 --> 00:35:59,850
Проблема в том, что с этими искусственными интеллектами на данный момент мы имеем очень интересный парадокс уже с точки зрения такой некой теории.

269
00:35:59,850 --> 00:36:05,830
Метафорично все сейчас заняты выработкой меча, технологического меча.

270
00:36:05,830 --> 00:36:11,010
Этот меч все оттачивают до такой степени, чтобы нанести первый обезоруживающий удар.

271
00:36:11,010 --> 00:36:18,670
Ну как, это образно, да, то есть я не говорю, что прямо у всех есть такое намерение, но общее восприятие картинки идет примерно таково.

272
00:36:19,632 --> 00:36:21,752
Плюс, да, делаются словесные омажи.

273
00:36:21,752 --> 00:36:26,952
Ну, мы, наверное, говоримся об общих правилах, какая-то гарантия взаимного уничтожения. Чёртовство.

274
00:36:26,952 --> 00:36:31,912
Ничего подобного вообще не происходит. Проблема-то в чём заключается?

275
00:36:31,912 --> 00:36:37,452
То, что щит находится вообще в другой области. Вот в этом, мне кажется, большой парадокс.

276
00:36:37,452 --> 00:36:43,312
То есть обычно щит и меч должны находиться в одной топологии. Ну, условно.

277
00:36:44,286 --> 00:36:46,946
Ну да, технология и технология.

278
00:36:46,946 --> 00:36:54,306
А у нас получается меч технологичный, а щит антропологичный. И вот здесь... Да, вот это, я.

279
00:36:54,306 --> 00:36:57,786
Думаю, самое важное, что нужно понимать.

280
00:36:57,786 --> 00:36:58,846
Мы можем раскрыть.

281
00:36:59,288 --> 00:37:08,848
Нет технологии, которой можно противопоставить технологии искусственного интеллекта, потому что она затрагивает уже сущностные свойства человека как такового.

282
00:37:08,848 --> 00:37:19,128
Да, единственный способ даже защититься вам как государству, начальству и прочим, это иметь такое население, которое обладает антропологической защитой от технологического манипулирования.

283
00:37:19,128 --> 00:37:25,628
Это, собственно, и есть то, что я описывал, как повышение антропологической субъектности в ответ на повышение технологического вызова.

284
00:37:26,018 --> 00:37:32,698
Но тут тогда возникают очень неприятные последствия для всей нашей политической системы, о которых я говорю из эфира в эфир.

285
00:37:32,698 --> 00:37:36,958
Алиса в курсе, и ты тоже, в принципе. Но другого выхода нет.

286
00:37:36,958 --> 00:37:42,478
Мне кажется, в этом тоже есть определенная парадоксальная красота. Не мытьем, так катанем.

287
00:37:42,478 --> 00:37:43,438
Понимаешь?

288
00:37:45,272 --> 00:38:00,052
Я понимаю, ты говоришь о том, что в предыдущей технологии нас с вами пытались оскотинить, давая максимум комфорта и не стимулируя быть с людьми в полном смысле этого слова.

289
00:38:00,052 --> 00:38:05,445
А сейчас будет радикальный выбор. Либо ты уже совсем в предаток превратишься.

290
00:38:05,445 --> 00:38:10,578
к экранчику, либо станешь человеком. Но давай мы зададим слово нашему эксперту.

291
00:38:10,578 --> 00:38:18,638
Алис, ваши мысли по поводу слов Павла о том, что может противостоять технологии в данном случае?

292
00:38:18,638 --> 00:38:30,018
Ох, знал бы прикуп. Проинвестировали бы. Я себе позволю такое маленькое грустное отхождение.

293
00:38:30,018 --> 00:38:33,478
Очень любят во всех статьях про искусственный интеллект сейчас, конечно же, писать.

294
00:38:35,678 --> 00:38:48,078
Если, собственно говоря, почитать его общую серию о роботах, там есть такая замечательная история, которая называется «Лжать», в которой им удалось создать робота, который читает человеческие мысли.

295
00:38:48,078 --> 00:38:54,258
И так как он не может людям вредить, то он им начинает врать напропалу, чтобы их чувства не задеть.

296
00:38:54,258 --> 00:38:56,498
То есть вред же может быть эмоциональный.

297
00:38:56,498 --> 00:39:01,158
И как только они это понимают, этого робота сразу же разбирают, потому что зачем человечеству такой робот?

298
00:39:02,323 --> 00:39:23,283
А вот мы его решили не разбирать, и ровно в нашей исторической реальности мы такого робота всячески пытаемся дальше усовершенствовать, который нам рассказывает, дальше нас успокаивает и говорит, что всё будет хорошо, и максимально пытается сделать нас счастливыми, не во благо нам же.

299
00:39:23,283 --> 00:39:32,217
Что касается такой геополитической… геополитической составной части.

300
00:39:32,217 --> 00:39:37,157
Я думаю, Павел, государства дойдут до вашей мысли, если ещё не целиком дошли.

301
00:39:37,157 --> 00:39:48,248
Пока что они находятся на точке чуть подальше, хотя бы, что уже ценно, хотя бы уже идёт вопрос о ИИ-суверенитете, то есть что происходит.

302
00:39:48,248 --> 00:40:03,780
государства начали понимать, что если их граждане будут пользоваться разработками, например, американскими, то будет идти колоссальнейшая культурная манипуляция, ценностная манипуляция и так далее.

303
00:40:03,780 --> 00:40:22,665
То есть не так много, но уже появляются работы на эту тему, наверное, моё любимое, которое использует эту карту с труднопроизносимым названием Ингельхарта-Вельцеля, которая наосяг выживание против общественного блага.

304
00:40:24,049 --> 00:40:25,709
Коллективность, индивидуальность.

305
00:40:25,709 --> 00:40:26,969
Да, коллективность, индивидуальность.

306
00:40:26,969 --> 00:40:35,849
И там, в общем-то, показано, что все нынешние самые популярные сети, они так очень хорошо пластеризованы в смысле того, какие они дают ответы, как они себя ведут.

307
00:40:35,849 --> 00:40:39,449
Мы сейчас не говорим о том, что они там реально думают, о том, какие они дают ответы.

308
00:40:39,449 --> 00:40:44,609
Такие протестантско-народические... протестантско-народические кластеры.

309
00:40:44,609 --> 00:40:48,129
Кто писал, тот и воспроизводит.

310
00:40:48,129 --> 00:40:52,389
Кто платит, тот музыку заказывает. И в этом смысле, если...

311
00:40:53,310 --> 00:40:59,970
Например, гражданин Индии спросит у отчёта JPT, напиши мне историю про мальчика.

312
00:40:59,970 --> 00:41:12,970
Тут известный пример, который приготовил завтрак, но мальчик будет готовить тост и бекончик, и яичнику, а не чапати с гей со всем остальным.

313
00:41:12,970 --> 00:41:16,210
И вот этим сейчас очень сильно озабочено государство.

314
00:41:16,210 --> 00:41:38,110
Поэтому, например, такие страны, как Дания, Израиль и так далее уже несколько лет назад прямо очень сильно встрепенулись, то есть уже хотя бы это они поняли, начали быстро-быстро собирать данные, формировать команды в свои государства для формирования тех моделей, которые будут соответствовать уже каким-то их представлениям, и даже некий успех там достигнут.

315
00:41:40,054 --> 00:41:52,034
Я целиком согласна с тем, что Вы, Павел, обозначили как сложность, то есть щит в другой сфере.

316
00:41:52,074 --> 00:42:06,014
Я, честно говоря, кроме очень страшных исторических событий, не знаю примеров, когда людей реально что-то мотивировало резко перестать расслабляться, а начать собираться.

317
00:42:06,126 --> 00:42:17,606
а именно ментально от нас, по сути, требуется это для того, чтобы окончательно не потонуть в истории.

318
00:42:17,606 --> 00:42:31,752
И здесь, к сожалению, в целом у нас… нет союзников, потому что даже корпорации, которые… И это, кстати, такое тоже грустное осознание.

319
00:42:31,752 --> 00:42:40,315
Например, когда люди говорят, ну вот я же программист или дизайнер, я в своей компании, могу столько всего теперь сделать.

320
00:42:40,315 --> 00:42:55,075
На самом деле компании, и это вам скажет любой продавец САС-продуктов, компании уже давно, наверное, лет 10, как ненавидят слово «продуктивность» и под эгидой продуктивности им продать какой-либо продукт очень сложно, потому что это тяжело измерить.

321
00:42:55,075 --> 00:43:01,155
Но если только вы не на заводе, тогда можете выпустить больше лампочек и так далее.

322
00:43:01,155 --> 00:43:09,061
Компаниям интересна целиком замена людей, потому что это является ключевым вот прямо таким Качественный скачок.

323
00:43:09,061 --> 00:43:10,781
Ну, качественный скачок именно.

324
00:43:10,781 --> 00:43:11,881
Да, с начком.

325
00:43:11,881 --> 00:43:19,901
Поэтому все, что до этого, это они пытаются не отстать, но ждут, когда уже можно будет щелкнуть пальцами.

326
00:43:19,901 --> 00:43:32,101
Несмотря на те заявления, которые делают Кларны мира сего, что мы столько-то людей уже сократили, или там Амазоны, которые говорят, а мы не наймем теперь тысячу кодеров, потому что у нас есть...

327
00:43:33,808 --> 00:43:44,928
Я себе позволю такое деткое замечание, что это просто звучит лучше, что мы должны сократить количество денег на персонал, потому что то, что мы пытались, наши инвестиции не оправдались в других областях.

328
00:43:44,928 --> 00:43:52,810
Но на самом деле мечта бизнеса, она скорее движется в этом направлении очень большого бизнеса.

329
00:43:52,810 --> 00:43:58,950
Смотрите, простите, перебью, просто это очень прекрасная же иллюстрация, то, что чистая идея.

330
00:43:58,950 --> 00:44:03,470
Ну, то есть бизнес стремится к тому, чтобы оставить единственное...

331
00:44:03,470 --> 00:44:12,610
Ну, то есть мы, видимо, то есть два варианта, по крайней мере, пока у нас не будет технологического коллапса, если он будет, то есть вот с отключением электричества и прочего.

332
00:44:12,739 --> 00:44:20,459
мы приходим в точку, в которой единственная добавленная стоимость генерируется в зоне идеи, творчества того самого.

333
00:44:20,459 --> 00:44:23,879
Все материальное воплощение стремится к полной автоматизации.

334
00:44:23,879 --> 00:44:31,879
Все материю стремится отдать на аутсорс, соответственно, единственная, в принципе, добавленная стоимость может быть именно сгенерирована в самой идее.

335
00:44:31,879 --> 00:44:37,079
Идея нового приложения, идея нового продукта, идея чего-то и тому подобное.

336
00:44:37,191 --> 00:44:44,471
Но вот тут-то возникает неприятная особенность, которую мы все знаем, что вот к такому генерации новых идей мы не то чтобы сильно были научены.

337
00:44:44,471 --> 00:44:49,591
Большая часть работы от тебя вообще исторически не требовала ничего генерировать нового.

338
00:44:49,591 --> 00:44:52,711
Особенно в индустриальном обществе.

339
00:44:52,711 --> 00:44:56,331
И поэтому в этом плане корпоративная логика мне тут очень понятна.

340
00:44:56,331 --> 00:45:07,688
Я просто хотел бы еще заострить, когда я говорил о вот этой гонке вооружений, для меня все-таки важно подчеркнуть, что на данный момент Вот то, что вы описываете, оно укладывается в нормальную государственную лойку.

341
00:45:07,688 --> 00:45:11,948
Они пока ищут технологическое решение, технологические проблемы.

342
00:45:11,948 --> 00:45:19,068
Условно говоря, да, и проблема, но если мы создадим свои, если мы создадим гигантский прекрасный фаервол, который создадит...

343
00:45:19,068 --> 00:45:20,866
Это хотя бы будет наша проблема.

344
00:45:20,866 --> 00:45:27,165
Это будет наша проблема, и мы ее, так сказать, героически уже будем с ней работать, мы эту штуку оседлаем.

345
00:45:27,165 --> 00:45:28,985
А мой же тезис более радикальный.

346
00:45:28,985 --> 00:45:43,645
И вот здесь я подчеркнул оригинальность для меня ИИ в том, что в принципе нету технологического решения этой проблемы в силу антропологического веса, антропологического масштаба давления этой технологии.

347
00:45:43,645 --> 00:45:52,590
Условно говоря, неважно, какой ИИ будет программировать мозги твоему населению, если оно будет сидеть по квартирам И ты его не заставишь не сделать ничего.

348
00:45:52,590 --> 00:46:04,650
То есть, если наша цель – производить добавленную эту стойбость с этим сверхтворческим субъектом, то тогда получается, повторюсь, и решение в принципе не находится в этой зоне.

349
00:46:04,650 --> 00:46:11,530
Если мы говорим о, допустим, более прикладных вещах, у нас же возникает целый комплекс проблем, о которых мы не говорили, но прикладные.

350
00:46:11,530 --> 00:46:17,522
Когнитивная война. То есть, СИИ — это уникальный пример, чтобы свести с ума население оппонента.

351
00:46:17,522 --> 00:46:20,802
То есть, можно таргетировать. Что делать в обратную сторону, никто не понимает.

352
00:46:20,802 --> 00:46:25,242
Ну, когнитивная война — это такая моя побочная тема. Можем уйти сюда. Но тут есть...

353
00:46:25,242 --> 00:46:27,342
Все знают эту историю с телефонными мошенниками.

354
00:46:27,342 --> 00:46:36,362
Только теперь представьте, что это телефонный мошенник, который звонит реально голосом внучка, которого ты не отличишь, причем реально не отличишь.

355
00:46:36,362 --> 00:46:42,442
Это вопрос там пяти лет, когда он сможет делать такие запросы. И вот все вот эти протоколы...

356
00:46:42,555 --> 00:46:43,015
Раньше.

357
00:46:43,015 --> 00:46:54,135
Но сейчас еще там есть такие шероховатости, можно еще, там он не полностью, по крайней мере массово, может быть в ваших лабораториях уже может, то массово пока еще так себе.

358
00:46:54,135 --> 00:47:04,055
Баловались мы тут с автоматическими переводами и голосом, так себе пока еще сервис, но идет в это направление, то есть он придет к той точке, что это будет практически неотличимо.

359
00:47:04,055 --> 00:47:11,915
И тогда единственное, что тебя там может спасти, то есть вопрос доверия, вопрос протокола, то есть это решения принципиально не могут быть технологическими.

360
00:47:12,087 --> 00:47:19,007
В этом, мне кажется, то, что мы пока не осознали радикальность этого вызова. Ну, это я вот застрел.

361
00:47:19,007 --> 00:47:30,607
Ну, я здесь могу сказать, что решение, конечно, можно сделать технологически, мы можем сделать, и который будет нас пинать и говорить, так, а ты сам-то подумал, прежде чем меня спрашивать.

362
00:47:30,607 --> 00:47:33,867
И я тут... То есть, но этим никто не будет пользоваться.

363
00:47:33,867 --> 00:47:38,866
Это интересно, что вы сказали. Никто не будет пользоваться. Это очень прикладной ответ.

364
00:47:38,866 --> 00:47:57,045
Но я скорее думал, давайте прикладной, можем ли мы сделать ИИ, которая будет блокировать на фоне подхода звонок из условного Дмитровского колл-центра через 15 айпи голосом внучка бабушки под Тамбовым, чтобы она принесла там 20 миллионов, потому что беспокоит ФСБ.

365
00:47:57,045 --> 00:47:59,445
Вот такой вы можете хотя бы представить?

366
00:47:59,573 --> 00:48:06,633
Конечно. Насколько я знаю, кстати, в России уже это решается просто немножко другим методом.

367
00:48:06,633 --> 00:48:34,479
Насколько я знаю, один из банков, не будем рекламу проводить, они просто начинают разговор как секретарь с мошенниками и в ходе получения информации потихонечку начинают фильтровать То есть я думаю, что здесь же, знаете как, если вас вот кто-то лично таргетирует и вот прям замучился сделать модели и узнал детали, и прям вот всё взял, тут, конечно, ну это как со взломом датам систем.

368
00:48:34,479 --> 00:48:36,034
Если вот кто-то именно, Интересно.

369
00:48:36,034 --> 00:48:54,731
скажем так, Ответ можно написать, но это же, знаете, как с соцсетями.

370
00:48:54,731 --> 00:49:09,871
Когда начались такие действительно большие кризисы в Фейсбуке, и многие оттуда начали уходить, сделали же столько альтернатив, которые пытались сделать, сказать, что наши будут не токсичными соцсетями.

371
00:49:09,871 --> 00:49:14,511
Это был скай, и сами люди из Фейсбука пытались это всё делать. Не пользуются.

372
00:49:14,515 --> 00:49:20,575
Ну вот, не так это же. Нет, там два варианта. Там интереснее.

373
00:49:20,575 --> 00:49:26,775
Интересно, как прокомментируете, потому что по соцсетям там произошло более интересно.

374
00:49:26,775 --> 00:49:35,315
Скажем так, у нас произошло падение Фейсбука, Фейсбук реально пал, но при этом не возникло прямой альтернативы Фейсбуку.

375
00:49:35,315 --> 00:49:40,551
То есть мы увидели дальнейшую фрагментацию сетевого пространства.

376
00:49:40,551 --> 00:49:52,888
То есть, ни один из тех, что возник после Фейсбука, не стал сам аналогичен Фейсбуку, но их общий потенциал, ну, примерно остался, скажем так, стал сравнимым.

377
00:49:52,888 --> 00:49:56,010
Ну и, условно, множество людей из Фейсбука в Сапстэк ушло.

378
00:49:56,010 --> 00:49:59,770
И вообще стали вводить разную систему протоколов.

379
00:49:59,770 --> 00:50:08,910
Получается, у нас возникает тема взаимодействия человека и машины, как еще дополнительная система протокола между технологией и антропологией.

380
00:50:08,910 --> 00:50:13,290
То есть, условно говоря, убедить, что машина... Но это все равно вбирается в сверхсубъектность.

381
00:50:13,290 --> 00:50:17,830
Сначала нужно пройти все эти этапы, о которых мы говорили вначале. Очень интересно.

382
00:50:17,830 --> 00:50:20,150
Евгений, мы тут уже в диалог ушли.

383
00:50:21,390 --> 00:50:27,870
Да-да-да, я вас слушаю внимательно. Я хотел бы потихонечку нас подвести к какому-то заключению.

384
00:50:27,870 --> 00:50:32,390
Мы начали с того, что я задал специально упрощённый вопрос.

385
00:50:32,390 --> 00:50:39,490
Так что же такое искусственный интеллект? Удобный инструмент или скрытая угроза?

386
00:50:39,490 --> 00:50:41,570
А может быть, и то, и другое.

387
00:50:41,570 --> 00:50:47,710
И из того, что прозвучало, я могу сделать несколько выводов, попробовать сделать несколько выводов.

388
00:50:47,800 --> 00:50:56,360
И я попрошу сначала Алису, а потом Павла откорректировать или подтвердить мои умозаключения. Первое.

389
00:50:58,043 --> 00:51:22,867
Сами создатели технологии и те, кто её развивает, до конца не представляют, с чем они имеют дело, не вполне осознают последствия, а скорее пытаются свои пожелания, свои идеальные стремления вербализовать как то, что они знают наверняка.

390
00:51:22,867 --> 00:51:31,412
Вообще говоря, мы имеем дело с технологией в руках людей, которые не до конца понимают ни как она работает, ни что с ней делать.

391
00:51:31,412 --> 00:51:34,292
Это первое. Второе.

392
00:51:34,292 --> 00:51:56,902
В силу того, как устроено современное общество, И как это общество привыкло реагировать на новые технологии, не стоит ожидать, что появится общепринятый подход к решению задачи, как противостоять рискам или как управлять рисками, связанные с искусственным интеллектом.

393
00:51:57,055 --> 00:52:11,835
Поскольку, как Павел очень верно и здорово заметил, в данном случае технологии, можно сказать, дошли до того предела, когда противостоять этой технологии… А зачем противостоять?

394
00:52:11,835 --> 00:52:16,915
Для того, чтобы не утратить, собственно, какую-то свою личность, субъектность.

395
00:52:16,915 --> 00:52:25,055
Вот как раз только увеличивая субъектность, можно противостоять это негативным, скажем так, последствиям.

396
00:52:25,962 --> 00:52:28,762
внедрение искусственного интеллекта.

397
00:52:28,762 --> 00:52:37,822
И это, как уже, наверное, третий пункт, как говорит Павел, наверное, самое позитивное, что есть в технологии искусственного интеллекта.

398
00:52:37,822 --> 00:52:45,862
Здесь у нас не остается выбора, либо быть человеком, либо раствориться в коконе из помощников.

399
00:52:46,633 --> 00:52:52,573
Я сказал три сразу, наверное, это было много, но давайте по одному, Алиса.

400
00:52:52,573 --> 00:52:57,113
Вы имеете в виду откомментировать это или согласиться или не согласиться?

401
00:52:57,113 --> 00:53:07,603
Да, где мои выводы с вашими не соотносятся или где я слишком драматизирую, как Павел сегодня сказал, «драма-менеджмент» что-то такое сегодня прозвучало.

402
00:53:07,603 --> 00:53:26,932
Мы далеки от того, чтобы гоняться за хайпом, все пропало, все умрут, но все-таки кажется, что особенность этого момента развития технологий в том, что ими занимаются люди не на том уровне, скажем так, мудрости.

403
00:53:28,787 --> 00:53:36,047
Мечи подросткам и отпустили их, скажем так, гулять.

404
00:53:36,047 --> 00:53:43,707
Есть большая вероятность того, что они не только друг друга покрошат, а, скорее всего, войдут в город и будет там что-то нехорошее.

405
00:53:43,707 --> 00:53:46,667
А мудрого предводителя нет.

406
00:53:47,383 --> 00:53:54,963
Это, знаете, как с Суперменом в утро проехали есть, но кто вам сказал, что у него те же ценности, что у вас?

407
00:53:54,963 --> 00:54:05,203
Я тогда попытаюсь коротко сказать, чтобы за папу было завершающее слово. Безусловно, большинство...

408
00:54:05,203 --> 00:54:10,903
Я не могу сказать, что вот в компаниях сидят люди, которые совсем не понимают, что они делают.

409
00:54:12,628 --> 00:54:19,268
Вот реально физически возможных пределов того, что можно понимать, понимают.

410
00:54:19,268 --> 00:54:28,018
Какая-то математика нам ещё не совсем понятна, но, наверное, мы её поймём. Другое дело, что...

411
00:54:28,018 --> 00:54:41,578
Те люди, которые в этом разбираются действительно хорошо, довольно часто выходят на конференции и произносят спички про то, что «ребята, нам нужно ответственно, нам нужно вот так вот делать».

412
00:54:41,578 --> 00:54:44,458
Мне всегда хочется спросить, к кому вы обращаетесь?

413
00:54:44,458 --> 00:54:48,738
Не люди, которые вас слушаются, могут что-то с этим сделать.

414
00:54:48,738 --> 00:54:51,458
Это как в той шутке, что «а если оно выйдет из тот контент?

415
00:54:51,458 --> 00:55:22,688
А вдруг оно выйдет из-под него?» И другое дело, что я думаю, что люди, которые обладают более глубоким пониманием, у них, судя по каким-то совсем, да, там, культурам, Небольшому количеству культуре, информации, которую я получаю от внутренних знакомых или каких-то уж совсем посторонних, опять же, как вы говорите, файл папочек не заносят.

416
00:55:22,688 --> 00:55:35,348
Деформация культуре, культуре, культуре, ментальная, которая происходит у людей, которые с этим действительно взаимодействуют каждый день, она на уровнях, я думаю, которые мы даже и не представляем.

417
00:55:35,348 --> 00:55:45,303
То есть даже те, у кого… я думаю, есть какой-то наработанный философский инструментарий, ценностный инструментарий.

418
00:55:45,303 --> 00:55:56,543
Я думаю, что им нереально в таком окружении, в такой ситуации, с такими задачами действительно сохранять холодную голову.

419
00:55:56,543 --> 00:56:07,428
Я предполагаю, что они, наверное, даже пытаются сделать что-то максимально хорошее, там да, вышло, а выходит, а выходит, как всегда.

420
00:56:07,428 --> 00:56:18,752
Кстати, вот Ян Лекун, которого я люблю слушать, потому что он уже, в общем-то, вышел из такой большой корпоративной игры, поэтому может себе позволить сказать гораздо больше.

421
00:56:18,752 --> 00:56:28,912
Он, правда, отвечал на вопрос, что сделать, чтобы я и нас не уничтожил, но, на мой взгляд, может быть, это и можно применить в какую-то такую более хорошую среду.

422
00:56:28,912 --> 00:56:50,155
Он тут говорит, слушайте, если мы говорим про ситуацию, когда у нас есть более глупое существо, человек, и более умное существо, там вот AGI, которое мы вдруг можем создать, не дай бог, вдруг, в этот день, в этот час он появится у этой команды, то давайте мы сделаем ценности материнские.

423
00:56:50,155 --> 00:57:03,295
Потому что мы знаем только один биологический случай, когда более умное существо готово щадить и как-то воспитывать, и давать какой-то конструктивный фидвейк более глупому существу.

424
00:57:03,522 --> 00:57:07,042
Но он это говорил с точки зрения, чтобы не уничтожило.

425
00:57:07,042 --> 00:57:27,802
А с нашей стороны, если сделать я достаточно занудным, неприятным, и это сделать как абсолютную необходимость для всех инструментов, может быть, как-то удастся допинать нас до чуть более лучшей ситуации.

426
00:57:27,802 --> 00:57:33,926
Но, опять же, можем, но не будем. Поэтому я думаю, что...

427
00:57:35,888 --> 00:57:55,148
мы тут уже действительно в такой очень нехорошей ситуации, из которой, скорее всего, если говорить о более историческом пласте, из которой нас может вытащить только, скорее всего, что-то очень радикальное, что прямо встряхнет нас.

428
00:57:55,148 --> 00:57:58,288
На этом завершу свои комментарии.

429
00:57:58,288 --> 00:58:02,408
После таких прекрасных завершений буквально два коротких комментария. Первый.

430
00:58:02,408 --> 00:58:07,187
Вспоминаем детскую сказку которая не очень детская. Лисы в стране чудес.

431
00:58:07,187 --> 00:58:10,427
И иногда, чтобы стоять на месте, нужно бежать в два раза быстрее.

432
00:58:10,427 --> 00:58:22,067
Вот на фоне развития наших вот этих магических артефактов, которые я теперь однозначно буду называть такими методами, бежать нужно даже и в два, может быть, по экспоненте быстрее.

433
00:58:22,067 --> 00:58:25,547
И в этом я вижу оптимистическую провиденческую работу.

434
00:58:25,547 --> 00:58:32,387
А то мы слишком долго прятались за разными технологическими, скажем так, идолами.

435
00:58:32,723 --> 00:58:58,080
считая себя их господами, ну, наконец-то мы создали голема, или создаем в процессе, чисто под прекрасные лозунги, с прекрасными технологическими возможностями, которые вопрос, собственно, нашей, прежде всего, на самом деле, этической субъектности, уже во вторую уровень и когнитивно-эмоциональную субъектность, ставят перед тем самым большим-большим зеркалом Которое нам и должно показать.

436
00:58:58,080 --> 00:59:03,740
Первое, то, что мы-то уже сами по себе почти-таки очень голенькие стали за последние лет 400.

437
00:59:03,740 --> 00:59:10,600
Ну ладно, все кроме Алиса. Мы с Евгением однозначно. Вот. И тут два варианта.

438
00:59:10,600 --> 00:59:17,180
Либо уж совсем залезать в эту капсулку, как в фильме Матрица, либо все-таки уже искать одежду, основание.

439
00:59:17,323 --> 00:59:20,503
И тренировать себя.

440
00:59:20,503 --> 00:59:26,123
То есть это самое внутреннее усилие подменять своё самоволие настоящим самовластием.

441
00:59:26,123 --> 00:59:30,803
То есть вот такого вот спрятаться не удастся. Такой вот для себя вывод, который я сделал.

442
00:59:33,220 --> 00:59:38,860
Отличное завершение. Спасибо, Алиса. Прежде всего, спасибо, Павел.

443
00:59:38,860 --> 00:59:47,080
Я думаю, что мы, по крайней мере, постараемся ещё вернуться к этой теме вместе с Алисой и Павлом.

444
00:59:47,080 --> 00:59:53,380
Ну а для тех, кто слушает или смотрит нас, пожалуйста, оставляйте свои комментарии.

445
00:59:53,385 --> 01:00:12,565
Дополняйте нас, мы будем рады услышать мнение тех людей, особенно кто находится в этой индустрии и сможет рассказать нам о том, что мы не знаем, например, или поделиться какими-то своими размышлениями, которые нас дополнят.

446
01:00:12,565 --> 01:00:15,085
Всем еще раз большое спасибо, ну и до встречи.

447
01:00:15,085 --> 01:00:15,525
До встречи.
