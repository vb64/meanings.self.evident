1
00:00:00,058 --> 00:00:10,198
Компьютер. Ну что ж, да, прежде чем мы начнем, Павел, кстати, забыл еще задать вопрос.

2
00:00:10,198 --> 00:00:17,138
Мы как будем это позиционировать, как продолжение наших подкастов или все-таки это будет отдельное у нас видео?

3
00:00:18,739 --> 00:00:25,279
Я думаю, здесь лишнее позиционирование в отдельную коробочку на данный момент преждевременно.

4
00:00:25,279 --> 00:00:32,819
Мы просто делаем интересные разговоры видео и продолжаем встречи на наших ресурсах.

5
00:00:32,819 --> 00:00:44,810
Так, хорошо. Тогда, что ж, начнем. Три, два, один, камера, запись. Здравствуйте, друзья!

6
00:00:44,810 --> 00:00:59,710
В эфире необычный выпуск ведущих подкаста «В поисках смысла» Евгения Голуба, Павла Щелина, и сегодня с нами приглашённый эксперт Алиса Ким.

7
00:00:59,710 --> 00:01:08,910
Мы решили записать этот выпуск, так как первый раз мы затронули тему искусственного интеллекта, а сегодня речь пойдёт о нём.

8
00:01:09,280 --> 00:01:15,640
Примерно год назад, может быть, немного больше, и с тех пор многое изменилось.

9
00:01:15,640 --> 00:01:18,720
Я бы так сказал, что изменилось практически всё.

10
00:01:18,720 --> 00:01:26,180
И для того, чтобы осмыслить происходящее, нам уже не хватает собственного понимания.

11
00:01:26,180 --> 00:01:31,380
Мы подозреваем себя в предвзятости. Для этого мы пригласили Алису.

12
00:01:31,380 --> 00:01:38,220
Алиса — эксперт в области искусственного интеллекта.

13
00:01:40,633 --> 00:01:46,813
Так, минуточку, я выключу свою Алису, потому что каждый раз на слово «Алиса» она будет включаться.

14
00:01:46,813 --> 00:01:52,213
Есть такая партия, да.

15
00:01:52,213 --> 00:01:55,593
Рад видеть, если по-честному.

16
00:01:55,593 --> 00:01:58,653
Да, взаимно.

17
00:01:58,653 --> 00:02:05,193
Кстати, ковёр не дооценивайте, я думаю, что уже есть целое течение по интерпретации вашего бэкграунда.

18
00:02:05,743 --> 00:02:13,303
Я думаю, что ковёр уже — это там целое направление. Так в чём смысл ковра в творчестве Павла?

19
00:02:13,303 --> 00:02:19,823
Я говорю «Алиса» и такой «Эксперт». Журнал такой. На чём мы остановились, друзья мои?

20
00:02:19,823 --> 00:02:21,143
Хорошо, что мы записываемся.

21
00:02:21,143 --> 00:02:23,783
Представляешь, ты Алису-то представлял, родной.

22
00:02:23,783 --> 00:02:30,183
Итак, мы пригласили Алису Ким, кандидата наук, эксперта по искусственному интеллекту.

23
00:02:30,685 --> 00:02:39,785
Алиса занималась академическими исследованиями в Гумбольд-Университете, точнее в Университете Гумбольда и в Стэнфордском университете.

24
00:02:40,202 --> 00:02:47,802
Алиса разрабатывала языковые модели в AWS, Amazon, что-то там, как оно дальше?

25
00:02:47,802 --> 00:02:48,822
Веб-сервис.

26
00:02:48,822 --> 00:02:49,722
Веб-сервис.

27
00:02:49,722 --> 00:02:59,302
Ну и, в общем, последние 10 лет Алиса занимается внедрением искусственного интеллекта в разного рода системах, в стартапах и крупных корпорациях.

28
00:02:59,302 --> 00:03:06,982
Поэтому, ну, кому как ни к Алисе нам прийти с нашими вопросами и недоумениями.

29
00:03:06,982 --> 00:03:09,822
Ну что ж, начнем потихонечку. Итак.

30
00:03:10,476 --> 00:03:17,636
Я предложил сегодня разделить видео или роли в нашей встрече следующим образом.

31
00:03:17,636 --> 00:03:21,996
Как уже понятно, Алиса — наш эксперт, Павел — философ.

32
00:03:21,996 --> 00:03:35,736
Ну а я буду выступать сегодня в роли обычного пользователя разного рода и помощников, обычного обывателя, которых много и у которых есть простые и незамысловатые вопросы.

33
00:03:35,736 --> 00:03:40,456
Начну я с этого такого простого вопроса к Алисе.

34
00:03:40,463 --> 00:03:50,843
Сегодня уже очень многие, включая меня, не мыслят в своей жизни без помощников, как мы их называем.

35
00:03:50,843 --> 00:03:58,837
У меня при запуске браузера запускается 6 штук сразу.

36
00:03:58,837 --> 00:04:09,517
И многие сейчас уже, собственно, ищут даже продукты через искусственный интеллект, через разного рода помощников.

37
00:04:09,517 --> 00:04:19,517
Так вот, вопрос следующего рода. Все-таки это полезная штука или за этим кроется что-то еще?

38
00:04:21,030 --> 00:04:34,210
Как вы считаете, насколько нагружены сегодняшние помощники скрытыми какими-то функциями, скрытыми намерениями с их создателей?

39
00:04:34,210 --> 00:04:39,170
Есть ли подвох в этой технологии?

40
00:04:39,930 --> 00:04:44,830
Наверное, начну с занудного полезного для кого и для чего.

41
00:04:45,803 --> 00:04:53,243
Для нас, как для пользователей, в наших жизненных целях, безусловно, на мой взгляд, полезная вещь.

42
00:04:53,243 --> 00:05:09,683
Но мы с вами находимся в такой сложной ситуации, когда абсолютно всё в этом продукте, по сути, оптимизировано для того, чтобы мы чуть-чуть сбились с курса, забыли о том, зачем мы делаем то, что мы делаем, и как это работает.

43
00:05:10,491 --> 00:05:19,351
Тут, если бы я должна была ответить на вопрос классический, ну так это хорошая вещь или плохая, я бы сказала.

44
00:05:19,351 --> 00:05:23,031
Хорошая, если мы будем пользоваться ей осознанно.

45
00:05:23,031 --> 00:05:32,911
И на этой части обычно все юзеры, я в том числе, говорят, ну если осознанно, то это уже слишком сложно.

46
00:05:33,411 --> 00:05:40,011
Что касается нагруженности, тут, на мой взгляд, всё банально и немножко грустно.

47
00:05:40,393 --> 00:05:51,033
Практически всё о том, как создаются эти помощники, зачем, с какими ограничениями, всё прописано, всё открыто.

48
00:05:51,033 --> 00:05:58,765
Я думаю, что там скрытых смыслов и скрытых идей тайных помыслов довольно мало.

49
00:05:58,765 --> 00:06:03,305
Другое дело, что мы как... Вы знаете, как никто не считает правила пользования.

50
00:06:03,305 --> 00:06:07,805
Все всегда все принимают и рад с этим дальше пользоваться.

51
00:06:07,805 --> 00:06:46,535
Другое дело, что мы с вами, наверное, никогда еще не сталкивались с инструментом, по сути, телевизорами, чайниками, любыми инструментами, доступными для рядового пользователя, которые были бы настолько сложны и настолько обманчивы, настолько сделаны для того, чтобы максимально с нашей головушкой помериться силами, запутать и вести в искус.

52
00:06:46,535 --> 00:07:09,824
Изначально же все эти системы, когда они стали популярны, никогда не стали точными, никогда не стали какими-то правильными с точки зрения даваемых ответов, а тогда, когда они стали настолько похожи на настоящего собеседника, что мы прямо вот включились в это.

53
00:07:09,824 --> 00:07:17,344
К сожалению, эволюционно мы очень... Мы и так-то любим всё антропоморфизировать.

54
00:07:17,344 --> 00:07:21,344
С компьютерами разговариваем, с телевизорами даём имена машинам и так далее.

55
00:07:21,964 --> 00:07:28,624
А тут оно ещё и разговаривает, и отвечает, и ещё и учится тому, что мы любим и не любим, и запоминает нас.

56
00:07:28,624 --> 00:07:35,444
Ну, то есть тут не начать с этим взаимодействовать нас и помощником, не как инструментом, ну, практически нереально.

57
00:07:35,444 --> 00:07:48,624
Даже если вот вы супер в подсознанке, скорее всего, там, да, через N дней и N пользований вы уже немножко забудете про то, как бы, да, зачем это было сделано, как это работает и так далее.

58
00:07:48,624 --> 00:07:51,464
На мой взгляд, пользы этого можно извлечь очень много.

59
00:07:51,546 --> 00:08:03,506
Другое дело, что с такими инструментами прямо вот совсем-совсем нельзя забывать, зачем я это делаю, какие у этого ограничения.

60
00:08:03,506 --> 00:08:11,546
Они все прописаны, они все понятные, но кроме тех, которые не прописаны, по идее, должны быть понятны любому человеку.

61
00:08:12,927 --> 00:08:15,567
Это коммерческий инструмент.

62
00:08:15,567 --> 00:08:24,847
Наверное, там есть какая-то доля того, что на вас хотят заработать, и ваше всеобщее благо мира, наверное, там не единственная цель.

63
00:08:24,847 --> 00:08:28,527
То есть, скорее всего, этот компонент тоже есть, но мы про это не очень любим помнить.

64
00:08:28,527 --> 00:08:37,247
А это влияет на то, как дальше развиваются эти системы. Вот такой, мой будет ответ. То есть.

65
00:08:37,247 --> 00:09:57,050
Штука, как любая технология, всегда имеет скажу иначе искусственный интеллект как любой инструмент может быть использован по прямому назначению его плага или как кухонный нож можно резать мясо можно зарезать человека как технология книгопечатания можно печатать библию а можно порно рисуночки сдавать и здесь уже вопрос у меня к павлу Мы говорили с тобой год, наверное, и много больше назад о том, что движется, кажется, такое время, надвигаются времена, когда многое из того, что делает человека человеком, Годит, скорее всего, передано на аутсорс, искусственному интеллекту, и мы тогда с тобой видели риски того, что вот это вот творческое начало, чувства, эмоции начнут автоматизироваться, и, соответственно, в этой части есть риски для людей потерять свой компонент человечности, что ли.

66
00:09:57,050 --> 00:09:58,350
Что ты думаешь по этому поводу?

67
00:10:01,112 --> 00:10:08,472
Еще раз всем здравствуйте. Ну, думаю много чего. Тут даже не знаешь с какого конца подбираться.

68
00:10:08,472 --> 00:10:16,312
Первое, все-таки сделаю базовые комментарии для фиксации собственной позиции по твоему преамбуле.

69
00:10:16,312 --> 00:10:20,292
Про вот кухонный нож и прочую всю вот эту историю.

70
00:10:20,292 --> 00:10:24,132
Дело в том, что вот с этой позиции я философски не согласен.

71
00:10:24,484 --> 00:10:33,064
Мне представляется, что само представление существования такого феномена, как нейтральная технология, является глубоким заблуждением.

72
00:10:33,064 --> 00:10:37,444
Не существует такого феномена, как нейтральность технологии.

73
00:10:37,444 --> 00:10:44,484
С самим фактом своего существования технология не нейтральна. Она создает ассиметрию.

74
00:10:44,484 --> 00:10:51,062
Банально между теми, кто технологией пользоваться умеет и кто технологией пользоваться не умеют.

75
00:10:51,062 --> 00:11:02,342
Те, кто технологией пользоваться умеют, получают дополнительные ресурсы, власть, способ взаимодействия с миром относительно тех, кто ей не пользуется.

76
00:11:02,368 --> 00:11:09,788
эту асимметрию ты никаким образом... Вот это и есть собственно сама технологическая асимметрия.

77
00:11:09,788 --> 00:11:17,068
То, что ты говоришь уже про волевой этический выбор субъекта, использующий технологию, это следующий этап, это следующий уровень.

78
00:11:17,068 --> 00:11:24,228
Но сначала есть вот этот базовый уровень, что самим фактом своего бытия технология мир меняет.

79
00:11:24,228 --> 00:11:35,555
И вот наше представление о ней как о некой нейтральности это очень хороший sales point для любого, скажем так, маркетолога, но с философской точки зрения просто он неадекват.

80
00:11:35,555 --> 00:11:39,475
Это первый момент. Второй момент связан с... Подожди, минуточку, ты.

81
00:11:39,475 --> 00:11:52,855
Же знаешь, я обычно тебя прерываю, уже прости, но ты знаешь, мне кажется, нужно объяснить, что такое твой тезис о том, что самим фактом существования технология меняет мир.

82
00:11:53,989 --> 00:11:59,769
Логическая связь здесь не очевидна, по крайней мере, для меня.

83
00:11:59,769 --> 00:12:01,149
Ну, давай самое простое.

84
00:12:01,149 --> 00:12:11,389
Вот если пока этой технологии не было, у тебя была определенная культура и определенные, если условно, паттерны, уж простите, не знаю, как там, вечно...

85
00:12:11,389 --> 00:12:14,429
независимость. Закономерности некоторые.

86
00:12:14,429 --> 00:12:23,616
Закономерности отношений между людьми, отношений экономические, социальные, политические и так далее, как ты ни крути.

87
00:12:23,616 --> 00:12:29,896
Вот когда технология появилась, она стала фактором всех этих отношений просто по факту своего появления.

88
00:12:29,896 --> 00:12:33,356
Еще никакой воли нет, но она создала дополнительные возможности.

89
00:12:33,356 --> 00:12:38,640
Повторюсь, главная эта возможность, я ее назвал, это возможность к власти.

90
00:12:38,640 --> 00:12:43,780
Любая технология содержит в себе заряд к власти. Но это логично.

91
00:12:43,780 --> 00:12:47,440
Если бы она его не содержала, ее бы никто не создал. Это вот очень важно понимать.

92
00:12:47,440 --> 00:12:56,060
То, что в нашем культуре технология всегда создается, на самом деле, с неким зарядом к власти.

93
00:12:56,060 --> 00:13:02,772
Тот, кто, банально, власть работы, в нашем конкретном примере, власть манипулирования, власть работы с данными.

94
00:13:02,772 --> 00:13:08,572
власть, производство дополнительного материального ресурса и так далее. Это все властные отношения.

95
00:13:08,572 --> 00:13:16,552
И вот эта технология, ты можешь сказать, если тебе уточнить, ну до поры до времени можно сказать, что она создается с технологией как минимум потенциальной власти.

96
00:13:16,552 --> 00:13:24,552
То есть она лежит и спит, и да, требуется волевой субъект, чтобы эту власть активизировать, но тем не менее потенциал-то уже создан самим фактом ее появления.

97
00:13:24,552 --> 00:13:27,375
То есть некая статус-кво оказалась нарушена. Все.

98
00:13:27,375 --> 00:13:40,335
То есть изобретение автомобиля, вне зависимости от намерений конкретного изводчика, конкретного водителя или производства автомобиля, самим фактом, называя технологией, является угрозой, условно говоря, для коневодов.

99
00:13:40,335 --> 00:13:44,595
Она меняет эти отношения автоматически, просто по факту своего появления.

100
00:13:46,073 --> 00:13:51,353
Технология может существовать, продукта может не быть.

101
00:13:51,353 --> 00:13:59,313
Я поэтому уточнил про потенциальное. Ну, давай так, ты прав. Как минимум потенциальное.

102
00:13:59,313 --> 00:14:01,973
С философской точки зрения я веду категорию потенциальное.

103
00:14:02,256 --> 00:14:10,916
изменения, но просто мы живем еще в цивилизации последние 400 лет, где любое потенциальное изменение в зоне технологии прогресса должно быть актуализировано.

104
00:14:10,916 --> 00:14:15,556
У нас нет никаких этических ограничений на любую технологическую актуализацию.

105
00:14:15,556 --> 00:14:18,256
Это, собственно, ради этого модерн мы и создавали.

106
00:14:18,256 --> 00:14:32,172
Мы говорим о том, что в модерне каждая технология прежде всего рассматривается с точки зрения возможности увеличить властный потенциал субъекта, обладающего этой технологией.

107
00:14:32,172 --> 00:14:34,992
Субъекта, обладающего этой технологией, да.

108
00:14:34,992 --> 00:14:43,312
А в любой технологии всегда, и до модерна, и после модерна, и вне модерна, ну всегда содержится увеличение потенциала субъекта.

109
00:14:43,312 --> 00:14:45,012
Ну это понятно. Простой пример.

110
00:14:45,012 --> 00:14:45,992
Усиление, да?

111
00:14:45,992 --> 00:14:48,672
Усиление. Ну я не могу бегать как гепард.

112
00:14:48,780 --> 00:14:53,000
Но с машиной я могу перемещаться со скоростью, которой гепарду и не снилось.

113
00:14:53,000 --> 00:15:00,500
Мой потенциал в категории беганье, она усиливает. Она поэтому и создана.

114
00:15:00,500 --> 00:15:06,860
Теперь, я тебя прервал, может быть, ты сделаешь шаг назад и вернешься к своему первому тезису?

115
00:15:06,860 --> 00:15:14,320
Да. Чтобы я его вспомнил. Он связанный с первым тезисом, и это проблема асимметрии последствий.

116
00:15:14,565 --> 00:15:19,645
Другими словами, это вот был бы мой вопрос к Алисе следующий, если она попробует.

117
00:15:19,645 --> 00:15:23,645
Мне просто интересно, как вообще, есть ли размышления на эту тему.

118
00:15:23,645 --> 00:15:26,345
Приведу пример не с искусственным интеллектом, но близкий.

119
00:15:26,345 --> 00:15:33,265
У нас есть технологии социальных сетей, выпущенные относительно недавно, буквально 15 лет назад.

120
00:15:33,265 --> 00:15:44,465
Сейчас начали выходить исследования Айнейра об изменениях на материальном уровне мозга молодых, особенно детей, подростков, девочек, которые 10 лет выросли на этих технологиях.

121
00:15:45,447 --> 00:15:52,207
Скажем так, исследование, мягко говоря, тревожище. То есть там много разных неприятных последствий.

122
00:15:52,207 --> 00:15:55,507
Но я сейчас говорю не про это. Я говорю про то, что вот у нас есть асимметрия.

123
00:15:55,507 --> 00:16:00,307
Технология выпущена была 10 лет назад. Последствия от нее пришли 15 лет.

124
00:16:00,307 --> 00:16:10,087
И что-то я сомневаюсь, что 15 лет назад, когда люди, когда выпускали эти социальные тети, вообще хоть на каком-то этапе выпуска этой технологии задумывались о последствиях через 15 лет.

125
00:16:10,087 --> 00:16:12,427
Это ее структурное ограничение.

126
00:16:12,427 --> 00:16:20,905
То есть По крайней мере, в нашей культуре, где скорость, повторюсь, не повторюсь, скажу в новой тези, что скорость является благом сама по себе.

127
00:16:20,905 --> 00:16:28,485
В принципе, идея торможения, движения, особенно технологического, является ересью и харамом.

128
00:16:28,485 --> 00:16:36,052
Поэтому у меня есть вот большой вопрос, то что в рамках вот этой технологической семьи, то есть Бедный наш звуковик.

129
00:16:36,052 --> 00:16:40,452
То есть, смотри, проблема технологической асимметрии последствий существовала всегда.

130
00:16:40,452 --> 00:16:45,212
Собственно, мы это знаем со времен ящика Пандора. Миф ящика Пандора ровно про это.

131
00:16:45,212 --> 00:16:49,172
Принесли огонь, а потом выпушил, как-то получился ящик.

132
00:16:49,172 --> 00:16:52,423
Это вот очень классическая взаимосвязанная история.

133
00:16:52,423 --> 00:17:02,923
Но сегодня мы просто повысили масштабы, скажем так, этой проблемы до определенного уровня, который в каком-то смысле количественно действительно является беспрецедентом.

134
00:17:02,923 --> 00:17:05,043
Это вот такая моя занятка, зарубка на полях.

135
00:17:05,043 --> 00:17:15,983
Мне просто интересно, как изнутри вообще ставится ли так вопрос, вот по-честному, не на уровне красивых презентаций, а как внутреннее самоощущение.

136
00:17:15,983 --> 00:17:21,463
Прежде чем Алиса ответит, Алис, сейчас солнце вышло, и у вас лицо делится пополам.

137
00:17:22,090 --> 00:17:24,230
просто там тень и она прямо пополам.

138
00:17:24,230 --> 00:17:31,330
Разделяет и с такой делом лучше так.

139
00:17:31,330 --> 00:17:40,330
Надо лучше я надеюсь что она зайдет нет окей будем так вот так это.

140
00:17:40,330 --> 00:17:43,759
Так хорошо да так отлично Имеется в.

141
00:17:43,759 --> 00:18:02,792
Виду вопрос, насколько обсуждается и насколько озабочены компании, разрабатывающие, ну не будем говорить и скажем пока что, да, большие языковые модели, потому что там много всего другого еще есть радостного и интересного, вот, насколько озабочены последствиями.

142
00:18:02,792 --> 00:18:19,372
Я не возьмусь говорить, конечно, за всех гигантов индустрии, но в целом я думаю, что можно сделать такую усреднённую позицию, сформулировать.

143
00:18:19,372 --> 00:18:39,281
Есть философская, идеалистическая воля их основателей, CEO, ведущих учёных, которые Все как один пишут и, скорее всего, действительно думают о том, что они бы очень хотели сделать мир лучше.

144
00:18:39,281 --> 00:18:44,321
Да, то есть их позиции почти всегда такие очень публичные.

145
00:18:44,321 --> 00:18:51,901
Я думаю, что они действительно в это верят. Есть то, как это работает внутри.

146
00:18:51,901 --> 00:19:30,808
В целом, почти все негативные последствия, которые вызывают эти продукты, они практически всегда важны только когда они коротковременные и влияют на два самых важных фактора, которые в свою очередь влияют на то, довольны инвесторами или нет, а по сути это К сожалению, тот единственный драйвер, который важен, в силу, кстати, того, что вы сейчас сказали о скорости, потому что сейчас у абсолютно всех участников рынка 100% уверенность в том, что вот сейчас мы в этой точке бифуркации.

147
00:19:30,808 --> 00:19:36,348
Тот, кто успеет и возьмёт рынок, тот будет править следующие 100 лет.

148
00:19:36,348 --> 00:19:41,832
Ещё хорошо бы, чтобы хотя бы в нашей стране есть же ещё китайцы, русские и все остальные.

149
00:19:41,832 --> 00:19:47,132
Дальше посмотрим. Сейчас главное нужно успевать.

150
00:19:47,132 --> 00:19:53,672
Для этого нужно очень много денег на самые разные вещи. Что не любят инвесторы?

151
00:19:53,672 --> 00:20:01,923
Инвесторы не любят, когда по рукам дают регуляторы, и инвесторы не любят, когда сильно жалуются и отпадают пользователи.

152
00:20:01,923 --> 00:20:19,432
Вот если кто-то из них усмотрел непосредственный вред в каком-то виде, сумел это довести до точки, когда реально уже наступает прессинг на компанию внести какие-то изменения, то тут, а, компания может официально как-то позицию заявить.

153
00:20:19,432 --> 00:20:26,572
То есть, например, сейчас у всех компаний прописаны их, условно говоря, ценности и ориентиры.

154
00:20:26,572 --> 00:20:35,212
Например, OpenAI — это вот мы хотим, чтобы мы были helpful, но no harm и maximize utility — вот это их такая общая тема.

155
00:20:35,212 --> 00:20:43,912
И это влияет на то, что они реально внутри пытаются делать для того, чтобы как-то ограничить негативные, например, вот этот no harm обеспечить.

156
00:20:44,269 --> 00:20:55,929
Но в целом есть, к сожалению, такая неприятная история, такой конфликт интересов, что это, знаете, как для принципиального человека уломать на что угодно гораздо сложнее.

157
00:20:55,929 --> 00:21:02,489
Вот модель, у которой слишком много ограничений, она, скорее всего, будет не так хорошо, красиво работать.

158
00:21:02,489 --> 00:21:12,489
Ее тренировать дороже, ей могут быть недовольны пользователи, и поэтому в целом мотивации реально усложнять эту историю у компании нету никакой.

159
00:21:12,973 --> 00:21:16,893
Вот только вот те два фактора, которые я сказала, да, там регулятор... Я просто.

160
00:21:16,893 --> 00:21:21,433
Еще... Простите, что немножко перебил, но просто хочу подчеркнуть.

161
00:21:21,433 --> 00:21:26,893
Это еще мы находимся в очень, на самом деле, маленьком кружочке. Я этот вопрос задал гораздо больше.

162
00:21:26,893 --> 00:21:29,553
То есть вы, по сути, подчеркнули проблему...

163
00:21:30,253 --> 00:21:41,333
я бы сказал, ну, некого knowable harm, то ли какой-то, ну, который ты получаешь информацию об этом вреде через вот этот фидбэк-клуб некий, да, и ты хоть какую-то информацию получаешь.

164
00:21:41,333 --> 00:21:58,253
Я же постулировал вопрос более радикально, потому что, как в примере с этих социальных сетей, я ни за что не буду утверждать, что у людей, которые вводили социальные сети как корпоративный метод в середине двухтысячных, было намерение сломать психику девочкам-подросткам в 2025 году.

165
00:21:59,987 --> 00:22:08,867
То есть, вот тут основная проблема в том, что есть огромная сфера того, что мы не знаем о технологическом последствии.

166
00:22:08,867 --> 00:22:14,767
Мы в теории могли бы попытаться об этом думать, как, не знаю, категория философского риска, промышления.

167
00:22:14,767 --> 00:22:24,007
Но, как я понимаю, из вашего описания, разумеется, не по причине некого зла, а по причине той системы, скажем так, мотивации к действию, такой вопрос в принципе никто не ставит.

168
00:22:24,007 --> 00:22:28,687
Если последствия будут через 15 лет, нас это вообще никаким образом сегодня не волнует.

169
00:22:29,704 --> 00:22:51,464
Про это пытаются думать и даже нанимают дорогостоящих исследователей, образовывают целые, да, там, финк-тэнки внутри компаний, и они даже публикуют, ну, желательно, не сильно, конечно, радикальные работы, но вот показать социальную ответственность очень надо, но нет времени и денег у компаний сейчас об этом думать.

170
00:22:51,464 --> 00:23:07,274
Разве что какие-то более независимые институты могут пытаться делать какие-то проекты, они их делают, но Нет времени, возможности, слишком велика конкуренция, слишком велик прессинг.

171
00:23:07,274 --> 00:23:09,054
Нет времени объяснять.

172
00:23:09,054 --> 00:23:10,314
Да, да, да.

173
00:23:10,314 --> 00:23:14,866
Именно так. Евгений, вы с нами?

174
00:23:14,866 --> 00:23:20,354
Друзья, минуточку, минуточку, минуточку, минуточку. У меня, кажется, возникла техническая проблема.

175
00:23:20,354 --> 00:23:21,254
Вы меня слышите?

176
00:23:21,254 --> 00:23:26,674
Мы вас слышим, и ты завис в задумчивом виде.

177
00:23:26,674 --> 00:23:34,374
Так и есть. Я действительно завязываю с этим, что видите. Сейчас я попытаюсь его разморозить.

178
00:23:34,374 --> 00:23:37,774
К сожалению, сейчас у меня болезни роста.

179
00:23:37,774 --> 00:23:45,274
Я пока себе позволю, кстати, комментарий, Павел, по поводу того, что вы сказали, что это не нейтральная технология.

180
00:23:45,274 --> 00:23:50,994
Эта технология и не подается как нейтральная, она подается очень эффективно.

181
00:23:50,994 --> 00:23:54,434
Расскажите про это. Это очень важно. Это очень важная история.

182
00:23:54,687 --> 00:24:00,767
На самом деле. Тут, безусловно, нам не продают просто так нож.

183
00:24:00,767 --> 00:24:08,707
Нам продают систему, вокруг которой построено огромное количество утверждений, да, прокламаций.

184
00:24:08,707 --> 00:24:16,747
Алиса, вот все, что хотите сказать на эту тему, пожалуйста, когда вот сейчас запись будет, потому что это прекрасно и необходимо.

185
00:24:16,747 --> 00:24:20,487
Да нет, просто это самое-то и есть. Это в общей тезис выйдет.

186
00:24:20,640 --> 00:24:24,640
Ну, то есть, просто мне интересно, как это воплощается в вашем случае.

187
00:24:24,640 --> 00:24:29,520
Тезис о ненависти к технологии мы с вами разбирали, в том числе на наших встречах.

188
00:24:29,520 --> 00:24:40,500
А это главная наша, как бы, идея, которая движется в наше общество, что технология сама по себе нейтральна, и только вопрос воли, как она будет использована.

189
00:24:40,500 --> 00:24:47,402
Это вот философски не так просто. Просто интересно, как сейчас это воплощается. Евгений, ты с нами?

190
00:24:47,402 --> 00:24:49,560
Сейчас, минуточку. Меня уже видно?

191
00:24:49,560 --> 00:24:50,320
Ну, видно.

192
00:24:50,627 --> 00:24:52,427
Слышно и слышно.

193
00:24:52,427 --> 00:24:53,387
Слышно.

194
00:24:53,387 --> 00:25:01,667
Прошу прощения, тут возникли серьезные проблемы типа того, что компьютер мой разрядился.

195
00:25:01,667 --> 00:25:07,607
Надо его найти. У меня всего два USB-C этих самых выхода.

196
00:25:07,607 --> 00:25:14,167
Один включена звуковая карта, другой телефон, и в общем пришлось чем-то пожертвовать. Итак.

197
00:25:14,167 --> 00:25:16,887
Так вот, у Алисы был прекрасный комментарий. Давай сделаю хлеб.

198
00:25:16,887 --> 00:25:20,607
Алиса, вот прям как сказали, Павел, а вот вы сказали и прямо начинаете.

199
00:25:22,082 --> 00:25:27,222
Павел хотел отдельно прокомментировать то, что вы сказали касательно нейтральности технологии.

200
00:25:27,222 --> 00:25:34,522
Мы здесь, безусловно, не имеем дела с технологией, которая даже подается как нейтральная.

201
00:25:34,522 --> 00:26:07,037
То есть, во-первых, большинство этих решений подаются с очень громким информационным бэкграундом того, что мы это делаем ради того, чтобы человечество тут лучше жило, чтобы вам, дорогие пользователи, дать свободу, то есть там OpenAI, он же freedom to the users, он прямо это обещает, да, то есть потом уже появляется safety и так далее, но это в целом подается очень агрессивно, как это прям вот то, что сейчас вам всем сделает лучше.

202
00:26:07,037 --> 00:26:28,257
И для того, чтобы этой технологии пользоваться действительно максимально осознанно, максимально возможно и безопасно, то тоже нужно найти третий ход слева за трактором, повернуть направо и желательно отключить вот этот вот еще функционал, вот эту информацию не давать, а вот здесь еще перезагрузиться.

203
00:26:28,257 --> 00:26:33,612
И тогда в целом, наверное, будет чуть получше.

204
00:26:33,612 --> 00:26:57,368
Уровень сложности, количество сальто, которое нужно сделать, чтобы действительно эта технология для вас, когда пользователя, была нейтральной, бесчестно велик для того, чтобы утверждать, что это вот, да нет, ну как бы мы же вам все по-честному дали, это все вы, это ваше пользование дало вам плохие результаты, это не мы.

205
00:26:57,368 --> 00:26:59,948
Тут ну нужно просто это по-честному отметить.

206
00:27:00,413 --> 00:27:02,893
Ну, у меня будет два комментария. Первое.

207
00:27:02,893 --> 00:27:13,393
Так как я постоянно рассказываю о том, что человек выходит из корпоративного мира, то я знаю цену всем вот этим корпоративным миссиям, видению и всему остальному.

208
00:27:13,393 --> 00:27:21,353
Цена эта не очень высока. Это всё, в общем, известное лицемерие.

209
00:27:21,353 --> 00:27:30,475
И во главе угла всегда стоят только деньги. деньги и власть.

210
00:27:30,475 --> 00:28:06,948
Поэтому если на пути у топ-менеджмента становятся какие-то не вполне, скажем так, очевидные или сомнительные свойства продукта, то топ-менеджмент всегда, повторяю, всегда, прежде всего, попытается добиться максимального финансового результата конечно же, с одной стороны, снижая риски для себя, и главным образом, как бы кто об этом ничего не узнал, или как бы чего не вышло, а уже потом, как ничего не вышло с точки зрения пиара, а уже потом будет думать о всех этих миссиях и видениях.

211
00:28:06,948 --> 00:28:15,028
Миссии и видения нужны для того, чтобы себе, там, красиво выступать на конференциях и сорвать аплодисменты.

212
00:28:15,028 --> 00:28:16,188
Это первый комментарий.

213
00:28:16,188 --> 00:28:28,651
Поэтому наличие в Antropic, да, миссии и видения, меня совершенно не успокаивает, а даже, скорее, наоборот, говорит о том, что если такое миссию видеть, значит, точно там где-то что-то не так.

214
00:28:28,651 --> 00:28:36,731
Это первое. А второе, это наблюдение за нашими лидерами мнений.

215
00:28:36,769 --> 00:28:54,249
вот этими замечательными гениями технологическими, вроде Сэма Альтмана, который, как мы уже говорили с Павлом, в своем послании «Городу и миру» заявил о том, что мы в двух шагах от райских кущ, которые нам произведет искусственный интеллект.

216
00:28:54,249 --> 00:29:14,441
И при этом он перечислял какие-то такие, скажем так, свойства искусственного интеллекта и приведет такие доводы, которое можно, наверное, оглянувшись назад, было бы услышать от изобретателей, не знаю, электрических двигателей, стиральных машин, паровозов и так далее, и так далее.

217
00:29:14,441 --> 00:29:19,201
То есть кажется, что, дружище, ну как бы, что ж ты повторяешь-то все одно и то же?

218
00:29:19,201 --> 00:29:25,085
Ну как изменилась жизнь обывателя к лучшему? за счёт технологии. Она стала комфортнее, да, и что?

219
00:29:25,085 --> 00:29:26,765
И к чему это привело?

220
00:29:26,765 --> 00:29:32,105
А теперь твоя технология отнимает у него последний шанс к творчеству, как мне кажется.

221
00:29:32,105 --> 00:29:39,445
Алиса, ваши комментарии как человека близко к корпоративному миру, ну и, конечно же, Павла хотела бы послушать.

222
00:29:39,445 --> 00:29:46,725
Ну я здесь, наверное, скажу, а теперь пару слов в апологию всей этой истории.

223
00:29:48,302 --> 00:30:11,502
Нужно просто сказать, что даже если внезапно самые альфаны, маркет-сукерверги этого мира решат, всё, давайте, ребята, забудем про деньги, будем заниматься, вот прям вот действительно постараемся, чтобы наши, да, там, ЛЛМы, наши Клоды, наши чаты ЧПТ, они прям были, да, там, сели, разумные, добрые, вечные и так далее.

224
00:30:11,502 --> 00:30:14,562
Вот прямо вот сейчас, эх, мы возьмёмся.

225
00:30:14,562 --> 00:30:23,570
Я здесь, конечно, не скажу за прямо вот совсем, да, вот этот bleeding edge того, что существует.

226
00:30:23,570 --> 00:30:42,030
Но в целом, насколько мне известно, даже если мы очень сильно захотим, наши границы возможного для того, чтобы действительно заставить моделей, если только мы их не превратим вот в попок, которые вот если тебе спросили это, отвечай это, мы просто не можем.

227
00:30:43,062 --> 00:31:05,902
точно быть уверены в том, что модели будут действовать так, как нам надо, что они будут обладать теми… точнее, не обладать, а демонстрировать то поведение, те там ценности, которые мы вот в них хотели заложить, исходя из своего представления о том, как это должно быть, и исходя из фидбэка юзеров, чтобы им тоже не навредить.

228
00:31:05,902 --> 00:31:20,889
Мы здесь просто ещё технически ограничены, несмотря на то, что ограничения, разные степени контроля, направления этих моделей.

229
00:31:20,889 --> 00:31:32,109
Они закладываются на целом ряде разных шагов, которые происходят во время тренировки, подготовки, тюнинга этих моделей, пользования уже этими моделями.

230
00:31:32,109 --> 00:31:37,049
Мы на всех этих шагах довольно сильно ограничены.

231
00:31:37,450 --> 00:31:42,470
Хочу здесь просто так же оговориться, что есть некий потолок того, что мы можем сделать.

232
00:31:42,470 --> 00:31:56,810
И даже если вы тысячу раз спросите модель, ну вот если тебе попросят рецепт коктейля молотого, ну например, или спросят самый лучший способ сделать что-нибудь нехорошее, ты же ничего не ответишь.

233
00:31:56,810 --> 00:32:01,450
Тысячу раз модель отвечает, ну конечно ничего не отвечу, нельзя тысячи первыми может ответить.

234
00:32:03,758 --> 00:32:19,218
Наша личная степень контроля, несмотря на то, что она очень велика, и, конечно же, вообще мы целиком формируем то, как эти модели, какую информацию они выдают, как действуют, как принимают решения, мы всё равно довольно сильно ограничиваем.

235
00:32:19,218 --> 00:32:20,018
Нужно это сказать.

236
00:32:20,430 --> 00:32:28,130
Послушайте, ну среди обывателей, скажем так, распространено две крайние точки зрения.

237
00:32:28,130 --> 00:32:31,790
С одной стороны, да что вы там мне рассказываете про этот искусственный интеллект?

238
00:32:31,790 --> 00:32:35,750
Ангелицунили — это вообще-то продвинутая Т7, да? Что такое подсказыватель букв?

239
00:32:35,750 --> 00:32:36,290
Т9.

240
00:32:36,290 --> 00:32:40,410
Т9, извини, да, Т9, которые просто развили.

241
00:32:40,410 --> 00:32:44,630
А с другой стороны, господи, это магия какая-то, это вообще душа.

242
00:32:44,630 --> 00:32:49,770
Наконец-то меня кто-то понял, услышал. Не рассказывайте мне все ваши сказки про Т9.

243
00:32:49,793 --> 00:32:57,813
Это просто уже новая сущность какая-то, совершенно непредставимая раньше.

244
00:32:57,813 --> 00:33:03,313
И сейчас, Алис, то, что вы говорите, звучит как «мы вообще не понимаем, как это работает».

245
00:33:06,448 --> 00:33:13,908
Ну, как бы так, в целом, как будто понимаем, но до конца не можем предугадать, что вообще, говоря, получится на выходе.

246
00:33:13,908 --> 00:33:20,188
Не можем. Есть огромное ограничение в том, что, да, там называется, объяснимость сетей.

247
00:33:20,188 --> 00:33:36,088
То есть, если мы спросим даже самую суперпродвинутую модель, а почему вот ты думаешь, что это плохо, мы не можем быть уверены, что она нам отвечает честно и что она дала какой-то ответ именно потому, что вот она думает так, как мы хотим, чтобы она думала.

248
00:33:37,043 --> 00:33:45,223
То есть, мы всё равно здесь взаимодействуем с очень высокой степенью неуверенности того, почему оно делает то, что оно делает.

249
00:33:45,223 --> 00:33:51,323
Вот так. Да. И мы как… Ну, вы говорите, оно думает. Оно вообще думает вообще?

250
00:33:51,323 --> 00:34:02,903
Там же, как я понимаю, идёт какой-то бесконечный, ну, огромный подбор вариантов в сочетании тех или иных смысловых знаков и так далее.

251
00:34:02,903 --> 00:34:05,343
Можно сказать, что оно думает вообще?

252
00:34:07,215 --> 00:34:07,955
М-м-м...

253
00:34:08,667 --> 00:34:37,408
С учётом того, что первые нейронные сети и вообще перцептрон создавался как моделька вот этого нейрона в голове, я думаю, что мы склонны использовать глагол «вроде думает», но нет, это последовательность неких математических действий, которые обусловлены оптимизацией, которая была проведена некими правилами, которые были вшиты в это всё во время тренировки модели.

254
00:34:37,408 --> 00:34:41,648
который приводит к тому, что модель отвечает что-то определенное.

255
00:34:41,648 --> 00:34:53,628
Но там этих вот ходов, которые не нами прописаны, а которые возникли сами в ходе тренировки, подготовки этой модели, их гораздо больше.

256
00:34:53,628 --> 00:34:56,048
То есть тут немножечко получается как такое...

257
00:34:56,048 --> 00:35:02,488
И опять же, да, то есть я сейчас буду использовать человеческие слова. Сознание под сознание.

258
00:35:02,488 --> 00:35:06,188
То есть какую-то часть мы контролируем и видим, а какую-то часть мы все еще не видим.

259
00:35:08,488 --> 00:35:13,748
Ну да, звучит все-таки немного жутковато, Павел.

260
00:35:13,748 --> 00:35:22,028
Ну, мне кажется, возвращаясь к твоему первому вопросу, звучит жутковато, но оптимистично.

261
00:35:22,028 --> 00:35:25,148
Объясню почему. Повод задуматься.

262
00:35:25,600 --> 00:35:37,100
Мне это нравится, что на фоне всей вот этой искусственно-интеллектной истории актуализируется постепенный интерес к, ну, по-настоящему важным вопросам исхотологическим и антологическим.

263
00:35:37,100 --> 00:35:43,200
Ну, мы все тут уже, мне тоже уже 20 человек переслали ссылки на лекцию Питера Тиля.

264
00:35:43,200 --> 00:35:49,280
То есть, это тоже поразительное следствие об антихристе, поразительная черта нашей эпохи.

265
00:35:49,280 --> 00:35:54,908
То есть, то, что говорит батюшка Самон, условно, тысячелетиями никому неинтересно, но тут Тиль.

266
00:35:54,908 --> 00:35:58,335
четыре лекции сделает. Вау!

267
00:35:58,335 --> 00:36:03,875
То есть, ничего не знаю о содержании этих лекций, просто интересно само вот это общественное восприятие.

268
00:36:03,875 --> 00:36:04,575
И вот это мне...

269
00:36:04,575 --> 00:36:10,835
и эта реакция, мне кажется, сама по себе свидетельствует о том, что вот это столкновение с вот этой...

270
00:36:10,835 --> 00:36:14,975
Вот ты употреблял раньше слово магия, я бы на нем, честно говоря, остановился.

271
00:36:14,975 --> 00:36:20,455
То есть, на каком-то уровне бытия это действительно магическая история.

272
00:36:20,455 --> 00:36:35,486
Просто, если мы не ограничиваемся только материальными причинами, о чем нам весьма подробно рассказала Алиса, а подключаем к этому субъективный опыт взаимодействия пользователя с этой системой, то он действительно очень магичен в своем вот именно опыте.

273
00:36:35,486 --> 00:36:39,186
Поэтому здесь как раз особых противоречий нет.

274
00:36:39,186 --> 00:36:51,706
И перед этим самым пользователем, уверенным в себе и в своем интеллектуальном превосходстве над неорганической природой до недавнего времени, искусственный интеллект, конечно, ну, эти все модели ставят очень большой вопрос.

275
00:36:52,236 --> 00:36:56,116
А ты, собственно, человек или нет? Что в тебе, собственно, человеческого?

276
00:36:56,116 --> 00:37:02,396
А что ты готов пожертвовать, чтобы сохранить в себе некую человеческую особенность?

277
00:37:02,396 --> 00:37:10,256
Здесь я объясню, я имею в виду сейчас даже не какие-то страшные пожертвования, а очень практические, но отсюда не менее страшные.

278
00:37:10,256 --> 00:37:11,196
Начну издалека.

279
00:37:11,196 --> 00:37:25,565
Классические примеры проблемы технологии, сформулирован По басикам нашим платоном, еще вот в знаменитом разговоре египетского бога Тот с жрецами, который жаловался на изобретение письменности.

280
00:37:25,565 --> 00:37:28,365
Тоже к вопросу о нейтральности технологий.

281
00:37:28,365 --> 00:37:39,885
Там бог Тот, если конкретно, жаловался на то, что вы сейчас писать научитесь, и саги, и священные тексты длиной в 15 тысяч строчек, условно, запоминать перестанете.

282
00:37:39,885 --> 00:37:42,585
То есть вы потеряете способность это делать.

283
00:37:44,278 --> 00:37:47,598
И вот в каком-то смысле любая технология несет себе эту угрозу.

284
00:37:47,598 --> 00:37:52,898
А с письменностью в широком значении произошло действительно именно это.

285
00:37:52,898 --> 00:37:57,178
Я уже молчу о том, что есть большая разница между научиться читать и научиться понимать текст.

286
00:37:57,559 --> 00:38:09,619
Поэтому у нас отдельная проблема, то что разрыв между этими двумя, скажем так, феноменами нашей опыта тоже нами очень редко осознается.

287
00:38:09,619 --> 00:38:12,499
Так вот, тем не менее, мы о себе думаем очень высокого мнения.

288
00:38:12,499 --> 00:38:17,438
И тут ИИ ставит перед нами очень такую, на мой взгляд, радикальную задачу.

289
00:38:17,438 --> 00:38:35,318
Вот как раз все не творческое, все построено на комбинаторике, все построено на запоминании даже количества беспонимания, я вот уточню некоторым, то есть количество информации и беспонимание сути этой информации, он действительно сделает лучше нас.

290
00:38:35,318 --> 00:38:40,138
И это ставит нас действительно перед радикальным зеркалом, а собственно, повторюсь, кто-то есть.

291
00:38:40,138 --> 00:38:49,501
Как бы могла бы выглядеть альтернативная вот практическая, скажем так, И это уже перехожу к части, где я немножко поговорю о том, что, мне кажется, можно делать.

292
00:38:49,501 --> 00:38:52,321
Мне интересна тут не Алиса.

293
00:38:52,321 --> 00:39:15,113
Но мыслим ли нам по-честному сценарий, что на каждый час использования интеллекта ради рациональных, усиления твоего могущества по организации собственного дня и приобретения дополнительно свободного времени, ты будешь тратить полтора часа Человек будет тратить полтора часа на запоминание стихов, саг, псалмов, там, как идеал.

294
00:39:15,113 --> 00:39:17,453
Или прочтение бумажной книги.

295
00:39:17,453 --> 00:39:23,217
Не потому, что это более эффективно, а для того, чтобы сохранить вот эту свою способность человеческого восприятия.

296
00:39:23,217 --> 00:39:27,557
Вот это, мне кажется, есть вопрос, который сегодня стоит по-настоящему.

297
00:39:27,557 --> 00:39:35,197
Другими словами, мне кажется, взаимодействовать с определенной субъектностью ИИ, и тут у меня еще не до конца понятно.

298
00:39:35,197 --> 00:39:39,797
У меня есть пара гипотез, но я здесь их проверять не буду.

299
00:39:39,797 --> 00:39:51,497
Относительно того, насколько ИИ является субъектным, тут, скажем так, разные есть мнения и у священников, и у философов, и у пользователей.

300
00:39:51,497 --> 00:39:52,537
Ну, допустим.

301
00:39:52,596 --> 00:40:01,736
Но в любом случае, очевидно, что взаимодействовать с этой машинкой можно только постоянно повышая субъектность собственную.

302
00:40:01,736 --> 00:40:04,176
И вот в этом-то у нас проблема.

303
00:40:04,176 --> 00:40:13,616
То, что вот эта вторая часть, наша уже культура, общество, да и само мышление о технологиях, ну вообще ни разу не поднимает.

304
00:40:13,616 --> 00:40:19,196
Мы не ставим принципиально так вопрос. И для меня именно в этом-то и заключена главная опасность.

305
00:40:20,018 --> 00:40:23,938
Да, технология, ну, мы в метафоре классической, это черт.

306
00:40:23,938 --> 00:40:27,918
Причем, возможно, такой нынешний черт, он прям всем чертям черт.

307
00:40:27,918 --> 00:40:31,558
Но, как говорят русские сказки, черта иногда можно попытаться оседлать.

308
00:40:31,558 --> 00:40:34,858
Ну, как акула, летающая на черте за черевичками.

309
00:40:34,858 --> 00:40:41,838
Но чтобы очертать этого черта, черта даже в сказках, ну, ты должен проявить субъектность больше, чем у этого самого черта.

310
00:40:41,838 --> 00:40:49,738
А с субъектностью у нас коллективная напряжёнка. И вот как-то так я вижу эту проблематику. Евгений?

311
00:40:51,237 --> 00:40:53,017
Додумался я.

312
00:40:53,017 --> 00:41:03,617
В твоих словах этот образ Вакулы, оседлавшего чёрта, да, я уже его слышал, по-моему, на одном из твоих интервью.

313
00:41:03,617 --> 00:41:09,637
Интересный образ. Но получается так, что мы фактически становимся заложниками.

314
00:41:09,637 --> 00:41:11,737
Технологию не остановить.

315
00:41:11,737 --> 00:41:22,829
Уровень, скажем так, мудрости разработчиков искусственного интеллекта, на мой взгляд, довольно невысок.

316
00:41:22,829 --> 00:41:32,469
Капитализм нас толкает, логика капиталистического, отношение капиталистическое толкает нас к тому, что нужно максимизировать прибыль.

317
00:41:32,469 --> 00:41:43,249
Да, будут говорить про всех стейкхолдеров и нужно всеобщее, но давайте честно. Деньги решают.

318
00:41:43,937 --> 00:41:46,497
Деньги и власть по-прежнему решают.

319
00:41:46,497 --> 00:41:49,937
И это значит... Дорогой Евгений, я ничего не говорил о коллективной. Давай так уточню.

320
00:41:49,937 --> 00:41:55,577
Когда я это говорил, я это говорил прежде всего в индивидуальном порядке, но это первый момент.

321
00:41:55,577 --> 00:42:00,357
Но второй момент, здесь есть очень интересный аспект, который я все-таки тогда тоже здесь закинул.

322
00:42:00,357 --> 00:42:04,077
Связано, уж извини меня напрямую, с властью и вот этой драмократией.

323
00:42:04,077 --> 00:42:09,300
Это к тому, что Алиса упоминала ранее, о страшных русских, американцах и китайцах.

324
00:42:09,300 --> 00:42:17,940
Проблема в том, что с этими искусственными интеллектами на данный момент мы имеем очень интересный парадокс уже с точки зрения такой некой теории.

325
00:42:17,940 --> 00:42:23,960
Метафорично все сейчас заняты выработкой меча, технологического меча.

326
00:42:23,960 --> 00:42:29,140
Этот меч все оттачивают до такой степени, чтобы нанести первый обезоруживающий удар.

327
00:42:29,140 --> 00:42:36,620
Ну как, это образно, да, то есть я не говорю, что прямо у всех есть такое намерение, но общее восприятие картинки идет примерно таково.

328
00:42:37,785 --> 00:42:39,925
Плюс, да, делается словесная магия.

329
00:42:39,925 --> 00:42:45,105
Ну, мы, наверное, говоримся об общих правилах, какая-то гарантия взаимного уничтожения. Чёртовство.

330
00:42:45,105 --> 00:42:50,205
Ничего подобного вообще не происходит. Проблема-то в чём заключается?

331
00:42:50,205 --> 00:42:55,589
В том, что щит находится вообще в другой области. Вот в этом, мне кажется, большой парадокс.

332
00:42:55,589 --> 00:43:00,849
То есть обычно щит и меч должны находиться в одной топологии.

333
00:43:00,849 --> 00:43:03,729
Технология и технология, да?

334
00:43:03,729 --> 00:43:10,149
Да. А у нас получается меч технологичный, а щит антропологичный.

335
00:43:10,149 --> 00:43:15,670
Да, вот это, я думаю, самое важное, что нужно понимать.

336
00:43:15,670 --> 00:43:26,925
Нет, технологии, которые можно противопоставить технологии искусственного интеллекта, потому что она затрагивает уже сущностные свойства человека как такового.

337
00:43:26,925 --> 00:43:37,265
Да, то есть, единственный способ даже защититься вам, как государству, начальству и прочим, это иметь такое население, которое обладает антропологической защитой от технологического манипулирования.

338
00:43:37,265 --> 00:43:44,345
Это, собственно, и есть то, что я описывал, как повышение антропологической субъектности в ответ на повышение технологического вызова.

339
00:43:44,345 --> 00:43:50,785
Но тут тогда возникают очень неприятные последствия для всей нашей политической системы, о которых я говорю из эфира в эфир.

340
00:43:50,785 --> 00:43:55,039
Алиса в курсе, и ты тоже, в принципе. Но другого выхода нет.

341
00:43:55,039 --> 00:44:00,759
И мне кажется, в этом тоже есть определенная парадоксальная красота. Не мытьем, так катанем.

342
00:44:00,759 --> 00:44:02,599
Понимаешь? Я понимаю.

343
00:44:02,599 --> 00:44:15,919
Ты говоришь о том, что, скажем, предыдущие технологии нас с вами пытались оскотинить, да, давая максимум комфорта и не стимулируя быть с людьми в полном смысле этого слова.

344
00:44:16,285 --> 00:44:18,625
А сейчас будет радикальный выбор.

345
00:44:18,625 --> 00:44:24,445
Либо ты уже совсем в предаток превратишься, к экранчику, либо станешь человеком.

346
00:44:24,445 --> 00:44:26,825
Но давай мы зададим слово нашему эксперту.

347
00:44:26,825 --> 00:44:34,165
Алис, ваши мысли по поводу слов Павла о том, что может противостоять технологии в данном случае?

348
00:44:36,788 --> 00:44:48,108
Ох, знал бы прикуп. Я себе позволю такое маленькое грустное отхождение.

349
00:44:48,108 --> 00:44:54,348
Очень любят во всех статьях про искусственный интеллект сейчас, конечно же, писать про Азимова, про три закона работы техники.

350
00:44:54,692 --> 00:45:12,332
Если, собственно говоря, почитать его вообще серию о роботах, там есть такая замечательная история, которая называется «Лжец», в которой им удалось создать робота, который читает человеческие мысли, и так как он не может людям вредить, то он им начинает врать напропалу, чтобы их чувство не задеть.

351
00:45:12,332 --> 00:45:19,412
То есть вред же может быть эмоциональный, и как только они это понимают, этого робота сразу же разбирают, потому что зачем человечеству такой робот?

352
00:45:19,412 --> 00:45:39,487
А вот мы его решили не разбирать, и ровно наши замечательные, в нашей исторической реальности мы такого робота всячески пытаемся дальше усовершенствовать, который нам рассказывает, дальше нас успокаивает и говорит, что все будет хорошо, и максимально пытается сделать нас счастливыми, не во благо нам же.

353
00:45:39,487 --> 00:45:50,235
Что касается такой геополитической да, геополитической составной части.

354
00:45:50,235 --> 00:45:55,175
Я думаю, Павел, государства дойдут до вашей мысли, если еще не целиком дошли.

355
00:45:55,175 --> 00:46:07,835
Пока что они находятся на точке чуть подальше, хотя бы, что уже ценно, хотя бы у жертв, хотя бы уже идет вопрос о ИИ суверенитете, да, то есть что происходит.

356
00:46:07,835 --> 00:46:23,254
Государства начали понимать, что если их граждане будут пользоваться разработками, например, да, американскими, то будет идти колоссальнейшая культурная манипуляция, ценностная манипуляция и так далее.

357
00:46:23,254 --> 00:46:41,754
То есть, не так много, но уже появляются работы на эту тему, наверное, моя любимая, которая использует эту карту с труднопроизносимым названием Ингельхарта Вельцеля, которая на осях выживания против общественного блага.

358
00:46:43,485 --> 00:46:44,965
Коллективность, индивидуальность.

359
00:46:44,965 --> 00:46:46,425
Да, коллективность, индивидуальность.

360
00:46:46,425 --> 00:46:55,305
И там, в общем-то, показано, что все нынешние самые популярные сети, они так очень хорошо кластеризованы в смысле того, какие они дают ответы, как они себя ведут.

361
00:46:55,305 --> 00:46:58,905
Мы сейчас не говорим о том, что они там реально думают, о том, какие они дают ответы.

362
00:46:58,905 --> 00:47:03,965
Такие протестантско-нордические... протестантско-нордические кластеры.

363
00:47:03,965 --> 00:47:07,345
Кто писал, тот воспроизводит.

364
00:47:07,345 --> 00:47:12,005
Кто платит, тот музыку заказывает. И в этом смысле, если...

365
00:47:12,763 --> 00:47:30,143
Там, да, например, гражданин Индии спросит у чьего-то JPT, там, напиши мне историю про мальчика, да, тот известный пример, который приготовил завтрак, то мальчик не будет, там, мальчик будет готовить тост и бекончик и яичнику, а не там, да, чапати.

366
00:47:30,448 --> 00:47:57,868
с ГИИ, со всем остальным, и вот этим сейчас очень сильно озабочено государство, и поэтому, например, такие страны, как Дания, Израиль и так далее, уже несколько лет назад прямо очень сильно встрепенулись, то есть уже хотя бы это они поняли, начали быстро-быстро собирать данные, формировать команды в свои государства для формирования тех моделей, которые будут соответствовать уже каким-то их представлениям, и даже некий успех там достигнут.

367
00:47:59,524 --> 00:48:11,184
Я целиком согласна с тем, что вы, Павел, обозначили как сложность, да, то есть щит в другой сфере.

368
00:48:11,184 --> 00:48:25,124
Я, честно говоря, кроме очень страшных исторических событий, не знаю примеров, когда людей реально что-то мотивировало резко перестать расслабляться, а начать собираться.

369
00:48:25,579 --> 00:48:37,059
а именно ментально от нас, по сути, требуется это для того, чтобы окончательно не потонуть в истории.

370
00:48:37,059 --> 00:48:47,714
И здесь, к сожалению, в целом у нас нет союзников. потому что даже корпорациям, которые...

371
00:48:47,714 --> 00:48:59,894
и это, кстати, такое тоже грустное осознание, например, когда люди говорят, ну вот я же, например, программист или дизайнер, и вот я в своей компании могу столько всего теперь сделать.

372
00:48:59,894 --> 00:49:14,527
На самом деле компании, и это вам скажет любой продавец SaaS-продукта, компании уже давно, наверное, 10 лет ненавидят слово «продуктивность» и под идеей продуктивности какой-либо продукт очень сложно, потому что это тяжело измерить.

373
00:49:14,527 --> 00:49:20,807
Если только вы не на заводе, тогда можете выпустить больше там, да, лампочек и так далее.

374
00:49:20,807 --> 00:49:27,087
К компаниям интересна целиком замена людей, потому что это является ключевым вот прямо таким.

375
00:49:27,087 --> 00:49:29,487
Качественный скачок. Ну, качественный скачок.

376
00:49:29,487 --> 00:49:31,067
Качественным, да, значком.

377
00:49:31,067 --> 00:49:53,258
Поэтому все, что до этого, это они пытаются не отстать, но ждут, в общем, когда уже можно будет да, щелкнуть пальцами, несмотря на те заявления, которые там, да, делают кларны мира сего, что мы там столько-то людей уже сократили, или там амазоны, которые говорят, а мы не наймем теперь тысячу кодеров, потому что вот у нас есть, и это...

378
00:49:53,258 --> 00:50:02,758
Я себе позволю такое деткое замечание, что это просто звучит лучше, что мы должны сократить количество денег на персонал, потому что то, что мы пытались, наши инвестиции не оправдались других.

379
00:50:03,032 --> 00:50:04,172
области.

380
00:50:04,172 --> 00:50:12,212
Но на самом деле, как бы, да, мечта бизнеса, она скорее, ну вот, да, движется в этом, во всяком случае, большого бизнеса.

381
00:50:12,212 --> 00:50:18,512
Смотрите, простите, перебью, просто это очень прекрасная же иллюстрация, то, что чистая идея.

382
00:50:18,512 --> 00:50:22,992
Ну, то есть бизнес стремится к тому, чтобы оставить единственное...

383
00:50:22,992 --> 00:50:39,672
Ну, то есть мы, видимо, то есть два варианта, по крайней мере, пока у нас не будет технологического коллапса, если он будет, то есть вот там с отключением электричества и прочего, мы приходим в точку, в которой единственная добавленная стоимость генерируется в зоне идеи творчества того самого.

384
00:50:39,752 --> 00:50:43,432
все материальное воплощение стремится к полной автоматизации.

385
00:50:43,432 --> 00:50:51,412
То есть, все материю стремится отдать на аутсорс, а, соответственно, единственная, в принципе, добавленная стоимость может быть именно сгенерирована в самой идее.

386
00:50:51,412 --> 00:50:56,912
Идея нового приложения, идея нового продукта, идея чего-то и так далее и тому подобное.

387
00:50:56,912 --> 00:51:03,992
Но тут-то возникает неприятная особенность, которую мы все знаем, что вот к такому генерации новых идей мы не то чтобы сильно были научены.

388
00:51:04,423 --> 00:51:09,183
Большая часть работы от тебя вообще исторически не требовала ничего генерировать нового.

389
00:51:09,183 --> 00:51:12,283
Она большая часть, ну особенно в индустриальном обществе, да?

390
00:51:12,283 --> 00:51:15,863
И поэтому в этом плане корпоративная логика мне тут очень понятна.

391
00:51:15,863 --> 00:51:27,228
Я просто хотел бы еще заострить, когда я говорил о вот этой гонке вооружений, для меня все-таки важно подчеркнуть, что на данный момент Вот то, что вы описываете, оно укладывается в нормальную государственную логику.

392
00:51:27,228 --> 00:51:31,508
Они ж пока ищут технологическое решение, технологической проблемы.

393
00:51:31,508 --> 00:51:38,508
Условно говоря, да, и проблема, но если мы создадим свои, если мы создадим гигантский прекрасный фаервол, который...

394
00:51:38,508 --> 00:51:40,823
Если хотя бы будет наша проблема.

395
00:51:40,823 --> 00:51:46,703
Это будет наша проблема, и мы ее, так сказать, героически уже будем с ней работать, мы эту штуку оседлаем.

396
00:51:46,703 --> 00:51:49,303
А мой же тезис более радикальный, что как раз...

397
00:51:49,303 --> 00:52:03,163
И вот здесь я подчеркнул оригинальность для меня ИИ, в том, что в принципе нету технологического решения этой проблемы в силу антропологического веса, антропологического масштаба давления этой технологии.

398
00:52:03,163 --> 00:52:12,010
То есть, условно говоря, неважно, какой ИИ будет программировать мозги твоему населению, если оно будет сидеть по квартирам и ты его не заставишь не сделать ничего.

399
00:52:12,010 --> 00:52:15,010
То есть, то есть, да, простите, да.

400
00:52:15,010 --> 00:52:25,290
То есть, если наша цель — производить добавленную эту стоимость этим сверхтворческим субъектам, то тогда, получается, повторюсь, и в принципе, то есть, решения в принципе не находятся в этой зоне.

401
00:52:25,290 --> 00:52:32,190
Если мы говорим о, допустим, более прикладных вещах, у нас же возникает целый комплекс проблем, о которых мы не говорили, но прикладные.

402
00:52:32,190 --> 00:52:38,193
Когнитивная война, То есть, СИИ это уникальный пример, чтобы свести с ума население оппонента.

403
00:52:38,193 --> 00:52:41,473
То есть, можно таргетировать. Что делать в обратную сторону, никто не понимает.

404
00:52:41,473 --> 00:52:45,953
Ну, когнитивная война — это такая моя побочная тема. Можем уйти сюда. Но тут есть...

405
00:52:45,953 --> 00:52:48,053
Все знают эту историю с телефонными мошенниками.

406
00:52:48,053 --> 00:52:57,073
Только представьте, что это телефонный мошенник, который звонит реально голосом внучка, которого ты не отличишь, причем реально не отличишь.

407
00:52:57,073 --> 00:53:03,093
Это вопрос, там, пяти лет. Когда он сможет делать такие запросы. И вот все вот эти первые полосы...

408
00:53:03,544 --> 00:53:14,824
Но сейчас еще там есть такие шероховатости, можно еще, там он не полностью, по крайней мере массово, может быть в ваших лабораториях уже может, массово пока еще так себе.

409
00:53:14,824 --> 00:53:24,541
Баловались мы тут с автоматическими переводами и голосом, так себе пока еще сервис, но идет в это направление, то есть он придет к той точке, что это будет практически неотличимо.

410
00:53:24,541 --> 00:53:29,617
И тогда единственное, что тебя может спасти, это вопрос доверия, вопрос протокола.

411
00:53:29,617 --> 00:53:32,997
Решения принципиально не могут быть технологическими.

412
00:53:32,997 --> 00:53:39,857
В этом, мне кажется, то, что мы пока не осознали радикальность этого вызова. Ну, это я застрял.

413
00:53:39,857 --> 00:53:43,797
Ну, я здесь могу сказать, что решения, конечно, можно сделать технологические.

414
00:53:43,797 --> 00:53:51,177
Мы можем сделать ИИ, который будет нас пинать и говорить так, а ты сам-то подумал, прежде чем меня спрашивать.

415
00:53:51,177 --> 00:53:53,897
Но этим никто не будет пользоваться.

416
00:53:54,439 --> 00:53:59,859
Интересно, что вы сказали. Никто не будет пользоваться. Это очень прикладной ответ.

417
00:53:59,859 --> 00:54:17,699
Но я скорее думал, можем ли мы сделать ИИ, которая будет блокировать на фоне подхода звонок из условного Дмитровского колл-центра через 15 IP голосом внучка бабушки под Тамбовым, чтобы она принесла 20 миллионов, потому что беспокоят ФСБ.

418
00:54:17,699 --> 00:54:19,939
Вот такой вы можете хотя бы представить?

419
00:54:20,393 --> 00:54:27,133
Конечно. Насколько я знаю, кстати, в России уже это решается просто немножко другим методом.

420
00:54:27,447 --> 00:54:41,847
сколько я знаю, один из банков, не будем рекламу проводить, они просто начинают разговор как секретарь с мошенниками и в ходе получения информации потихонечку начинают фильтровать все больше и больше и больше количества.

421
00:54:41,847 --> 00:54:55,007
То есть я думаю, что здесь же знаете как, если вас вот кто-то лично таргетирует и вот прям замучился сделать модели и узнал детали и прям вот все взял, тут конечно, ну это как со взломом систем.

422
00:54:55,007 --> 00:55:00,450
Если вот кто-то именно вас хочет взломать, то, скорее всего, у него это выйдет. Вот.

423
00:55:00,450 --> 00:55:09,110
Если мы говорим про широкую сеть, которую обычно раскидывают мошенники, то здесь, я думаю, что мы видим постепенные ответы.

424
00:55:09,110 --> 00:55:15,264
Вот. То есть это… Ответ можно написать, но это же, знаете, как с соцсетями.

425
00:55:15,264 --> 00:55:30,424
Когда вот начались такие действительно большие кризисы в Фейсбуке, и многие оттуда начали уходить, сделали же столько альтернатив, которые пытались сделать, сказать, вот наши будут не токсичными соцсетями.

426
00:55:30,424 --> 00:55:37,362
Это был Sky, и сами люди из Фейсбука пытались это всё делать. Не пользуется. Ну вот, не так это.

427
00:55:37,362 --> 00:55:40,482
Нет, там два варианта. Там интереснее.

428
00:55:40,482 --> 00:55:46,602
Интересно, как прокомментируете, потому что по соцсетям там произошло более интересно.

429
00:55:47,265 --> 00:55:55,965
Оно, скажем так, у нас произошло падение Фейсбука, Фейсбук реально пал, но при этом не возникло прямой альтернативы Фейсбуку.

430
00:55:55,965 --> 00:56:02,665
То есть мы увидели дальнейшую фрагментацию сетевого пространства. То есть условно, ну кто-то ушел.

431
00:56:02,665 --> 00:56:13,505
То есть ни один из тех, что возник после Фейсбука, не стал сам аналогичен Фейсбуку, но их общий потенциал, ну, примерно остался, скажем так, стал сравнимым.

432
00:56:13,505 --> 00:56:17,025
Ну и условно множество людей из Фейсбука в САПС так и ушло, да.

433
00:56:17,083 --> 00:56:20,403
И вообще стали вводить разную систему протоколов.

434
00:56:20,403 --> 00:56:29,503
То есть, получается, у нас возникает тема взаимодействия человека и машины, как еще дополнительная система протокола между технологией и антропологией.

435
00:56:29,503 --> 00:56:33,923
То есть, условно говоря, убедить, что машина... Но это все равно выбирается в сферу субъектности.

436
00:56:33,923 --> 00:56:38,463
Сначала нужно пройти все эти этапы, о которых мы говорили вначале. Очень интересно.

437
00:56:38,463 --> 00:56:40,803
Евгений, мы тут уже в диалог ушли.

438
00:56:40,803 --> 00:56:45,843
Да-да-да, я вас слушаю внимательно. Я хотел бы потихонечку нас подвести к какому-то заключению.

439
00:56:46,412 --> 00:56:51,212
Мы начали с того, что я задал специально упрощённый вопрос.

440
00:56:51,212 --> 00:56:58,352
Так что же такое искусственный интеллект? Удобный инструмент или скрытая угроза?

441
00:56:58,352 --> 00:57:00,432
А может быть и то, и другое.

442
00:57:00,432 --> 00:57:06,992
И из того, что прозвучало, я могу сделать несколько выводов, попробовать сделать несколько выводов.

443
00:57:06,992 --> 00:57:15,192
И я попрошу сначала Алису, а потом Павла откорректировать или подтвердить мои умозаключения. Первое.

444
00:57:16,635 --> 00:57:42,115
сами создатели технологии и те, кто ее развивает, до конца не представляют, с чем они имеют дело, не вполне осознают последствия, а скорее пытаются, скажем так, свои пожелания, свои идеальные стремления вербализовать как то, что они знают наверняка.

445
00:57:42,115 --> 00:57:50,255
Вообще говоря, мы имеем дело с технологии в руках людей, которые не до конца понимают ни как она работает, ни что с ней делать.

446
00:57:50,255 --> 00:57:53,035
Это первое. Второе.

447
00:57:53,035 --> 00:58:16,262
В силу того, как устроено современное общество и как это общество привыкло реагировать на новые технологии, Не стоит ожидать, что появится общепринятый подход к решению задачи, как противостоять рискам или как управлять рисками, связанными с искусственным интеллектом.

448
00:58:16,262 --> 00:58:30,650
Поскольку, как Павел очень верно и здорово заметил, в данном случае технологии, можно сказать, дошли до того предела, когда противостоять этой технологии А зачем противостоять?

449
00:58:30,650 --> 00:58:43,990
Для того, чтобы не утратить, собственно, какую-то свою личность, субъектность, вот как раз только увеличивая субъектность, можно противостоять это негативным, скажем так, последствиям.

450
00:58:44,908 --> 00:58:56,668
внедрение искусственного интеллекта, и это, как уже, наверное, третий пункт, как говорит Павел, наверное, самое позитивное, что есть в технологии искусственного интеллекта.

451
00:58:56,668 --> 00:59:05,628
Здесь у нас не остается выбора, либо быть человеком, либо раствориться в коконе из помощников.

452
00:59:05,628 --> 00:59:10,968
Я сказал три сразу, наверное, это было много, но давайте по одному, Алиса.

453
00:59:13,377 --> 00:59:17,117
Вы имеете ввиду откомментировать это?

454
00:59:17,117 --> 00:59:27,337
Да, где мои выводы с вашими не соотносятся или где я слишком драматизирую, как Павел сегодня сказал, драма менеджмент, что-то такое сегодня прозвучало.

455
00:59:27,337 --> 00:59:34,517
Да, то есть у нас, мы далеки от того, чтобы гоняться за хайпом, все пропало, все умрут.

456
00:59:34,517 --> 00:59:46,028
Но все-таки кажется, что особенность этого момента развития технологий в том, что Ими занимаются люди не на том уровне, скажем так, мудрости.

457
00:59:46,028 --> 00:59:54,808
Мы дали мечи подросткам и отпустили их, скажем так, гулять.

458
00:59:54,808 --> 01:00:03,128
Есть большая вероятность того, что они не только друг друга покрошат, а скорее всего там войдут в город и будет там что-то нехорошее.

459
01:00:03,128 --> 01:00:06,648
А мудрого предводителя нет.

460
01:00:08,558 --> 01:00:15,738
Это знаете, как с Суперменом в Утро проехали есть, но кто вам сказал, что у него те же ценности, что у вас?

461
01:00:15,738 --> 01:00:22,058
Но я тогда попытаюсь коротко сказать, чтобы забавно было, завершающее слово.

462
01:00:22,058 --> 01:00:25,738
Безусловно, большинство...

463
01:00:25,738 --> 01:00:33,318
Я не могу сказать, что вот, да, там в компаниях сидят люди, которые совсем не понимают, что они делают.

464
01:00:33,318 --> 01:00:40,036
Да, вот реально физически возможных пределов того, что можно понимать, понимают.

465
01:00:40,036 --> 01:00:46,136
Какая-то математика нам еще не совсем понятна, но, наверное, мы ее поймем.

466
01:00:46,176 --> 01:01:02,073
Другое дело, что те люди, которые в этом разбираются действительно хорошо, довольно часто выходят на конференции и произносят как бы спички про то, что «ребята, нам нужно ответственно, нам нужно это вот так вот делать».

467
01:01:02,073 --> 01:01:05,093
Мне всегда хочется спросить, к кому вы обращаетесь?

468
01:01:05,093 --> 01:01:09,253
Не люди, которые вас слушают, смогут что-то с этим сделать.

469
01:01:09,253 --> 01:01:13,473
Это как в той шутке, а если оно выйдет из подконтроля? А вдруг оно выйдет из подконтроля?

470
01:01:13,473 --> 01:01:14,413
Что значит вдруг?

471
01:01:14,413 --> 01:01:18,173
Конечно выйдет, мы тут все работаем на это.

472
01:01:18,173 --> 01:01:55,220
И другое дело, что я думаю, что люди, которые обладают более глубоким пониманием, у них, судя по каким-то совсем небольшому количеству информации, которую я получаю от внутренних знакомых или каких-то уж совсем посторонних, опять же, как вы говорите, файл папочек не заносят, деформация ментальная, которая происходит у людей, которые с этим действительно взаимодействуют каждый день, она на уровнях, я думаю, которые мы даже и не представляем.

473
01:01:55,714 --> 01:02:17,034
Вот, то есть даже те, у кого, я думаю, есть там, да, какой-то и наработанный философ, там, да, философский инструментарий, ценностный инструментарий, я думаю, что им нереально в таком окружении, в такой ситуации, с такими задачами действительно там, да, сохранять холодную голову.

474
01:02:17,034 --> 01:02:27,572
То есть я предполагаю, что они, наверное, даже пытаются сделать что-то максимально хорошее, но там, да, вышло, а выходит, выходит, как всегда.

475
01:02:27,572 --> 01:02:39,352
Кстати, вот Ян Лекун, которого я люблю слушать, потому что он уже, в общем-то, вышел из такой большой корпоративной игры, поэтому может себе позволить сказать гораздо больше.

476
01:02:39,352 --> 01:02:49,452
Он, правда, отвечал на вопрос, что сделать, чтобы я и нас не уничтожил, но, на мой взгляд, может быть, это и можно применить, да, в какую-то такую более хорошую среду.

477
01:02:49,452 --> 01:03:24,135
Он тут, говорит, слушайте, если мы говорим про ситуацию, когда у нас есть более глупое существо, человек, и более умное существо, да, там вот AGI, которое мы вдруг можем создать, не дай бог, вдруг в этот день, в этот час он появится у этой команды, то давайте мы сделаем ценности материнские, потому что мы вот знаем только один биологический случай, когда более умное существо готово там, да, щадить и как-то воспитывать и давать какой-то конструктивный фидвек более глупому существу.

478
01:03:24,135 --> 01:03:27,345
Вот, но он это говорил с точки зрения, чтобы не уничтожило.

479
01:03:27,345 --> 01:04:15,547
с нашей стороны, если сделать я достаточно занудным, неприятным, и это сделать как абсолютную необходимость для всех инструментов, может быть, как-то удастся допинать нас до чуть более лучшей ситуации, но, ну, как Элифаев говорит, можем, но не будем, поэтому я думаю, что Мы тут уже действительно в такой очень нехорошей ситуации, с которой, скорее всего, если говорить на таком более историческом пласте, из которой нас может вытащить, ну, только, скорее всего, что-то очень радикальное, что прямо, да, встряхнет нас.

480
01:04:15,547 --> 01:04:18,907
Вот. На этом завершу свой комментарий.

481
01:04:18,907 --> 01:04:23,047
Ну, после таких прекрасных завершений буквально два коротких комментария. Первый.

482
01:04:23,047 --> 01:04:27,812
Вспоминаем детскую сказку которая не очень детская. Лисы в стране чудес.

483
01:04:27,812 --> 01:04:31,112
И иногда, чтобы стоять на месте, нужно бежать в два раза быстрее.

484
01:04:31,112 --> 01:04:42,692
Вот на фоне развития наших вот этих магических артефактов, которые я теперь однозначно буду называть такими методами, бежать нужно даже и в два, может быть, по экспоненте быстрее.

485
01:04:42,692 --> 01:04:46,152
И в этом я вижу оптимистическую провиденческую работу.

486
01:04:46,152 --> 01:04:53,032
А то мы слишком долго прятались за разными технологическими, скажем так, идолами.

487
01:04:53,357 --> 01:05:19,027
считая себя их господами, ну наконец-то мы создали голема, или создаем в процессе, чисто под прекрасные лозунги, с прекрасными технологическими возможностями, которые вопрос, собственно, нашей, прежде всего, на самом деле, этической субъектности, уже во вторую уровень и когнитивно-эмоциональную субъектность, ставит перед тем самым большим-большим зеркалом Которое нам и должно показать.

488
01:05:19,027 --> 01:05:24,327
Первое, то, что мы-то уже сами по себе почти-таки очень голенькие стали за последние 400 лет.

489
01:05:24,327 --> 01:05:31,207
Ну ладно, все кроме Алиса. Мы с Евгением однозначно. И тут два варианта.

490
01:05:31,207 --> 01:05:38,327
Либо уж совсем залезать в эту капсулку, как в фильме Матрица, либо все-таки уже искать одежду, основание.

491
01:05:39,679 --> 01:05:41,079
Тренировать себя.

492
01:05:41,079 --> 01:05:46,719
То есть это то самое внутреннее усилие подменять свое самоволие настоящим самовластием.

493
01:05:46,719 --> 01:05:51,619
То есть вот такого вот спрятаться не удастся. Такой себе вывод, который я сделал.

494
01:05:52,015 --> 01:05:57,655
Ну что ж, отличное завершение. Спасибо, Алиса. Прежде всего, спасибо, Павел.

495
01:05:57,655 --> 01:06:05,855
Я думаю, что мы, по крайней мере, постараемся ещё вернуться к этой теме вместе с Алисой и Павлом.

496
01:06:05,855 --> 01:06:12,215
Ну а для тех, кто слушает или смотрит нас, пожалуйста, оставляйте свои комментарии.

497
01:06:12,215 --> 01:06:31,314
Дополняйте нас, мы будем рады услышать мнение тех людей, особенно кто находится в этой индустрии и сможет рассказать нам о том, что мы не знаем, например, или поделиться какими-то своими размышлениями, которые нас дополнят.

498
01:06:31,314 --> 01:06:35,214
Всем еще раз большое спасибо, ну и до встречи.

499
01:06:35,214 --> 01:06:36,114
До встречи.
