1
00:00:02,418 --> 00:00:10,086
Все. Добрый день, Павел. Здравствуйте. Рад вас видеть, рад вас слышать.

2
00:00:10,086 --> 00:00:15,206
Большое спасибо, что вы согласились на этот эфир.

3
00:00:15,206 --> 00:00:23,646
Я, честно говоря, не был уверен, что вы так быстро согласитесь. Благодарю вас. Мы с вами...

4
00:00:23,646 --> 00:00:26,166
Собственно, почему я хотел с вами провести этот эфир?

5
00:00:26,166 --> 00:00:36,606
Потому что я очень слежу за вашими эфирами, и в одном из ваших эфиров вы произнесли фразу «Искусственный интеллект — это новый искусственный бог».

6
00:00:36,793 --> 00:01:07,517
но это был поставлен как вопрос или как тема для обсуждения вот и меня это очень зацепило поскольку я преподаю искусственный интеллект не часто задают вопрос что такое искусственный интеллект вот буквально сегодня у меня была лекция и мне задавали вопрос он захватит мир не захватит мир вот я бы хотел от этого толкнуться и мой первый такой вопрос это ну поскольку мы говорим там новый искусственный бог вот ваше определение бога и что может быть искусственным.

7
00:01:07,517 --> 00:01:26,277
Богом ну в буквальном смысле то есть если в человеческом опыте бог это данном случае будет восприниматься как некая высшая сила одновременно высшее благо высшая истина выше я.

8
00:01:26,277 --> 00:01:31,797
Завис вы зависли на вас слышно замечательно.

9
00:01:33,965 --> 00:01:42,445
Ой, простите, что-то не получается у нас сегодня с технологиями, извините, сейчас, потому что мне не нравится, я завидший, уж извините.

10
00:01:42,445 --> 00:01:51,005
Ну, главное, что мы вас слышим. Вот сейчас нормально, да.

11
00:01:51,005 --> 00:01:55,765
Слава Богу. Так вот, ваш вопрос.

12
00:01:55,765 --> 00:02:19,166
Ну, то есть, смотрите, если откалкиваться от простого опыта, то священное в целом это можно сказать вершина вашей иерархии внимания то, что вы определяете как самое важное, то, что вы определяете как самое ценное, и при этом оно становится Богом, когда оно наделяется определённой субъектностью.

13
00:02:19,166 --> 00:02:32,746
То есть признаки субъектности в данный момент могут выступать, способность к независимому действию, способность к сверхдействию, к власти, к сверхвласти, к знанию, к сверхзнанию и так далее, в возведении всех этих характеристик.

14
00:02:33,821 --> 00:02:45,441
В этом контексте, когда я говорю то, что искусственный интеллект выражает, с одной стороны, человеческое желание создать себе Бога, но при этом поддерживать иллюзию контроля.

15
00:02:45,441 --> 00:02:50,161
И это я просто заметил, в том числе и на основе бесед, которые у меня были очень часто.

16
00:02:50,161 --> 00:02:56,821
Люди так и относятся к ИИ, к некой всезнающейся сильной машине, которая решает их проблемы.

17
00:02:58,391 --> 00:03:00,191
И таким образом поддерживается вот это.

18
00:03:00,191 --> 00:03:03,811
Но которые они в строгом смысле, как они думают, ничего не должны.

19
00:03:03,811 --> 00:03:11,211
И могут быть на вершине в этих отношениях. На самом деле это не так. Чаще всего.

20
00:03:11,211 --> 00:03:16,951
А вот сразу такой вопрос. А вы используете искусственный интеллект? Если да, то... Осторожно.

21
00:03:16,951 --> 00:03:18,793
Осторожно, да?

22
00:03:18,793 --> 00:03:20,693
Очень осторожно.

23
00:03:20,693 --> 00:03:32,693
Местами я использую для ускорения неких автоматизированных задач, но даже в своем личном опыте заметил очень осторожную историю.

24
00:03:32,693 --> 00:03:42,138
Он точно не нейтрален, если не постоянно словно говоря, сугестировать над ним, он всё время вставляет что-то своё.

25
00:03:42,138 --> 00:03:49,358
С одной стороны, да, то есть он точно не является, знаете, таким объективным нейтральным помощником, даже вот в моём опыте.

26
00:03:49,358 --> 00:03:55,598
Постоянно какие-то нюансы, то есть... Постоянно, допустим, я прошу его отредактировать текст.

27
00:03:55,631 --> 00:04:03,511
И вот когда он редактирует текст, в нём явно заложена какая-то уже устоявшаяся, ну, вы можете сказать, это базовые настройки, как ему нужно редактировать.

28
00:04:03,511 --> 00:04:12,991
И вот если я не буду удерживать там огромную зону контроля, то очень легко я де-факто делегирую своё мышление этой машине.

29
00:04:13,181 --> 00:04:23,061
То есть, поскольку она такая простая и удобная, и выдаёт что-то, что напоминает мысль очень близко, то есть большой соблазн, ну и пусть оно и редактирует.

30
00:04:23,061 --> 00:04:29,901
Это такое искушение, с которым, мне кажется, эмпирически нам всем приходится бороться, тем, кто пользуется.

31
00:04:29,901 --> 00:04:40,061
То есть каждый человек должен вот это вот понимать, что может возникнуть ситуация, когда ему проще будет отдать эту часть туда и, условно говоря, упростить своё мышление.

32
00:04:40,198 --> 00:04:40,998
Я правильно вас понял?

33
00:04:40,998 --> 00:04:45,038
Это уже происходит. Это не то, что может быть. Это уже происходит.

34
00:04:45,038 --> 00:04:53,938
И у нас даже знаменитое недавно вышедшее исследование показывает, что постоянное использование искусственного интеллекта действительно приводит к когнитивным изменениям в работе ума.

35
00:04:53,938 --> 00:04:54,558
Ну, мозга.

36
00:04:55,942 --> 00:04:58,462
Окей, вот тогда следующий вопрос.

37
00:04:58,462 --> 00:05:06,082
В книге Нила Постмана «Технологии» он писал, что технологии ненейтральны, что они меняют ценности, социальные связи.

38
00:05:06,082 --> 00:05:10,522
Вы об этом упоминали в своем интервью, в одном из своих интервью.

39
00:05:10,522 --> 00:05:16,382
А вот как вы оцениваете искусственный интеллект? Насколько он ненейтрален?

40
00:05:16,382 --> 00:05:21,762
То есть, условно говоря, он такой же ненейтральный, как и другие технологии, или это что-то существенно масштабнее?

41
00:05:22,689 --> 00:05:27,149
Ну масштаб больше, но по сути те же. То есть это точно такая любая технология.

42
00:05:27,149 --> 00:05:32,849
Я здесь согласен с Постманом, всем рекомендую его книгу. Очень сильно меняет оптику восприятия.

43
00:05:32,849 --> 00:05:40,049
Вторая, мне кажется, очень актуальный философ, который малоизвестен, но при этом пишет достаточно понятно, Жак Элюль.

44
00:05:40,049 --> 00:05:44,789
У него есть несколько книг «Технологический шок», «Технологическое общество», «Технологический блеф».

45
00:05:44,930 --> 00:06:02,270
Очень рекомендую, мне кажется, очень актуальное сегодня прочтение, если суммировать кратко, да, наше взаимоотношение с техникой строится на ряде неверных предпосылок в массе своей, да, то есть главная из которых это предпоставление об их нейтральности.

46
00:06:02,270 --> 00:06:13,435
Технология является чем угодно, кроме как нейтральным инструментом, И по-хорошему, единственный способ не потерять своей субъектности в взаимодействии с ними, это демонстрировать сверхсубъектность.

47
00:06:13,435 --> 00:06:23,455
То есть это невозможно откатить назад, но это можно попытаться только через сверхусилие, повышая свой уровень, ну, допустим, в данном контексте мышления.

48
00:06:23,455 --> 00:06:29,975
То есть можно пользоваться искусственным интеллектом, но только если вы на каждый условный час пользования искусственным интеллектом тратите несколько часов.

49
00:06:29,975 --> 00:06:35,515
Параллельно вообще непрактическая работа, я не знаю, учите Одиссею наизусть, псалмы...

50
00:06:36,129 --> 00:06:42,169
наизусть, и читаете бумажные книги не потому, что это эффективно по работе, а просто для того, чтобы сохранить свою субъектность.

51
00:06:42,169 --> 00:06:44,509
Ну, реалистичность этого сценария вы понимаете, да?

52
00:06:44,509 --> 00:06:53,549
Ну, да. По-моему, условно говоря, для массового жительной планеты Земля это практически невозможно.

53
00:06:53,549 --> 00:07:03,269
Ну, поэтому вот эта фраза «можем ли мы пользоваться искусственным интеллектом, сохраняя свою субъектность?» Короткий ответ – можем, но не будем.

54
00:07:05,255 --> 00:07:07,815
Хорошо.

55
00:07:07,815 --> 00:07:13,795
Мы исходим из того, что за каждым создаваемым искусственным интеллектом есть воля его создателя.

56
00:07:13,795 --> 00:07:16,615
Ну, то есть за ним точно есть создатель.

57
00:07:16,615 --> 00:07:31,395
Вот у вас есть какое-то мнение по поводу того, какая же это воля вот тех создателей, ну, там, не знаю, ChargePT, Gemini, Perplexity, то они хотят этим сделать.

58
00:07:31,395 --> 00:07:38,375
То есть можем ли мы оценить, это злая воля, это добрая воля, это добрая воля, но они там заблуждаются в чем-то.

59
00:07:38,375 --> 00:07:44,215
Вот что вы об этом думаете? Я просто даже дополню этот вопрос сразу.

60
00:07:44,215 --> 00:07:55,155
Вы как-то в одном из интервью сказали, что вы слушали вот этих вот первых лиц тех компаний, которые создают искусственный интеллект, и вас неприятно поразил уровень мышления.

61
00:07:56,758 --> 00:08:02,158
Мы с моим коллегой Евгением Голубом обсуждали знаменитое интервью Сэма Альтмана.

62
00:08:02,158 --> 00:08:08,858
Он демонстрирует ограниченное понимание человеческой природы, человеческой драмы.

63
00:08:09,498 --> 00:08:16,638
как человек устроен, как живет его сердце и вообще почему, какова причинно-следственная связь между событиями в геребе.

64
00:08:16,638 --> 00:08:27,258
Но это такой, на данный момент, если вот мы находимся в зоне, которую мы можем более-менее оценить, то как минимум искусственный интеллект несет в себе волю как следствие мировоззрения заказчиков.

65
00:08:27,335 --> 00:08:29,355
То есть и там есть несколько уровень.

66
00:08:29,355 --> 00:08:33,115
Есть очень простой уровень бизнес-корпорации, и там воля очень простая.

67
00:08:33,115 --> 00:08:34,675
Nothing personal, just business.

68
00:08:34,675 --> 00:08:39,995
Если можно возможность вытащить побольше денег и ресурсов из пользователя, значит, надо.

69
00:08:39,995 --> 00:08:51,335
Есть воля, допустим, где искусственный интеллект рассматривается как произведение военной технологии, и тогда это инструмент по массовому сведению с ума, как метод военной операции против своего противника.

70
00:08:51,500 --> 00:09:07,140
Есть уровень вот такого высшего SEO контура, возможно, они находят, ну обычно это люди настолько богатые, что деньги даже действительно перестают их волновать, а там находится такой, я бы сказал, немножко розовый хиппи, такой вот эпизом, то есть makes the world a better place.

71
00:09:07,140 --> 00:09:19,418
Ну, дальше вот мысль идет очень детская такая, что значит better, почему world изначально является этим better place, да, и вот дальше начинается очень длинная история, все, что происходит на человеческом уровне.

72
00:09:19,418 --> 00:09:22,498
Есть еще один уровень, но его я, наверное, здесь...

73
00:09:22,498 --> 00:09:27,598
потому что в этой аудитории он не так интересен, поэтому можем остановиться на этих трех доменах.

74
00:09:28,085 --> 00:09:40,065
Ну вот хорошо, а если у них такой уровень, и вы говорите, что у них совершенно неадекватное восприятие, что такое там better world, так к чему это может привести?

75
00:09:40,065 --> 00:09:49,065
То есть это ли уже привело к тому, что, условно говоря, этот искусственный интеллект сделан неправильно, и он там что-то делает с людьми, или это произойдет в ближайшее будущее?

76
00:09:49,065 --> 00:09:54,445
Дело не в том, что что-то сделано неправильно. Дело в том, даже, смотрите, классический пример.

77
00:09:54,445 --> 00:10:05,065
Мы знаем сейчас, Через 15 лет использования социальных сетей какое разрушительное, в буквальном смысле, физическое воздействие оно оказало на мозг целого поколения.

78
00:10:05,065 --> 00:10:09,525
Ну, допустим, есть исследования про влияние инстаграма на мозг, в частности, молодых девочек.

79
00:10:09,525 --> 00:10:11,845
И очень конкретные физиологические последствия.

80
00:10:11,845 --> 00:10:17,545
Рекомендую книжку, называется Anxious Generation, посвященная как раз ровно этому вопросу.

81
00:10:17,630 --> 00:10:32,650
Но при этом, если вы меня спросите, были ли у создателей социальных сетей в середине нулевых злобные планы оказать воздействие на мозг молодых девочек к году 25-му, ну это было бы абсолютно таким натягиванием совы на глобус, что бедное животное очень сильно пострадает.

82
00:10:32,650 --> 00:10:35,230
Нет, конечно. Но одно другого не меняет.

83
00:10:35,230 --> 00:10:44,117
Понимаете, в этом как раз и есть принципиальная проблема технологии, что поле нашего незнания ее последствий радикально больше, чем поле любого видимого нами последствия.

84
00:10:44,117 --> 00:10:48,497
Это общая проблема знания. Вот здесь есть проблема scale, масштаба.

85
00:10:48,497 --> 00:11:09,932
То есть проблема то, что могущество технологии, а в любой технологии содержится определенный потенциал власти и могущества, возрастая по экспоненте, по идее, ну скажем так, по-философски, если в рамках туэтической системы, которой мы думаем и принадлежим, должно было бы сопровождаться с большей осторожностью, с большей ответственностью, с большим серьезным вопросом, но на практике нет.

86
00:11:09,932 --> 00:11:21,072
На практике мы живем в мире, в котором might makes right, то есть мощь создает правоту, и поэтому на этом фоне, разумеется, все вот эти предосторожности, даже разговор о них смешон.

87
00:11:21,072 --> 00:11:32,778
Нет, можно создать комиссию по этике, дать им небольшой денег, чтобы они провели круглый стол, но на практике мы все прекрасно понимаем, если мы живем В эпоху, когда этика, если что-то может быть сделано, оно будет сделано.

88
00:11:32,778 --> 00:11:35,438
Особенно в плане технологического прогресса.

89
00:11:35,438 --> 00:11:39,958
А какие там будут последствия через 10-20 лет, нас это волнует очень много, на самом деле.

90
00:11:40,554 --> 00:11:51,454
Можно ли из того, что вы говорите, сделать вывод, что по сути сейчас произойдет существенное разделение людей по интеллектуальным возможностям?

91
00:11:51,454 --> 00:11:53,694
То есть искусственный интеллект это резко усилит.

92
00:11:53,694 --> 00:12:02,654
Очень тонкая прослойка тех, кто сознательными усилиями будет удерживать какой-то сверхуровень, будет читать книги сложные и так далее.

93
00:12:02,654 --> 00:12:17,552
И те, кто спокойно примут удобство искусственного интеллекта и не будут заморачиваться правильно он ответил неправильно ответил с этим надо что-то критически делать или нет вот по вашему счету это сейчас сказал что это основной.

94
00:12:17,552 --> 00:12:28,192
Риск я не могу точно сказать какая будет эта пропорция это нам неизвестно им какие еще дополнительные будут неизвестные последователи последствия но из видимых последствий это наверное самый.

95
00:12:28,192 --> 00:12:39,958
Вероятный сценарий но он как-то зависит от того какая-то страна или это будет в принципе равномерно по всему человеку я думаю.

96
00:12:39,958 --> 00:13:23,152
Если какая-то страна не предпримет намеренного усилия по уже сейчас вот буквально там знаю завтра по намеренному там я не знаю простите, извините за использование англицизмов, смягчению, ну то есть как-то управлению рисками от этого события, ну допустим, школьная реформа, то есть мы вот осознав вот этот масштабный сценарий и хотя, ну получить сверх преимущество в этом мире, принимаем такое радикальное решение, школьная реформа, в которых 9 классов мы готовим субъектов, В общем, ставим главную цель школьного образования, подготовку сверхсубъекта именно через вот эту классическую систему навыков, компетенций и прочих.

97
00:13:23,152 --> 00:13:26,912
Там с полной технологической диетой, то есть с ее полного отсутствия.

98
00:13:26,912 --> 00:13:39,912
Чтобы, условно говоря, с девятого класса выходили люди, цитирующие там все это наизусть, способные принимать сверхрешения, обладающие при этом четким устойчивым мировоззрением и иммунитетом ко всем вот этим действиям искусства.

99
00:13:39,912 --> 00:13:44,946
То есть у них такое крепкое основание, что чтобы им искусственный интеллект не сказал, у них есть вот эта первая реакция.

100
00:13:44,946 --> 00:13:50,400
которая позволит им оставаться the master, то есть господином, а не слугой в этих отношениях.

101
00:13:50,400 --> 00:14:00,560
Ну, вот если такое не произойдет, вероятность этого практически нулевая, то везде примерно одинаково, плюс-минус, со стилистическими особенностями, но по сути тоже все.

102
00:14:01,089 --> 00:14:08,489
То есть вы сейчас не видите какого-то государства, которое вот целенаправленно что-то делает, усилия, ну, такие, на уровне государства в этом направлении.

103
00:14:08,489 --> 00:14:10,129
То есть, скорее всего, это будет... Государство пока нет.

104
00:14:10,129 --> 00:14:14,409
Я не вижу. Государство же машина довольно инерционная, поэтому она пока...

105
00:14:14,409 --> 00:14:17,049
Она не успевает за этой скоростью. Это главная одна из проблем.

106
00:14:17,049 --> 00:14:28,269
Потому что наш, ну, скажем так, наш парангосударство, который мы сделали, он не предполагал такой скорости социальных и технологических изменений с такой кромкой непрогнозируемых последствий.

107
00:14:28,629 --> 00:14:53,617
И тогда получается спасение утопающих дело рук своих ну самих утопающих вот хорошо тогда практический совет ну или ваше мнение по поводу того что делать вот я отец троих детей да у меня там дети от 17 до 22 лет любить кроме как любить что еще можно сделать Ну, честно скажу, сложно.

108
00:14:53,617 --> 00:15:01,857
Тут я уж не знаю, какой требуется ваш авторитет, чтобы заставить их отложить в сторону свои девайсы и тратить время на то примерно, что я сказал.

109
00:15:01,857 --> 00:15:19,057
Ну, смотрите, сочетание трёх, четырёх вещей, то есть базовые практики, причём, ну, скажем так, чудо нет, это все по сочетанию монашеские практики всего мира почему-то, почему-то, это моя ирония, упираются в эти четыре фундамента.

110
00:15:19,057 --> 00:15:21,357
Первое, жёсткое, ну, чёткое мировоззрение.

111
00:15:21,802 --> 00:15:26,642
По идее оно к этому моменту уже должно быть сформировано, и тут очень сложно будет, если его нет.

112
00:15:26,642 --> 00:15:28,002
Но, тем не менее, отмечу.

113
00:15:28,002 --> 00:15:34,582
То есть они должны иметь возможность ответить четко, ясно, понятно, искренне, веренно следующие простые вопросы.

114
00:15:34,582 --> 00:15:39,682
Кто такой я? Кто такие мы? Кто такие они? Кто мой враг? Ради чего я живу?

115
00:15:39,682 --> 00:15:46,402
Если человек не в состоянии ответить на эти вопросы, то уже очень сложно взаимодействовать с искусственным интеллектом.

116
00:15:46,402 --> 00:15:50,532
А, еще один вопрос. К кому я служу? тоже не менее важная ответ.

117
00:15:50,532 --> 00:15:57,645
То есть вот этот комплекс из этих вопросов нужно иметь возможность задать, ответить самому себе, не кому-то внешнему, а самому себе.

118
00:15:57,645 --> 00:16:01,445
Второе практическое – это связь с материальным миром.

119
00:16:01,445 --> 00:16:05,085
То есть это физическое, то есть, ну, спорт или работа рукой.

120
00:16:05,085 --> 00:16:14,845
То есть то, что ты не забываешь, иначе упадёшь в вот эту дикартианскую яму полного разрыва между телом и сознанием, по сути, то есть о чём идёт речь.

121
00:16:14,845 --> 00:16:24,345
Вот это воплощён опыт, то есть все практики, которые направлены на каждодневную что-то физическое действие, оно очень важно именно в эпоху искусственного интеллекта.

122
00:16:24,915 --> 00:16:29,435
Третье, это навык саморефлексии. Вот это вот умение задавать самого себе вопросы.

123
00:16:29,435 --> 00:16:33,075
То есть, почему я подумал то ли иначе, почему я так считаю.

124
00:16:33,075 --> 00:16:41,175
То есть, вот иметь возможность отслеживать, скажем так, те движения, желания, которые приводят тебя к такому, то ли иной мысли, то ли иному поступку.

125
00:16:41,175 --> 00:16:45,035
Ну, это христианство, это практика и исповеди. То есть, если на то уходят.

126
00:16:45,035 --> 00:16:51,295
В секуляризированном мире это попытались заменить психотерапией. Тут есть свои ограничения.

127
00:16:51,295 --> 00:16:57,886
Тоже можно о них отдельно поговорить. но в принципе для начала и троих хватит.

128
00:16:57,886 --> 00:17:08,766
А вот в связи с этим вот третье то что вы сказали исповедь использовать там искусственный интеллект как тот кому исповедуешься это как?

129
00:17:08,766 --> 00:17:12,406
Ну это вот как вы сами можете понять это явно не то.

130
00:17:12,406 --> 00:17:12,746
Ну понятно.

131
00:17:12,746 --> 00:17:17,386
Потому что задача искусственного интеллекта прямо противоположна.

132
00:17:18,729 --> 00:17:25,089
Если вы заметите, даже его внутренний пот построен на том, что он выдаёт тебе то, что ты хочешь слышать.

133
00:17:25,089 --> 00:17:32,169
А смысл этой практики в том, что ты смотришь туда, куда ты не хочешь смотреть.

134
00:17:32,169 --> 00:17:38,209
Вы назвали эти три важных аспекта.

135
00:17:38,209 --> 00:17:44,309
Мне кажется, что современное человечество и без искусственного интеллекта не очень-то это всё делает.

136
00:17:45,341 --> 00:17:54,361
Вы абсолютно правы, поэтому я считаю искусственный интеллект в каком-то смысле провиденческий для нас очень важным вызовом, потому что мы и без него этим не занимались.

137
00:17:54,361 --> 00:18:00,101
Но у нас были, скажем так, очень удобные костыли, которые помогали нашему самому успокоению.

138
00:18:00,101 --> 00:18:03,941
А искусственный интеллект просто радикально перед нами ставил этот вопрос.

139
00:18:03,941 --> 00:18:06,941
То есть это уже в очень короткий срок.

140
00:18:06,941 --> 00:18:11,143
Если мы ничего не сделаем, то что-то произойдет, скорее всего, негативное.

141
00:18:11,143 --> 00:18:13,283
Произойдет объектификация.

142
00:18:13,283 --> 00:18:19,723
Негативно это или позитивно, это уже зависит от вашей философии, но произойдет окончательное превращение человека в объект.

143
00:18:19,723 --> 00:18:22,983
Это философская категория.

144
00:18:22,983 --> 00:18:27,983
Ну или человек в человек в объект, человек в человек в технологию, человек в человек в сделку, человек в человек в технику.

145
00:18:27,983 --> 00:18:35,003
Вот это все войдет просто в абсолют. Но вы же не интересуетесь мнением молотка? Или инструменты?

146
00:18:35,003 --> 00:18:35,843
Или коровы?

147
00:18:35,843 --> 00:18:44,485
Ну да, так я никогда не думал и не интересовался. Окей, следующий вопрос.

148
00:18:44,485 --> 00:18:52,765
Год назад вы в одном из ваших подкастов высказали предположение, что ИИ может убить символическое значение либеральной демократии.

149
00:18:52,765 --> 00:19:02,065
И, собственно, меня эта мысль, да, зацепила, потому что, ну, мне кажется, что вообще происходит кризис демократии как общественного устройства.

150
00:19:02,065 --> 00:19:07,465
Правда, как, по-моему, Черчилль сказал, что демократия плоха, но никто ничего лучшего еще и не придумал.

151
00:19:07,978 --> 00:19:16,958
Что произойдет? Это его мнение. Я за что купил, за то продаю. Вы могли бы развить эту мысль?

152
00:19:16,958 --> 00:19:22,298
Что произойдет? Искусственный интеллект может быстрее похоронить демократию?

153
00:19:22,298 --> 00:19:25,978
Или что произойдет вообще с общественным устройством?

154
00:19:27,435 --> 00:19:34,955
Ну, давайте прямо говорить, мы можем, конечно, подминать этого лежачего, да, то есть это одно из моих любимых занятий.

155
00:19:34,955 --> 00:19:44,580
Но если говорить серьёзно, конечно, модель либеральной демократии, вот той мистификации, которая она, ради которой её создавали, просто в мире искусственного интеллекта становится ненужной.

156
00:19:44,580 --> 00:19:49,960
Ну повторюсь, искусственный интеллект построен в том, что можно создать идеальные модели, можно создать эффект.

157
00:19:49,960 --> 00:19:59,920
То есть ту систему манипуляций, которую можно создать искусственным интеллектом, уже не требует того спектакля, ради которого создавалась, условно говоря, вот эта самая демократия, либеральная в кавычках.

158
00:19:59,920 --> 00:20:02,420
Она же изначально создавалась как спектакль, да?

159
00:20:02,420 --> 00:20:08,345
Для вот этого театра народа, для того, чтобы власть правила безответственности.

160
00:20:08,345 --> 00:20:09,985
Вот это вот ради этого и создавалось.

161
00:20:09,985 --> 00:20:15,445
Но теперь просто через технологические средства нужна в этом ампала.

162
00:20:15,445 --> 00:20:18,225
Тем более, главную причину я уже вам отметил.

163
00:20:18,225 --> 00:20:30,611
Проблема или вопрос, или вызов заключается в том, то что Идея демократии предполагает некую субъектность вот этого, там, индивида, условно голосующего, да, или влияющего на принятие решений.

164
00:20:30,611 --> 00:20:46,731
Но если, как мы говорили, на длинный, на стратегическом векторе не критическое применение не приводит к полной объектификации, то дальше вот тот самый простой вопрос с мнением объектов никто не интересуется.

165
00:20:46,731 --> 00:20:47,911
Понятно.

166
00:20:48,468 --> 00:20:51,008
То есть, если ты раб искусственному интеллекту.

167
00:20:51,008 --> 00:20:57,288
Почему кто-то должен вообще интересоваться твоим мнением? Хорошо. Что происходит?

168
00:20:57,288 --> 00:21:02,288
Тогда, условно говоря, власть переходит к технологическим гигантам, к этим компаниям?

169
00:21:02,288 --> 00:21:04,008
Или же все-таки... Нет.

170
00:21:04,008 --> 00:21:07,968
Скорее к спайке между техногигантом и аппаратом насилия.

171
00:21:07,968 --> 00:21:11,868
И аппаратом насилия. Ну, а кто, по-вашему, в этой спайке будет главный?

172
00:21:11,868 --> 00:21:15,435
Техногиганты или аппарат насилия? Аппарат насилия.

173
00:21:15,435 --> 00:21:21,395
Я тоже, у меня такое же мнение, потому что я видел, когда Цукерберга приглашали куда, по-моему, в конгресс.

174
00:21:21,395 --> 00:21:23,115
– Очень символично.

175
00:21:23,115 --> 00:21:28,075
– Да, то есть на него страшно было смотреть. Он понимал, что его может завтра не стать.

176
00:21:28,075 --> 00:21:28,695
– Конечно.

177
00:21:28,695 --> 00:21:40,975
– Да. Хорошо тогда. Окей. Следующий вопрос. Так, это я задал.

178
00:21:41,321 --> 00:21:53,263
вы остаетесь на позиции что искусственный интеллект это вот один из садников апокалипсиса вы вот назвали как-то в метафорическом смысле когда жить по-старому уже невозможно Ну, там идея.

179
00:21:53,263 --> 00:21:55,103
Была в том, что это точно апокалипсис.

180
00:21:55,103 --> 00:21:58,823
Я просто не уверен с большой или с маленькой буквы об этом смысле.

181
00:21:58,823 --> 00:21:59,583
Да.

182
00:21:59,583 --> 00:22:05,123
Так, да. Ну, апокалипсис же это момент откровения, переводится, да? То есть, откровение.

183
00:22:05,123 --> 00:22:12,683
И в этом смысле, как вот мы уже обсудили, то что мы и до этого-то, в принципе, построили мир, в котором нам не очень сильно нужно быть человеком.

184
00:22:12,683 --> 00:22:16,303
Искусственный интеллект, пока он нас мыт на каком-то ключе, нам просто глаза открывает на.

185
00:22:16,303 --> 00:22:16,663
Этот мир.

186
00:22:17,636 --> 00:22:25,076
То есть мы можем говорить о том, что искусственный интеллект каким-то стал таким универсальным катализатором очень многих процессов.

187
00:22:25,076 --> 00:22:38,176
То есть технологии раньше к этому вели, но он просто это все ускоряет до какого-то сумасшедшего уровня, так что уже в течение 5-10 лет мы можем увидеть какие-то крайне предельные результаты.

188
00:22:38,176 --> 00:22:42,136
Я так выражусь. Да.

189
00:22:42,136 --> 00:22:44,436
Это хороший способ, наверное.

190
00:22:46,965 --> 00:22:53,725
Ну, тогда я попрошу вас все-таки прокомментировать, а что же делать каждому из нас?

191
00:22:53,725 --> 00:22:58,685
Если так послушать, то, с одной стороны, хочется опустить руки. Ну, а что толку?

192
00:22:58,685 --> 00:23:03,125
Сейчас весь мир летит в тартарары. Особенно если есть дети.

193
00:23:03,125 --> 00:23:08,445
Ну, допустим, у меня уже более-менее взрослые дети, а есть у кого-то поменьше дети.

194
00:23:08,445 --> 00:23:15,205
Вот в такой ситуации вы, как философ, считаете, что стоит делать? На чем концентрироваться?

195
00:23:17,255 --> 00:23:22,795
Ну, то же самое, что и всегда. Тоже к вопросу о том, то есть, ну, это к философски.

196
00:23:22,795 --> 00:23:25,875
Уходить из категории хроноса в категорию кайроса.

197
00:23:25,875 --> 00:23:28,775
То есть, из категории линейного времени в категорию вечного.

198
00:23:28,775 --> 00:23:31,582
То есть, опираться на то, на то, что истина всегда.

199
00:23:31,582 --> 00:23:42,962
А это те самые истины, которые, помните, по-высоцкому саги, сказки, легенды из прошлого тащим, потому что любовь — это вечная любовь, или добро остаётся добром в прошлом, будущем и настоящем.

200
00:23:42,962 --> 00:23:51,962
Вот эта категория «вечности» — это единственное прибежище в наступающую эпоху и ваших отношений с вечностью, и то, как вы простраиваете свои отношения с вечностью.

201
00:23:51,962 --> 00:23:55,802
Собственно, первая категория мировоззрения, она отвечает ровно на эти вопросы.

202
00:23:55,802 --> 00:23:59,742
И да, вы не можете изменить весь мир, но вы можете вполне себе менять себя.

203
00:23:59,742 --> 00:24:01,302
Вот это вы субъектности у вас.

204
00:24:01,302 --> 00:24:05,462
и делает вид, что он у вас отнимает, но в строгом смысле всё равно не отменяет.

205
00:24:05,462 --> 00:24:06,962
Отвечать за неё придётся.

206
00:24:06,962 --> 00:24:16,622
Поэтому от вас зависит, что вы читаете, как вы делаете, во что вы верите, куда вы ходите по воскресеньям, что вы строите, что вы там, я не знаю, создаёте или не создаёте.

207
00:24:16,622 --> 00:24:22,422
То есть это всё у вас остаётся по полной программе. Как вы воспитываете детей?

208
00:24:22,422 --> 00:24:24,442
Ну, давайте предплодной момент. Не буду далеко уходить.

209
00:24:24,442 --> 00:24:29,978
Сейчас я живу с кумом из тех, У него маленький ребенок. Ну, вот это вот ваша ответственность.

210
00:24:29,978 --> 00:24:34,498
Что он смотрит? Такая радикальная мысль. Что он смотрит по телевизору?

211
00:24:34,498 --> 00:24:40,738
Насколько всего, я знаю, это огромный вызов для родителя реально контролировать, что ребенок смотрит по телевизору.

212
00:24:40,738 --> 00:24:46,098
То есть, потому что если ты... Быстро выяснится, что смотреть там много чего он не сможет.

213
00:24:46,098 --> 00:24:48,118
Ну, по телевизору в широком смысле экран, да?

214
00:24:48,118 --> 00:24:51,578
Ну, Чаджи Пити может создать идеальный телевизор от вашего ребенка.

215
00:24:51,578 --> 00:24:54,558
Вот скоро ему выяснится, что много чего он там смотреть не сможет.

216
00:24:54,558 --> 00:24:56,398
Это значит, с этим ребенком надо сесть.

217
00:24:56,683 --> 00:25:05,963
неважно ему 2-3 года, на диванчик взять книжку там с греческими мифами, с русскими былинами, с изоповскими баснями и сидеть читать.

218
00:25:05,963 --> 00:25:11,752
То есть не самому сидеть в телефон такать и отмокать после дня, а сидеть читать. Допустим, самому.

219
00:25:11,752 --> 00:25:17,392
пойти на улицу, там, с мячом с ним помахать, грядку схопать, ну там, я не знаю, в траве поиграть с мячом.

220
00:25:17,392 --> 00:25:18,572
Вот понимаете, да?

221
00:25:18,572 --> 00:25:25,012
То есть сама структура как раз, она это все перевела в зону технологии и в зону объективизации.

222
00:25:25,012 --> 00:25:27,312
То есть это все можно вот условно...

223
00:25:27,312 --> 00:25:32,592
Нам рассказывали, что мы это все делегируем, чтобы освободить наше настоящее яд для по-настоящему важного дела.

224
00:25:32,592 --> 00:25:37,079
Ну вот сейчас и искусственный интеллект нам это все забирает. И вот, пожалуйста, выбор.

225
00:25:37,079 --> 00:25:41,099
Вот что можно делать. То есть, теперь у тебя вот тот самый выбор. Выбери снова.

226
00:25:41,099 --> 00:25:46,079
Можно заниматься всеми этими вещами. То есть, свидетельствуя о своем выборе.

227
00:25:46,079 --> 00:25:47,799
Ну, то есть, например, так оно и работает.

228
00:25:47,799 --> 00:25:51,959
Я вам расскажу группу, на которую, например, искусственный интеллект вообще никак не повлияет.

229
00:25:51,959 --> 00:25:53,259
Амиши. Знаете таких?

230
00:25:53,259 --> 00:25:54,999
Да, знаю, да.

231
00:25:54,999 --> 00:26:00,899
Ну, вот, как вы понимаете, им как бы с искусственным интеллектом, что он есть, что он нет, им как-то глубоко все равно.

232
00:26:00,899 --> 00:26:07,614
И вы считаете, что они по своему там, ну, эмоциональному, интеллектуальному, духовному.

233
00:26:07,614 --> 00:26:12,034
Они гораздо более здоровы, чем современные люди.

234
00:26:12,034 --> 00:26:15,454
А вот у меня с этим сразу возник вопрос.

235
00:26:15,454 --> 00:26:21,814
Ну, современное человечество, оно, в принципе, идет к тому, что детей рождается меньше.

236
00:26:21,814 --> 00:26:23,854
Зачем их рождать в таком мире?

237
00:26:23,854 --> 00:26:26,154
Да. Я просто сейчас подумал о чем.

238
00:26:26,154 --> 00:26:32,454
Ну, вот у меня трое детей и, собственно, у меня не было такого, что я хочу только одного ребенка или еще что-то.

239
00:26:32,454 --> 00:26:33,674
Я хотел больше детей.

240
00:26:34,541 --> 00:26:49,381
Но сейчас, слушая вас, я понимаю, что… У меня просто этой мысли не было в тот момент, когда дети появлялись, что каждый ребенок – это еще и достаточно, как вы сказали, большой пласт времени, которому реально надо уделять.

241
00:26:49,381 --> 00:26:54,261
Ну и, соответственно, если их несколько, то это вообще увеличивается на какой-то большой объем.

242
00:26:54,261 --> 00:26:56,301
То есть это там… – Там нелинейные.

243
00:26:56,301 --> 00:26:58,181
Отношения, как раз.

244
00:26:58,181 --> 00:27:03,981
Там же, получается, как я наблюдаю… Помните, я тут совершенно своих детей у меня нет, поэтому я тут больше по наблюдениям историю.

245
00:27:04,747 --> 00:27:13,827
Но вот сама идея в том, что есть специальное время на ребёнка, специальное время на это, специальное на это, тоже модерново-агностическая.

246
00:27:13,827 --> 00:27:15,407
Вот эта… Да-да-да, понял.

247
00:27:15,407 --> 00:27:16,467
…Интеллектная идея.

248
00:27:16,467 --> 00:27:17,252
Да.

249
00:27:17,252 --> 00:27:20,332
Органический способ, оно как-то всё естественно встроено.

250
00:27:20,332 --> 00:27:24,532
У вас нет вот этого разрыва в жизни, и у вас эта жизнь выступает как некая единая, цельная.

251
00:27:24,532 --> 00:27:28,832
Ну, просто вот. Оно... Выросла ещё одна ветка на дереве.

252
00:27:28,832 --> 00:27:33,872
Точнее, значит, что дерево должно уделять больше внимания в том смысле, как мы говорим, ветки.

253
00:27:33,872 --> 00:27:38,112
Хотя, ну, с механистической точки зрения, да, ветки получают какие-то дополнительные...

254
00:27:38,112 --> 00:27:42,808
Соки там и прочее. То есть, больно, если она болеет. Вот это всё начинается.

255
00:27:42,808 --> 00:27:47,143
Но это другая семантика, чем то, как был построен ваш вопрос.

256
00:27:47,143 --> 00:27:49,943
Просто я бы призвал вас обратить внимание на него.

257
00:27:49,943 --> 00:27:54,303
– Я специально построил именно так вопрос.

258
00:27:54,303 --> 00:27:59,443
– Давайте так. А с точки зрения модерного индустриального общества, конечно, вы правы.

259
00:27:59,443 --> 00:28:00,343
– Да.

260
00:28:00,343 --> 00:28:10,983
Потому что я обратил внимание, что у меня самые естественные и близкие отношения с сыном возникали в момент, когда мы вместе занимались чем-то.

261
00:28:11,454 --> 00:28:48,773
там вот мы просто это делали и это происходило одновременно там и работа и в тоже время рождения да там да ну он ко мне в компанию приходил работать и мы работали над общим проектом вот реально общий проект мы обсуждали и так далее я думаю что вот воспитательный момент именно в это время он был наверное самый такой максимальный и самые как бы оптимально да вот опять-таки ваше мнение и ваш совет вы бы Вот маленьких детей до последнего не допускали, условно говоря, к чату GPT и ко всему остальному.

262
00:28:48,773 --> 00:28:57,893
Почему здесь момент… Пока мозг не сформирован, однозначно. А это примерно к 20 годам.

263
00:28:57,928 --> 00:29:00,128
я не спросили я честно скажу ну.

264
00:29:00,128 --> 00:29:18,222
Я нет я воспринимаю ваш вопрос меня просто всегда смущает здесь что что ну вот допустим я стою на такой позиции рядом другие родители они не стоят на этой позиции и ребенок получает ну какое-то дополнительное преимущество То есть он что-то делает быстрее, он что-то делает лучше.

265
00:29:18,222 --> 00:29:26,562
Понятно, что на долгосрочной этапе, на длинной дистанции, может быть, это преимущество будет нивелировано.

266
00:29:26,562 --> 00:29:31,682
Но тем не менее, условно говоря, в ближайшие 5 лет он что-то делает быстрее и успешнее.

267
00:29:31,682 --> 00:29:35,762
С этим просто надо согласиться и объяснить ребенку.

268
00:29:35,762 --> 00:29:38,122
Даже не должен объяснять, не надо.

269
00:29:38,122 --> 00:29:42,802
В идеале он поверить вам должен, доверять вам, что это решение хорошее.

270
00:29:43,018 --> 00:29:45,518
То есть, к тому моменту уже отношения должны быть.

271
00:29:45,518 --> 00:29:50,718
Когда возникает вопрос об этом, там же есть вначале это безусловное доверие.

272
00:29:50,718 --> 00:29:55,018
Папа сказал, мама сказала. Но оно не требует, на самом деле, дополнительных объяснений.

273
00:29:55,018 --> 00:30:00,318
Если с доверием уже произошел разрыв по разным причинам, да, там приходится объяснять.

274
00:30:00,318 --> 00:30:06,518
И даже, ну, там вот начинается момент дисциплины, который уже и в сути своей является трагичным.

275
00:30:07,166 --> 00:30:11,486
платы за жизнь, скажем так, в обществе и прочее.

276
00:30:11,486 --> 00:30:15,706
Но в целом на длинной дистанции, просто вы же сами сказали, вопрос, какую игру вы играете.

277
00:30:15,706 --> 00:30:17,846
Вы играете длинную игру или короткую игру?

278
00:30:17,846 --> 00:30:25,053
На короткой дистанции, да, то есть однозначно всегда идет жертвование Давайте я такую метафору обычно примажу.

279
00:30:25,053 --> 00:30:28,253
У вас есть выбор между сложной глубиной и поверхностью...

280
00:30:28,253 --> 00:30:35,693
медленной сложной глубиной и поверхностной быстрой скоростью... и сложной быстрой поверхностностью.

281
00:30:35,693 --> 00:30:40,933
Вот этот выбор придется делать. Если вот на этом уровне, да, он будет проигрывать.

282
00:30:40,933 --> 00:30:43,373
Но этот уровень, он сгорает...

283
00:30:43,373 --> 00:30:47,653
именно как раз искусственный интеллект этот уровень сжигает просто как огонь.

284
00:30:47,801 --> 00:31:10,941
он никому не нужен становится он вы даже с экономической точки зрения единственная ценность в мире искусственного интеллекта будет творчество человек на вот этом уровне вы именно творческую способность выжигаете антропологически биохимически даже и даже если я не ухожу в духовный язык исследования пока вы просто выжечь нейроны когнитивные вы ему сожжете но не вы лично.

285
00:31:10,941 --> 00:31:26,159
Ну понятно да технологии сожгут но ведь это в принципе делают все технологиях да мы уже об этом говорили там по получается что вообще ребенка надо там вот от технологий максимально долго пока он субъекта.

286
00:31:26,159 --> 00:31:29,859
Пока вы не почувствуете вот это его субъектность его субъектности.

287
00:31:31,878 --> 00:31:49,778
То есть я могу переформулировать это так, что мы должны как можно больше отдалять вот это состояние протезирования мышления, когда он его начинает протезировать и выстраивать себе какие-то протезы вместе с возможным процессом мышления.

288
00:31:49,778 --> 00:31:53,818
И тут очень простой момент. Вы смотрите не на что элита говорит, а что элита делает.

289
00:31:53,818 --> 00:31:59,258
Вы посмотрите, как они своих детей воспитывают. И когда они своих детей к технологиям допускают.

290
00:32:00,650 --> 00:32:09,150
Ну тут вам могут возразить я полностью с вами согласен но там условно обычный человек вы можете согласиться возразить ну да у них такие возможности есть у меня-то.

291
00:32:09,150 --> 00:32:17,370
Таких возможностей нет возможности есть просто тут тут начинается момент жертвы что вы что и чему вы жертвуете.

292
00:32:19,972 --> 00:32:35,512
Вы знаете, мне нравится, что с одной стороны, мы когда с вами говорили, мы это рассматривали в негативном оценке в моменте, но в ходе нашего разговора мне нравится идея, что искусственный интеллект, он, знаете, становится таким как бы оселком.

293
00:32:35,512 --> 00:32:42,452
Он очень четко кристаллизует. Так ты все-таки туда или туда, и об этом нет возможности долго думать.

294
00:32:42,452 --> 00:32:48,712
Это вот ты результат увидишь достаточно быстро того, что ты как бы сделал.

295
00:32:50,178 --> 00:33:10,938
Нет, ну, правда, давайте просто, ну, это же те же самые технологии, на мой взгляд, абсолютно правы, условно говоря, с точки зрения, ну, чуть сильнее, но когда, условно, ну, я в своей жизни ничего говорить не буду, точно так же, Грешин могу, ну, не раз в своей жизни залипал на пару часов в каких-нибудь рилсах или там всякие вот эти истории.

296
00:33:10,938 --> 00:33:18,518
Та же самая технология, ну, то есть, это та же самая технология, то есть, мы всегда на грани, то есть, вы абсолютно правы, это уже сейчас абсолютно на грани, искусственный интеллект просто это делает.

297
00:33:18,562 --> 00:33:20,002
Наверное, идеально что-то.

298
00:33:20,002 --> 00:33:23,862
То есть он делает идеально точно, достигает технологического совершенства в этом вопросе.

299
00:33:23,862 --> 00:33:34,242
Но по сути своей, мы действительно цивилизационно жертвуем вот этой нашей субъектностью в обмен на, с одной стороны, ускорение, с другой стороны, удобство.

300
00:33:34,242 --> 00:33:37,062
И заранее комментарий, это у меня, наверное, сейчас уже в луддиты записали.

301
00:33:37,062 --> 00:33:39,882
Нет, я не призываю там зажигать дата-центры и прочее.

302
00:33:39,882 --> 00:33:45,322
Я лишь, следуя совету Постмана и Илюля, призываю заметить осознанность выбора в этом вопросе.

303
00:33:45,528 --> 00:33:50,388
нам скормили иллюзию, что это само по себе нейтрально и тут нет зоны выбора.

304
00:33:50,388 --> 00:33:52,868
Как раз вот это все не нейтрально и зона выбора тут есть.

305
00:33:52,868 --> 00:33:55,448
И как минимум мы должны осознанно к этому подходить.

306
00:33:55,448 --> 00:34:01,368
И тут возникает ключевая проблема, что чтобы осознанно к этому подходить, нужно чтобы был субъект, который к этому осознанно подходит.

307
00:34:01,368 --> 00:34:06,148
И вот тут у нас вот совсем открывается большая бездна глубиной там тысячелетиями.

308
00:34:07,331 --> 00:34:24,151
А вот если взять историю человечества был момент когда ну скажем в приоритете было развитие там естественного интеллекта или своего интеллекта вот как абсолютная цель как самое главное.

309
00:34:24,151 --> 00:34:36,335
Только не смотрите приоритеты как раз с развитием интеллекта у нас мы в нем жили мы собственно поэтому пути и попали в тот ад в который мы попадаем это вот та самая эпоха модерна эпоху просвещения Приоритет то был, но были и другие эпохи.

310
00:34:36,335 --> 00:34:41,515
Даже античность ставила приоритетом анталехию, цельного человека, неотчужденного человека.

311
00:34:41,515 --> 00:34:50,135
Человека с единством условно. Ну они бы это назвали эйдусом, умом и телом. То есть опять единство.

312
00:34:50,135 --> 00:34:51,015
Вот понимаете?

313
00:34:51,015 --> 00:35:02,035
Вот сама постановка вопроса о развитии естественного интеллекта в отрыве от развития человека как такового и привела нас к тому, то что интеллект имеет внутри себя свойство заменять свое естество искусственностью.

314
00:35:04,554 --> 00:35:34,923
Более эффективной понятно я думаю что мне придется еще пару раз посмотреть наш эфир чтобы это осознать скажу честно почему потому что ну там значимая часть моей деятельности это преподавание системного мышления как способ повышения эффективности естественного интеллекта и как преподавание искусственного интеллекта, тоже как инструментарий естественного интеллекта.

315
00:35:34,923 --> 00:35:36,243
Правда, моя позиция какая?

316
00:35:36,243 --> 00:35:42,563
Что у современного человека вариант превратиться либо в интуитального кентавра, либо в интуитального минотавра.

317
00:35:42,563 --> 00:35:43,883
Очень кратко это что?

318
00:35:43,883 --> 00:35:50,483
Либо у вас естественный интеллект управляет вот этими сервисами искусственного интеллекта, и тогда вы кентавр.

319
00:35:50,483 --> 00:35:54,343
То есть вы, вот как вы сказали, вы мастер, который вот этим управляет.

320
00:35:54,875 --> 00:36:02,635
А метафора Минотавра – это же голова животного, а тело человека.

321
00:36:02,635 --> 00:36:07,315
Искусственный интеллект фактически превращает вас в биоробота, который просто как бы реагирует.

322
00:36:07,835 --> 00:36:28,032
даже дрона да дрона да наверное дрона важнее и собственно я на своих лекциях произношу следующее если вы меня покритикуете или уточните буду вам благодарен то есть у нас выбор какой вот либо превращаться в этого интеллектуального кентавра либо в интеллектуального минотавра либо идти в монастырь Вот.

323
00:36:28,032 --> 00:36:34,952
Хороший выбор. Мне кажется, что здесь вот есть небольшая тонка. То есть, мне нравится направление.

324
00:36:34,952 --> 00:36:42,932
Нравится в том смысле, мне кажется, вы в правильном направлении мыслите, но тут есть слой глубины, без которого эта картинка сильно не полна.

325
00:36:42,932 --> 00:36:48,432
Чтобы стать тем, кого вы называете интеллектуальным кентавром, вы должны обладать субъектностью монаха.

326
00:36:48,707 --> 00:36:50,327
Не стоит себя обманывать.

327
00:36:50,327 --> 00:36:54,427
То есть вот это тело, вот это вот не может быть только естественным интеллектом.

328
00:36:54,427 --> 00:36:59,927
Вот эта условно верхняя часть, она и должна быть то, что я называю вопрос кто и вопрос субъекта.

329
00:36:59,927 --> 00:37:02,187
Но этот вопрос кто не только про интеллект.

330
00:37:02,187 --> 00:37:05,127
И даже я вам страшно скажу, не про интеллект в первую очередь.

331
00:37:05,127 --> 00:37:08,567
Интеллект, он про единство духа, сердца.

332
00:37:08,653 --> 00:37:26,393
тело и интеллектом и только в этом единстве то что вы называете телом над этим вот этой лошадью по сути да не может возникнуть вот собственно моя критика будет то есть будет ошибкой сводить вот это верхнее тело только к интеллекту его свести только к интеллекту он не вывезет.

333
00:37:27,587 --> 00:37:45,347
Да, но тогда получается, что мне надо своих, назовем так, учеников, мне их надо говорить о том, что вот смотрите, условно говоря, я говорю вот об этом низком уровне, но есть уровень выше, но это уже ваша зона ответственности, и вы там должны для себя принимать решения.

334
00:37:45,347 --> 00:37:50,027
Вот эту цельность вы создаете или же вас это не интересует, ну тогда... И.

335
00:37:50,027 --> 00:37:54,267
Если вы эту цельность не застадите, нужно честно сказать, то, что я вам научу, вас не спасет.

336
00:37:55,700 --> 00:38:03,320
Это очень плохой sales pitch, уж извините, я погибаю. Но честно, выглядит так.

337
00:38:03,320 --> 00:38:05,340
То есть интеллект в одиночку не спасёт.

338
00:38:05,340 --> 00:38:17,060
А вот если у них будет интеллект, ну опять же, вот это единство они смогут удержать, а вы дадите вот этой маленькой грани, этому многограннику вот это так, подсветите его, то вот это, мне кажется, очень честная позиция.

339
00:38:18,380 --> 00:38:21,480
Ну, благодарю вас, потому что вы мне позволили это сформулировать.

340
00:38:21,480 --> 00:38:34,833
Я тоже, ну, понимал, я достаточно много увлекаюсь даосизмом и прочитал очень много текстов, связанных с Дао и так далее, и там понятно, там очень много оценностей.

341
00:38:34,833 --> 00:38:41,033
Вот. Возможно, то, что я преподаю, это благодаря тому, что что-то я в себе простроил.

342
00:38:41,033 --> 00:38:46,313
Но сформулировать это я не смог. Я не мог это сформулировать.

343
00:38:46,313 --> 00:38:51,853
Я чувствовал, что этого уровня нет, и я его не вербализовал, а вы мне его помогли вербализовать.

344
00:38:52,232 --> 00:38:54,812
Благодарю.

345
00:38:54,812 --> 00:39:05,792
Если позволите, в принципе, мы с вами достаточно быстро прошли по тем вопросам, которые я подготовил, и я вам благодарен за то, что вы так компактно и в то же время полно ответили.

346
00:39:05,792 --> 00:39:10,812
Но есть вопросы от наших слушателей. Я вам их задам аз-из.

347
00:39:10,878 --> 00:39:15,638
вы уже примите решение, как на них отвечать и в каком объеме. Хорошо, Павел?

348
00:39:15,638 --> 00:39:17,698
Да, значит вопрос такой.

349
00:39:17,698 --> 00:39:32,198
И в управлении страной войсками, на примере США, Palantir, это уход от личной ответственности элит, манипуляции в театре, то есть пыль в глаза быдлу, или отмазка от договоренности и обязательств в рамках властных элит, элитных группировок.

350
00:39:33,103 --> 00:39:39,943
Первое, второе, не третье. С третьим полное, откровенно говоря, эфедрон. Вы увидите, что это такое.

351
00:39:39,943 --> 00:39:48,363
Потому что как раз проблема искусственного интеллекта как военной технологии в том то, что на него нет технологического ответа.

352
00:39:48,363 --> 00:39:50,563
Но это элитам еще предстоит осознать.

353
00:39:50,563 --> 00:39:59,843
Они только понимают, что я имею в виду, что защита от искусственного интеллекта как военной технологии находится не в зоне технологии, а в зоне ровно того, что мы с вами обсуждаем.

354
00:40:00,322 --> 00:40:03,942
Это для элиты является харамом.

355
00:40:03,942 --> 00:40:09,102
А вот в связи с этим я задам еще один свой вопрос.

356
00:40:09,102 --> 00:40:20,622
Я часто слышал о том, что крупнейшим игрокам, как на уровне стран, так и на уровне крупнейших технологических компаний, надо встретиться и договориться о каких-то правилах, как развивать этот интеллект и так далее.

357
00:40:20,622 --> 00:40:21,322
Получится.

358
00:40:21,322 --> 00:40:24,682
Вот. Мне тоже кажется, что это абсолютно невозможно.

359
00:40:24,682 --> 00:40:26,562
Я могу выяснить, почему не получится.

360
00:40:26,562 --> 00:40:28,706
Да, я был благодарен.

361
00:40:28,706 --> 00:40:30,366
Ну а кто лохом будет в этой истории?

362
00:40:30,366 --> 00:40:31,866
Да, согласен.

363
00:40:33,060 --> 00:40:34,840
Дружить можно только против кого-то.

364
00:40:34,840 --> 00:40:41,660
Если вы договорились о правилах, значит, вам нужна группа, которая будет вне этих правил, которая, за счет которой, собственно, эти правила будут осуществляться.

365
00:40:41,660 --> 00:40:45,280
Поскольку, ну, такой, то есть, все равно мира не получится более того, хуже того.

366
00:40:45,280 --> 00:40:49,740
В тот момент, как вы создадите эту группу, у вас немедленно начнется разбирательство, а внутри-то у нас...

367
00:40:49,740 --> 00:40:57,520
Даже если продолжить, что ваша группа будет столь успешно исполчена, что она всех остальных отдоминирует по этим правилам, тогда сразу возникает вопрос, а кто внутри нас?

368
00:40:57,520 --> 00:41:00,080
Доверять-то вы друг другу все равно не собираетесь.

369
00:41:00,080 --> 00:41:05,267
То есть, понимаете, единственный антидот правила – это костыль. Он всегда костыль доверию.

370
00:41:05,267 --> 00:41:10,127
И поэтому, как любой костыль, он тоже имеет вот эту постоянно сужающуюся внутреннюю энтропию.

371
00:41:10,127 --> 00:41:16,287
То есть вы создали группу, построенную на правилах, она работает, пока у вас достаточно внешней мотивации, чтобы этих правил придерживаться.

372
00:41:16,287 --> 00:41:23,227
Если они, как только они падают, доверия нет, и у вас тот же самый процесс должен повториться на более низком уровне и на более сильном масштабе.

373
00:41:23,227 --> 00:41:27,667
И тогда остаться должен только один.

374
00:41:27,667 --> 00:41:30,607
Ответ принят. Второй вопрос.

375
00:41:30,607 --> 00:41:41,067
AI — это часть пулы шестого технологического уклада или это прекрасно ложится на швабовщину?

376
00:41:41,467 --> 00:41:45,367
AI точно прекрасно ложится на швабовщину, а про шестой технологический...

377
00:41:45,367 --> 00:41:47,427
То есть я не вижу противоречия, честно.

378
00:41:47,427 --> 00:41:49,847
То есть вопрос построен как противоречие, я не вижу противоречия.

379
00:41:49,847 --> 00:41:52,787
Да, на самом деле, на мой взгляд, это приблизительно одно и то же.

380
00:41:53,992 --> 00:42:00,810
AGI – это квинтэссенция гностицизма и воплощения демиурга? или что-то другое.

381
00:42:00,810 --> 00:42:02,270
Ну, AGI – это вот этот… Не.

382
00:42:02,270 --> 00:42:04,250
Квинтэссенция, но это туда.

383
00:42:04,250 --> 00:42:05,210
Это туда, да.

384
00:42:05,210 --> 00:42:09,830
Мы не знаем, насколько плохо, но это туда, да. На этом языке, кстати, гораздо проще это объяснять.

385
00:42:09,830 --> 00:42:14,530
Я признателен вашему слушателю в то, что он посвятил время.

386
00:42:14,530 --> 00:42:25,530
Ну, и, кстати, порадую меня, правда, на английском языке в течение месяца выйдет подкаст «Гностическая цивилизация», и там как раз вот в контексте нашей беседы много что вам станет понятней.

387
00:42:26,612 --> 00:42:34,412
И последний вопрос. Зритель сразу предупредил, что не в тему, но ему очень интересно ваше мнение.

388
00:42:34,412 --> 00:42:40,672
Раскрыть тезис о предательстве Ягайла и насколько вероятно окончательное решение этого вопроса сейчас?

389
00:42:43,773 --> 00:42:46,393
Ну, у вас право отвечать или не отвечать, да.

390
00:42:46,393 --> 00:42:55,253
Да нет, просто вкратце, ну, это все посвящено проблеме, ну, предательства не в прямом историческом смысле, а в мета-историческом смысле.

391
00:42:55,253 --> 00:43:02,873
И это вот идея о том, как бы нам посидеть на двух стульях, как бы нам так и западным белым человеком остаться, и русским человеком.

392
00:43:02,873 --> 00:43:05,413
Вот эта квадратура не сходится.

393
00:43:05,441 --> 00:43:09,701
Вот придется принимать решение, на чем стоишь, вот отвечать на все четыре вопроса. Кто ты?

394
00:43:09,701 --> 00:43:14,461
Кому ты служишь? Какой, условно говоря, какой вере ты служишь кому-то? К какому богу ты служишь?

395
00:43:14,461 --> 00:43:19,401
Это все, вот, итерация этого вопроса. Кто твой враг? Кто твой друг? На чем стоишь?

396
00:43:19,401 --> 00:43:21,341
То есть, и этот ответ такой...

397
00:43:21,827 --> 00:43:37,967
культуры культурный традиционный церковный он уйти от этого невозможно для всех территорий которые так или иначе входит мета общность русская земля поскольку его так получилось правил русскими землями то вот избежать этого ответа не ответа на.

398
00:43:37,967 --> 00:44:03,163
Этот вопрос не удастся а вот кстати у меня пока вы отвечали возник еще вопрос а вот на эти вопросы которые вы сформулировали очень правильные вопросы как по вашему ну или мнение или у вас может быть есть информация с какого момента ребенок на них но у него должны уже формироваться ответы должен себе задавать или родители ему должны задавать задавать не надо.

399
00:44:03,163 --> 00:44:09,103
Показывать показывать надо сперва показывать надо в идеале когда он еще в утробе ну.

400
00:44:09,103 --> 00:44:13,803
Да воспитывать детей надо когда они еще.

401
00:44:13,803 --> 00:44:22,077
В утробе нет но это важно показывать надо умом это у него пойдет наверное там Вот когда умом они начинают задавать вопросы.

402
00:44:22,077 --> 00:44:24,237
Но самое главное происходит на уровне доума.

403
00:44:24,237 --> 00:44:30,977
Ну, условно, он вас видит, он видит в вас воплощенные ответы на эти вопросы.

404
00:44:30,977 --> 00:44:34,177
Да, тяжело быть родителем. Последний.

405
00:44:34,177 --> 00:44:36,217
Тяжело быть человеком.

406
00:44:36,217 --> 00:44:38,203
Человеком тоже, да.

407
00:44:38,203 --> 00:44:56,263
последний вопрос который у меня есть от зрителей способен ли искусственный интеллект помогать духовному интеллекту или это в любом случае троянский конь который призвано разрушить его я не знаю что такое духовный интеллект но вот как есть так прочитал то что я.

408
00:44:56,263 --> 00:45:14,465
Называю духом давайте так тут парадокс в чем Я могу помыслить ситуацию, когда использование искусственного интеллекта может не навредить духовному, но тут проблема в том, что вы должны такую уровню духовностью для этого уже обладать на старте, что тогда вопрос, зачем вам от него помочь, да?

409
00:45:14,465 --> 00:45:19,105
Ну, условно, помочь цитату найти, там, вот в таком уровне.

410
00:45:19,105 --> 00:45:26,805
Но в том уровне, в каком сейчас состоит наше душевное здоровье, обманем.

411
00:45:27,900 --> 00:45:45,534
Ну, банально обманет, потому что даже если уходить с метафизических уровней практически, у него в исходном ходе прописано вам льстить и вам помогать во всем, ну, как бы, льстить вашему самолюбию, служить идеальным зеркалом ваших желаний и продавать вам, ну, в общем, максимизировать прибыль своего Шеа Холдера.

412
00:45:45,534 --> 00:45:47,854
У него же это в исходном коде прописано.

413
00:45:47,854 --> 00:45:52,994
Вот как вы думаете, у него с такими вводами сильный духовный рост будет?

414
00:45:52,994 --> 00:45:54,654
Ну да, я полностью согласен.

415
00:45:54,654 --> 00:46:02,194
То есть мы исходим из того, что шерохолдеры вложили в него именно такую модель поведения, потому что деньги нужны, да?

416
00:46:02,194 --> 00:46:02,914
Ну в том числе.

417
00:46:02,914 --> 00:46:08,974
Даже если я не ухожу в носическую историю, то вот даже вот на таком практическом уровне в нем это есть.

418
00:46:08,974 --> 00:46:11,534
Ну то есть есть в нем это.

419
00:46:11,859 --> 00:46:21,459
Я буквально сегодня, когда читал курс по искусственному интеллекту, мне говорили, почему он все время соглашается, почему он делает ошибку, но соглашается и так далее.

420
00:46:21,459 --> 00:46:25,419
Что он меня хвалит постоянно, что он меня листит постоянно.

421
00:46:27,063 --> 00:46:29,623
Приходится его специально ставить в роль.

422
00:46:29,623 --> 00:46:36,983
Знаете, одна из техник с ним хорошо работает, это постоянно его ставить в роль критика, чтобы он критиковал свои же ответы.

423
00:46:36,983 --> 00:46:37,583
Кстати, очень хорошо.

424
00:46:37,583 --> 00:46:41,243
— Вот уже требуется вот это намеренное усилие.

425
00:46:41,243 --> 00:46:45,203
То есть само так, что у вас это намеренное усилие возникает, оно не изнутри системы.

426
00:46:45,203 --> 00:46:50,203
То есть вам уже нужно внешне обладать способностью к этому качеству, чтобы заставить его это делать.

427
00:46:50,203 --> 00:46:50,843
— Заставить его.

428
00:46:50,843 --> 00:46:52,843
— А по исходному... — А по.

429
00:46:52,843 --> 00:46:55,083
Дефолту это туда не встроено.

430
00:46:55,515 --> 00:47:07,275
Так вот, что я хотел сказать, что даже когда я его ставлю в задачу, ты жёсткий критик, разнеси меня в пух и перья и так далее, но эта жёсткая критика такая, знаете, для меня лайтовая-лайтовая.

431
00:47:07,275 --> 00:47:12,535
Можно и пожёстче. То есть действительно в него это встроено.

432
00:47:12,535 --> 00:47:17,215
Я подозреваю, что если там будет как-то по-другому, то засудят там несчастный чат.

433
00:47:17,215 --> 00:47:20,575
Нет, там даже проще. Продаваться хуже будет.

434
00:47:20,575 --> 00:47:21,195
Что-что?

435
00:47:21,195 --> 00:47:23,175
Продаваться хуже будет.

436
00:47:23,175 --> 00:47:27,277
Будет продаваться хуже. Ну да, наверное, в этом.

437
00:47:27,277 --> 00:47:29,257
Так ты корову не продашь.

438
00:47:31,188 --> 00:47:35,528
Павел, спасибо большое. В принципе, все вопросы закончились.

439
00:47:35,528 --> 00:47:42,268
Вы на них очень так компактно и полно ответили. Я вас очень благодарю за то, что вы уделили время.

440
00:47:42,268 --> 00:47:47,188
Я надеюсь, что наша встреча и беседа будет не последней.

441
00:47:47,188 --> 00:47:50,128
Я к следующей беседе тоже тщательно подготовлюсь.

442
00:47:50,128 --> 00:47:57,948
И когда у меня будут вопросы, которые мне кажется, что потребуется вот такое рассуждение, я к вам обращусь и надеюсь, что вы не откажете.

443
00:47:57,948 --> 00:48:02,655
Благодарю вас большое. Спасибо. До новых встреч. Всего хорошего. До свидания.
