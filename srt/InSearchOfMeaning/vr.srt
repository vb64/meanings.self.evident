1
00:00:08,300 --> 00:00:14,600
Speaker 0: Масштаб боли от этой техногенной революции будет в разы больше от техногенной революции, созданной паровым двигателем.

2
00:00:15,020 --> 00:00:21,680
Speaker 1: Пришло время, когда человек будет вынужден становиться человеком, а не быть разновидностью высокоорганизованного животного.

3
00:00:36,720 --> 00:00:37,600
Speaker 1: Здравствуйте, друзья!

4
00:00:38,100 --> 00:00:43,820
Speaker 1: Сегодня очередной выпуск подкаста Евгения Голуба и Павла Щелина «В поисках смысла».

5
00:00:44,180 --> 00:00:59,820
Speaker 1: Мы решили продолжить тему предыдущего выпуска, связанного с нейросетями и с их влиянием на нашу жизнь, тем влиянием, которое мы чувствуем уже сегодня и которое многократно сможем почувствовать во много раз больше в ближайшем будущем.

6
00:01:00,100 --> 00:01:01,460
Speaker 0: Да, не отпускает нас эта тема.

7
00:01:01,660 --> 00:01:06,360
Speaker 0: По итогам предыдущего выпуска почувствовали, что сильно недоговорили и решили продолжить.

8
00:01:06,760 --> 00:01:11,520
Speaker 1: А начнем мы с политики, с конька политического философа Павла.

9
00:01:11,800 --> 00:01:17,800
Speaker 0: Я просто вкратце эту тему поднял в прошлый раз, но она, на мой взгляд, достаточно важная, чтобы акцентировать ее снова.

10
00:01:18,140 --> 00:01:25,540
Speaker 0: Дело в том, что в перспективе чат GTP убивает... символическое обоснование модели либеральной демократии.

11
00:01:25,860 --> 00:01:44,860
Speaker 0: Оставим вопрос, что в реальности она не особо-то и работала, и на самом деле механизмы функционирования демократические, они работают по скорее теневому и непрямому принципу, но на уровне образов именно то, что я расскажу вам в дальнейшем, и составляло главный аспект, если угодно, демократического достоинства.

12
00:01:45,080 --> 00:01:46,580
Speaker 0: В чем суть либеральной демократии?

13
00:01:47,000 --> 00:02:06,020
Speaker 0: Суть в том, что... Свободные люди, обладающие независимой экономической ценностью, определенной экономической самодостаточностью, на самом деле, просто говоря, обладающие собственностью, очень важный момент, являются... достаточным, независимым агентом, чье мнение должно быть услышано.

14
00:02:06,420 --> 00:02:09,600
Speaker 0: И без голосов этих агентов решение становится не полностью.

15
00:02:09,820 --> 00:02:19,880
Speaker 0: Другими словами, в самой модели либеральной демократии заложена связка между агентностью, то есть способностью принимать решения за свою собственную жизнь, и вашим политическим весом.

16
00:02:20,220 --> 00:02:27,820
Speaker 0: А как мы уже начали на прошлом выпуске, вот именно эту агентность для абсолютного большинства населения чат-GTP разрушает.

17
00:02:28,220 --> 00:02:31,900
Speaker 0: В экономическом смысле и даже Возможно, в социальном смысле.

18
00:02:32,200 --> 00:02:35,320
Speaker 1: Расскажи поподробнее, как именно он разрешается с твоей точки зрения.

19
00:02:35,680 --> 00:02:38,320
Speaker 0: Ну смотри, потому что, во-первых, экономический вызов.

20
00:02:38,380 --> 00:02:45,700
Speaker 0: То есть если роботы могут заменить человека в области производства технологического или машинного, эту проблематику фантасты обсуждали.

21
00:02:46,020 --> 00:02:50,240
Speaker 0: Но теперь мы, по сути, видим роботов, которые могут заменить человека в сфере услуг.

22
00:02:50,560 --> 00:02:52,880
Speaker 0: Не полностью, но в огромной части сферы услуг.

23
00:02:53,120 --> 00:02:54,200
Speaker 0: Тогда возникает вопрос.

24
00:02:54,500 --> 00:03:10,200
Speaker 0: Если человек становится... не активным участником экономической деятельности в массе своей, то есть он находится на иждивении других, так или иначе, если он не обладает собственной собственностью, то есть не заложенной в кредит, а именно той собственностью, которую он обладает как носитель ее.

25
00:03:10,940 --> 00:03:15,480
Speaker 0: В рамках либеральной модели какой даже теоретический вес имеет его голос?

26
00:03:15,820 --> 00:03:17,600
Speaker 1: Ты не слишком ли сгущаешь краски?

27
00:03:18,000 --> 00:03:22,700
Speaker 1: Какая-то часть людей по мере технического прогресса, развития теряет работу.

28
00:03:23,060 --> 00:03:24,900
Speaker 1: Машинисток сейчас ты уже не найдёшь.

29
00:03:25,180 --> 00:03:27,060
Speaker 1: Есть для этого принтер и компьютер.

30
00:03:27,280 --> 00:03:28,980
Speaker 1: Но ничего, небо на землю не упало.

31
00:03:29,360 --> 00:03:32,560
Speaker 1: Но сейчас какая-то часть людей переориентируется.

32
00:03:32,620 --> 00:03:33,140
Speaker 1: Почему вдруг?

33
00:03:33,320 --> 00:03:35,180
Speaker 1: Прямо такие драматические изменения.

34
00:03:35,460 --> 00:03:38,840
Speaker 0: Не обязательно, что изменения наступят прямо сразу, но они наступят.

35
00:03:39,020 --> 00:03:42,240
Speaker 0: Наступят они потому, что это то, что называется кризисом легитимности.

36
00:03:42,500 --> 00:03:56,960
Speaker 0: Потому что в рамках либеральной модели заложено вот это ощущение самодостаточного свободного собственника, если ты помнишь, идет от греческого полюса, проходит через всю традицию американской демократии и доходит до наших дней, как вот этот образ, как некий идеал.

37
00:03:57,380 --> 00:04:06,340
Speaker 0: Чат GTP, он значительно сокращает количество возможностей для того, чтобы быть таким свободным самодостаточным собственником.

38
00:04:06,660 --> 00:04:08,620
Speaker 1: Тогда я попробую так это для себя связать.

39
00:04:08,700 --> 00:04:21,700
Speaker 1: Ты говоришь о том, что масштаб влияния нейросетей на базу избирательную, на количество независимых и экономически самодостаточных людей будет несопоставим с тем, что мы видели до сих пор.

40
00:04:21,920 --> 00:04:31,120
Speaker 1: Потому что огромное количество тех работ, которые ассоциировались только с человеческим интеллектом, не подразумевалось, что они могут быть автоматизированы.

41
00:04:31,180 --> 00:04:34,420
Speaker 1: Эта работа не связана с трудом на конвейере.

42
00:04:34,820 --> 00:04:43,440
Speaker 1: эта работа, которая всегда ассоциировалась с умственными усилиями, она может быть легко заменена или со временем заменена нейросетями.

43
00:04:43,480 --> 00:04:46,020
Speaker 0: И тут можно посмотреть с другой точки зрения, с другого угла.

44
00:04:46,240 --> 00:04:51,780
Speaker 0: В каком-то смысле демократия всегда является некоторым способом компромисса между элитами и населением.

45
00:04:52,100 --> 00:05:00,140
Speaker 0: И можно сказать, что степень успешности демократической модели пропорциональна тому, насколько элита в этом населении нуждается в услугах этого населения.

46
00:05:00,280 --> 00:05:01,740
Speaker 0: Я могу сказать прямо по-рыночному.

47
00:05:02,020 --> 00:05:07,200
Speaker 0: Так вот, чат-GTP значительно, качественно сокращает потребность элит в населении.

48
00:05:07,340 --> 00:05:12,740
Speaker 1: Нас, видимо, очередной раз обвинят в теории заговора, о которой мы обязательно поговорим попозже.

49
00:05:13,160 --> 00:05:17,340
Speaker 1: Можно так понять, что есть некоторые элиты, есть все остальные.

50
00:05:17,660 --> 00:05:21,400
Speaker 1: И вот эти элиты манипулируют остальными для своих корыстных целей.

51
00:05:21,720 --> 00:05:28,620
Speaker 1: И мы сейчас как будто бы говорим о том, что с новым инструментом заинтересованность в манипуляции у них изменится каким-то образом.

52
00:05:28,860 --> 00:05:31,160
Speaker 0: Это даже не теория заговора, это просто здравый смысл.

53
00:05:31,260 --> 00:05:33,180
Speaker 0: В любом обществе всегда есть некая элита.

54
00:05:33,380 --> 00:05:34,460
Speaker 0: Это неизбежный момент.

55
00:05:34,520 --> 00:05:39,660
Speaker 0: И у них главным интересом является интересы своей собственной малой группы, иначе бы элитами они бы не стали.

56
00:05:39,920 --> 00:05:41,380
Speaker 0: Это никакая не теория заговора.

57
00:05:41,420 --> 00:05:43,760
Speaker 0: Теория заговора — это если бы я рассказывал, что у них есть какие-то репланы.

58
00:05:43,800 --> 00:05:51,060
Speaker 0: В данном случае чат GTP для них может быть той же самой новостью, которая для всех остальных, просто технология всегда подобна джину из бутылки.

59
00:05:51,300 --> 00:05:57,240
Speaker 0: Раз ее выпустив наружу, ты не загонишь ее назад, и она начнет сама влиять на происходящие процессы.

60
00:05:57,300 --> 00:06:06,160
Speaker 0: В этом плане есть великолепная книга Нейла Постмана, называется «Технополи», и он там подробно обсуждает базовый тезис, что технология не нейтральна.

61
00:06:06,380 --> 00:06:13,280
Speaker 0: Это огромное заблуждение, то, что мы склонны думать, что технология сама по себе обладает некой нейтральностью по отношению к ценностям.

62
00:06:13,460 --> 00:06:21,660
Speaker 0: И вот он там показывает на примере индустриальной революции, на отказы от технологий, условно говоря, типа арт-искусства.

63
00:06:21,820 --> 00:06:25,380
Speaker 0: То есть, когда искусство распространялось не только на картины, но на все.

64
00:06:25,440 --> 00:06:29,480
Speaker 0: Искусство делать карету, искусство делать стол, искусство делать доспех.

65
00:06:29,540 --> 00:06:30,760
Speaker 0: Это все называлось искусством.

66
00:06:31,020 --> 00:06:33,940
Speaker 0: Описывает вот переход от вот этого к production, к industry.

67
00:06:34,360 --> 00:06:37,180
Speaker 0: И вот показывает, что вот этот переход, он не был нейтральным.

68
00:06:37,400 --> 00:06:39,580
Speaker 0: Он повлек фундаментальным изменением ко всему.

69
00:06:39,680 --> 00:06:43,860
Speaker 0: От культуры до семейных моделей, до отношений между элитами и обществом.

70
00:06:44,060 --> 00:06:45,180
Speaker 0: Но это опять-таки очень зависит от этого.

71
00:06:45,320 --> 00:06:51,000
Speaker 0: То, что каждая революция в военном или экономическом деле влияет на... методы взаимоотношения элиты с обществом.

72
00:06:51,060 --> 00:07:01,400
Speaker 0: Условно говоря, если элите необходимо для ведения боевых действий послушное, но достаточно образованное население, способное выполнять приказы, оно будет создавать ту школьную систему, в которой мы все с вами учились.

73
00:07:01,620 --> 00:07:03,640
Speaker 0: А тут, соответственно, качественно меняется запрос.

74
00:07:03,860 --> 00:07:09,660
Speaker 0: И под это изменение запроса нужны будут другие социальные модели, нужны будут достаточно другие политические модели.

75
00:07:09,720 --> 00:07:12,040
Speaker 0: То есть это даже не теория заговора, это простое наблюдение.

76
00:07:12,180 --> 00:07:25,160
Speaker 0: Теория заговора, повторюсь, будет позже, не в этом выпуске, а это просто наблюдение, которое мы должны осознавать, что технологически... изменения подобного масштаба неизбежно приведут к фундаментальным изменениям в социально-политических отношениях.

77
00:07:25,620 --> 00:07:29,180
Speaker 1: Поговорим теперь тогда немного о этих технологических изменениях.

78
00:07:29,480 --> 00:07:45,700
Speaker 1: Недавно вышел ролик в YouTube от известного медиа Wired относительно того, как 20 профессий люди тестировали нейросеть, GPT, относительно того, насколько этот искусственный интеллект, назовем это так, способен заменить их.

79
00:07:45,960 --> 00:07:51,020
Speaker 1: От повара и программиста до врача и, по-моему, пожарного, там 20 профессий.

80
00:07:51,200 --> 00:07:54,080
Speaker 1: Ну и там возникло такое достаточно парадоксальное впечатление.

81
00:07:54,420 --> 00:08:00,140
Speaker 1: Большинство говорило, что ну нет, ещё не может, никак из этого не обойтись, это всё ещё слишком слабо.

82
00:08:00,580 --> 00:08:06,080
Speaker 1: Но уже сейчас очевидно, что, во-первых, и люди были шокированы, удивлены.

83
00:08:06,440 --> 00:08:09,340
Speaker 1: И, во-вторых, самой технологии ещё без году недели.

84
00:08:09,600 --> 00:08:16,900
Speaker 1: И если уже сейчас мы задаёмся этим вопросом, то очевидно, что в ближайший год-два вопросы эти исчезнут.

85
00:08:17,240 --> 00:08:26,340
Speaker 1: И, соответственно, огромное количество творческих профессий будут вынуждены задавать вопросы, будут подвергнуты сомнению их ценность.

86
00:08:26,560 --> 00:08:33,960
Speaker 1: Вот как ты думаешь, действительно ли искусственный интеллект, нейросеть... сможет полностью заменить творчество человека?

87
00:08:34,179 --> 00:08:36,240
Speaker 0: Вот здесь как раз есть некая тонкость.

88
00:08:36,520 --> 00:08:41,659
Speaker 0: Полностью, на мой взгляд, по крайней мере, в силу моего нынешнего понимания, он заменить человека не может.

89
00:08:42,020 --> 00:08:47,960
Speaker 0: Но он сможет заменить большинство, как мы уже говорили, псевдотворческих деятельностей.

90
00:08:48,040 --> 00:08:56,700
Speaker 0: Потому что, как мы уже обсуждали в предыдущем выпуске, именно... Комбинаторика, по сути, является сегодня основным методом творчества.

91
00:08:56,980 --> 00:09:02,140
Speaker 0: А ChatGTP является своеобразным краш-тестом для всей этой модели, для всей человеческой природы.

92
00:09:02,320 --> 00:09:10,780
Speaker 0: Потому что именно эта технология нейросети, огромного объема информации, делает ценным в творчестве только то, что, собственно, является творческим.

93
00:09:10,840 --> 00:09:15,160
Speaker 0: Или можно сказать, что делает ценное... в человеке только, собственно, человеческое.

94
00:09:15,240 --> 00:09:18,800
Speaker 0: Как люди религиозные, я прямо скажу то, что это божественное.

95
00:09:18,840 --> 00:09:20,040
Speaker 0: То есть вот это подобие Бога.

96
00:09:20,220 --> 00:09:27,600
Speaker 0: То есть человек ценен, и он является партнером для нейросети только в той мере, в какой он является подобием Бога.

97
00:09:27,880 --> 00:09:32,360
Speaker 0: И тогда это резко просто поднимает планку требований к человеку.

98
00:09:32,660 --> 00:09:40,220
Speaker 0: В каком-то смысле это такой искусственный кнут и обстоятельство непреодолимой силы, которое, можно сказать, заставляет человека преображаться духовно.

99
00:09:40,340 --> 00:09:42,820
Speaker 0: В противном случае конкурировать с нейросетью он не сможет.

100
00:09:43,080 --> 00:09:56,800
Speaker 1: Другими словами, инструментарий, который человек изобрёл или подготовил для себя, достиг такого уровня, когда возникает вопрос, нужен ли этому инструменту хозяин или человек, если он не является человеком в полном смысле этого слова.

101
00:09:57,140 --> 00:09:59,500
Speaker 1: С этой точки зрения, конечно, звучит замечательно.

102
00:09:59,580 --> 00:10:00,260
Speaker 1: Что тут скажешь?

103
00:10:00,340 --> 00:10:01,080
Speaker 1: Звучит здорово.

104
00:10:01,420 --> 00:10:02,040
Speaker 1: Наконец-то!

105
00:10:02,160 --> 00:10:02,580
Speaker 1: Аллилуйя!

106
00:10:02,800 --> 00:10:10,980
Speaker 1: Пришло время, когда человек будет вынужден становиться наконец-то человеком, а не быть разновидностью высокоорганизованного животного.

107
00:10:12,100 --> 00:10:16,620
Speaker 1: Понимаешь ли ты, что все-таки может быть и другой вариант развития событий?

108
00:10:17,020 --> 00:10:28,760
Speaker 1: Я видел уже десятки приложений в Apple Store, десятки, которые предлагают все виды искусства и написать вместе с искусственным интеллектом, или он запишет.

109
00:10:29,020 --> 00:10:36,260
Speaker 1: текст, нарисовать картину, написать картину в любом стиле, речь создать и так далее, и так далее.

110
00:10:36,520 --> 00:10:40,620
Speaker 1: Мне кажется, что не нужно недооценивать и негативную сторону.

111
00:10:40,900 --> 00:10:42,200
Speaker 0: Я совсем ее недооцениваю.

112
00:10:42,260 --> 00:10:49,920
Speaker 0: Я как раз думаю то, что масштаб боли от этой техногенной революции будет в разы больше от техногенной революции, созданной паровым двигателем.

113
00:10:49,960 --> 00:10:51,940
Speaker 1: То есть человек станет перед выбором, да?

114
00:10:52,020 --> 00:10:56,020
Speaker 1: То есть каждый человек, личность, будет находиться перед выбором, в какую сторону пойти.

115
00:10:56,340 --> 00:11:02,100
Speaker 1: Либо идти в сторону комфорта и... наверное, и сейчас во многом выбор такой стоит.

116
00:11:02,380 --> 00:11:13,120
Speaker 0: Очень важно подчеркнуть, мы говорили в прошлый раз, это будет самый комфортный комфорт, это будет самая приятная деградация, потому что это будет бесконечное удовлетворение вашей нарциссической потребности.

117
00:11:13,300 --> 00:11:19,120
Speaker 0: В той мере, в какой никакая технология до этого не могла удовлетворить вот эту нарциссическую дыру в сердце человека.

118
00:11:19,420 --> 00:11:22,300
Speaker 1: Объясни, пожалуйста, каким образом это будет происходить?

119
00:11:22,520 --> 00:11:26,820
Speaker 0: Ну, потому что чат GTP совместно с робототехникой может создать вам идеального партнера.

120
00:11:27,380 --> 00:11:34,060
Speaker 0: элементарный момент, который только и будет заниматься тем, что удовлетворять вашу нарциссическую потребность, не требуя ничего взамен.

121
00:11:34,160 --> 00:11:35,040
Speaker 1: Да, это важный момент.

122
00:11:35,160 --> 00:11:36,260
Speaker 0: Это как один из примеров.

123
00:11:36,440 --> 00:11:43,480
Speaker 1: Мы говорили в прошлый раз, что дописание чувственных смс-ок совершенно не истощает нейросеть, в отличие от любого молодого человека.

124
00:11:43,640 --> 00:11:56,680
Speaker 1: Точно так же, как удовлетворение прихотей капризного нарцисса, в общем-то, никаким образом не будет энергетически опустошать системы, продукты, искусственных партнёров, находящихся под управлением нейросети.

125
00:11:56,980 --> 00:11:57,680
Speaker 1: В этом смысле, да.

126
00:11:57,760 --> 00:12:00,280
Speaker 1: Так что, мы разделимся на Илоев и Марлоков?

127
00:12:00,520 --> 00:12:01,360
Speaker 1: Или как это будет?

128
00:12:01,400 --> 00:12:03,640
Speaker 0: Ну, ты знаешь, я вот по ходу нашей беседы другое.

129
00:12:03,680 --> 00:12:05,780
Speaker 0: Мне пришло скорее на Люденов и остальных.

130
00:12:05,840 --> 00:12:08,960
Speaker 0: Если ты помнишь, у Стругацких было «Волны гасят ветер».

131
00:12:09,420 --> 00:12:13,960
Speaker 0: И там тоже, условно говоря, предсказано великое разделение человечества.

132
00:12:14,200 --> 00:12:19,500
Speaker 0: Судя по всему, чат GPT является одной из ключевой точек по вот этому некому великому разделению.

133
00:12:19,860 --> 00:12:29,620
Speaker 0: На людей, которые будут творцами, будут обладать необходимым уровнем психологической, эмоциональной, духовной дисциплины, без которого настоящее творчество невозможно.

134
00:12:30,000 --> 00:12:30,560
Speaker 0: И на остальных.

135
00:12:30,780 --> 00:12:42,820
Speaker 0: Я боюсь, что вот эта технология в данном ее виде, если она никаким образом не будет ограничена, а я сомневаюсь, потому что технологию ограничить пока что в истории нам ни разу не удалось, то да, мы будем двигаться в этом направлении, в эту сторону.

136
00:12:43,100 --> 00:12:49,760
Speaker 1: Но можно ли сказать, что, скажем так, предыдущая история человечества, она предуготовила вот этот соблазн?

137
00:12:50,100 --> 00:12:53,000
Speaker 1: Фактически эмоции стали товаром уже достаточно давно.

138
00:12:53,380 --> 00:12:55,040
Speaker 1: Они вплетены в любой продукт.

139
00:12:55,300 --> 00:13:05,280
Speaker 1: И сейчас вот этот соблазн... будет полностью удовлетворен или полностью можно будет приблизиться к идеальному миру своей уникальности через нейросеть.

140
00:13:05,680 --> 00:13:06,520
Speaker 0: О да, абсолютно.

141
00:13:06,740 --> 00:13:07,760
Speaker 0: Тут много разных вариантов.

142
00:13:07,840 --> 00:13:09,540
Speaker 0: Можно соединить нейросеть с метой.

143
00:13:09,720 --> 00:13:13,300
Speaker 0: Представь виртуальный мир, созданный нейросетью под каждого отдельного человека.

144
00:13:13,560 --> 00:13:15,880
Speaker 1: То есть соединяем несколько технологий.

145
00:13:16,100 --> 00:13:29,960
Speaker 1: включаем туда психологию раздражённую или растравленную нашим капиталистическим способом производства и экономики, которая воспитывает максимальный индивидуализм и максимальную жадность до внимания.

146
00:13:30,280 --> 00:13:30,780
Speaker 1: и успеха.

147
00:13:31,080 --> 00:13:36,020
Speaker 1: И вот тебе мирок, в котором ты можешь счастливо пробыть, сколько там биологической жизни у тебя есть.

148
00:13:36,160 --> 00:13:43,800
Speaker 0: Возвращаясь к первой теме, с какой стати у тебя должен быть голос по влиянию на события в реальном мире с точки зрения людей, владеющих этими технологиями?

149
00:13:44,080 --> 00:13:44,520
Speaker 0: Никакого.

150
00:13:44,860 --> 00:13:45,920
Speaker 1: Зачем ты вообще нужен?

151
00:13:46,040 --> 00:13:49,780
Speaker 1: Что ты производишь, кроме тепла и отходов жизнедеятельности?

152
00:13:49,840 --> 00:13:52,300
Speaker 1: Для меня это вопрос, который непонятный.

153
00:13:52,340 --> 00:13:57,780
Speaker 1: Мне кажется, что в этом направлении развития человечества там одни тупики.

154
00:13:58,160 --> 00:13:59,460
Speaker 0: Ну смотри, опять не тупики.

155
00:13:59,560 --> 00:14:03,700
Speaker 0: это скорее, для меня это не обязательно тупик, это скорее именно радикализация.

156
00:14:03,920 --> 00:14:05,760
Speaker 0: То есть просто вопрос ставится ребром.

157
00:14:05,840 --> 00:14:10,160
Speaker 0: Я тебе могу сразу сказать, какие сообщества абсолютно нормально переживут эту революцию и не заметят.

158
00:14:10,220 --> 00:14:20,980
Speaker 0: Какие-нибудь староверы в Сибири, амиши в Пенсильвании, менониты в Португалии, сообщество кооперативное Мандрагон в Испании и так далее и тому подобное.

159
00:14:21,140 --> 00:14:31,780
Speaker 0: То есть везде, где простроены глубинные настоящие социальные связи, где люди включены как люди, где они объединены не... духом, они эту революцию переживут вполне спокойно.

160
00:14:31,840 --> 00:14:34,280
Speaker 0: То есть те люди, которые сохраняют, повторюсь, себя человечество.

161
00:14:34,520 --> 00:14:38,760
Speaker 0: А вот массовое капиталистическое население городов, да, вот им будет плохо.

162
00:14:39,000 --> 00:14:40,740
Speaker 1: Они просто не заметят, я так понимаю.

163
00:14:41,160 --> 00:14:42,460
Speaker 0: Они в эту игру играть не будут.

164
00:14:42,660 --> 00:14:45,820
Speaker 0: И более того, только такие люди для этой системы и будут ценными.

165
00:14:46,040 --> 00:14:47,340
Speaker 1: Я хотел сказать об этом, да.

166
00:14:47,460 --> 00:14:51,740
Speaker 1: Они не повреждены всем тем, чем повреждены мы там с тобой, да.

167
00:14:51,960 --> 00:14:56,820
Speaker 1: Поскольку они имеют дело непосредственно с природой, с миром, с другим человеком.

168
00:14:57,220 --> 00:15:00,280
Speaker 1: И поэтому в них вот это человеческое развито.

169
00:15:00,940 --> 00:15:01,420
Speaker 1: максимально.

170
00:15:01,460 --> 00:15:06,760
Speaker 0: Говоря научно, в них развита компетенция построения настоящих эмоциональных связей.

171
00:15:06,980 --> 00:15:08,180
Speaker 0: Они умеют этого делать.

172
00:15:08,480 --> 00:15:13,200
Speaker 0: И в них развита компетентность по, если угодно, мышлению.

173
00:15:13,240 --> 00:15:15,260
Speaker 0: То есть это действительно лишь биологическая разница.

174
00:15:15,340 --> 00:15:26,800
Speaker 0: Я думаю, если проводить нейротесты, мы увидим, что сами закономерности в работе их нейросетей в голове, условно говоря, качественно отличаются от той работы нейросети, которую обладают среднестатистические потребители интернета.

175
00:15:27,040 --> 00:15:37,960
Speaker 0: И вот как раз если за вторую нейросеть GPT заменили, может с легкостью, то я подозреваю, есть какие-то парадоксы в первом способе мышления, которые пока нейросети заменить крайне сложно.

176
00:15:38,080 --> 00:15:42,860
Speaker 0: Потому что пока что нейросети одно принципиальное ограничение, она занимается комбинаторикой.

177
00:15:42,980 --> 00:15:45,420
Speaker 0: Нейросеть не создает принципиально новое.

178
00:15:45,560 --> 00:15:48,060
Speaker 0: Она создает предельно качественные компиляции.

179
00:15:48,240 --> 00:15:50,240
Speaker 0: Они очень качественные, но они компиляции.

180
00:15:50,280 --> 00:15:52,000
Speaker 0: Их можно именно разглядеть как компиляции.

181
00:15:52,220 --> 00:15:55,320
Speaker 0: Проблема в том, что большая часть из нас тоже занята компиляциями.

182
00:15:55,560 --> 00:15:56,840
Speaker 0: Повторюсь, в этом основной вызов.

183
00:15:57,200 --> 00:16:04,840
Speaker 0: То есть насколько вы способны творить новое, насколько мы с тобой способны творить новое, настолько мы и будем ценные в этом мире, наступающем нейросети.

184
00:16:05,260 --> 00:16:14,300
Speaker 1: Да, и здесь стоит отметить еще такой момент, что основной способ взаимодействия с искусственным интеллектом сейчас — это диалог, это речевая модель.

185
00:16:14,420 --> 00:16:17,120
Speaker 1: Ты задаешь вопрос, ты вступаешь в беседу.

186
00:16:17,460 --> 00:16:25,560
Speaker 1: И это значит, что качество разговора между людьми должно повышаться, иначе не будет смысла в этом общении.

187
00:16:25,620 --> 00:16:30,580
Speaker 1: То есть ты можешь с большей продуктивностью разговаривать с искусственным интеллектом.

188
00:16:30,780 --> 00:16:44,580
Speaker 1: Мы можем надеяться, вот мы затрагивали эту тему в прошлый раз, что исподволь вот это изменение приведёт хотя бы часть человечества к новому, более высокому уровню развития мышления и речевой.

189
00:16:44,940 --> 00:16:49,740
Speaker 1: культуры, навыков ведения разговора, аргументированного разговора и так далее.

190
00:16:49,840 --> 00:16:56,000
Speaker 1: Потому что тот не сблушит, что там вот эти все манипуляции и передёргивания с нейросетью не работают.

191
00:16:56,080 --> 00:17:01,060
Speaker 1: Она будет терпеливо и неэмоционально возвращать собеседника к сути вопроса.

192
00:17:01,460 --> 00:17:02,180
Speaker 0: Опять два варианта.

193
00:17:02,240 --> 00:17:04,740
Speaker 0: Либо она будет просто этого собеседника удовлетворять.

194
00:17:05,020 --> 00:17:06,740
Speaker 0: Беседа же бывает двух ипостасей.

195
00:17:06,800 --> 00:17:11,579
Speaker 0: Беседа бывает как исключительный способ интеллектуального самовыдовлетворения вашего эго.

196
00:17:11,800 --> 00:17:19,540
Speaker 0: То есть, когда вам просто нужно в беседе с собеседником услышать, насколько вы правы, насколько вы замечательны, и вместе убедиться в том, насколько они правы остальные.

197
00:17:19,720 --> 00:17:22,619
Speaker 0: И вот на кого рода беседа, да, нейросеть заменяет очень просто.

198
00:17:22,980 --> 00:17:28,540
Speaker 0: Поэтому большая часть политических комментаторов на современном телевидении работы нейросетям лишится очень просто.

199
00:17:28,600 --> 00:17:34,840
Speaker 0: Будут простые цифровые аватары, которые... будут выдавать правильные тезисы и неотличимы от всех остальных блогеров.

200
00:17:34,960 --> 00:17:37,060
Speaker 0: Это к вопросу о том, какие профессии можно заменить.

201
00:17:37,120 --> 00:17:37,960
Speaker 0: Вот 90%...

202
00:17:38,280 --> 00:17:55,820
Speaker 1: Там среди этих 20 было ведущие, так как... по сути, по содержанию речи заменителя в нейросети искать было нечего, она обратила внимание, ну посмотрите, у неё же неестественно двигаются губы, мы видим, что очевидно, что не совпадают звуки и артикуляция, но это ещё типа это не работает.

203
00:17:56,000 --> 00:18:00,060
Speaker 1: Так что самое очевидное, что вопрос двух-трёх итераций апдейта системы.

204
00:18:00,260 --> 00:18:01,520
Speaker 1: Здесь сказать просто нечего.

205
00:18:01,640 --> 00:18:09,500
Speaker 1: Ну я сразу говорю, что на мой взгляд политических не только обозревателей, но и... политиков, по большому счету, можно будет заменять GPT.

206
00:18:09,820 --> 00:18:10,980
Speaker 0: Демократических политиков.

207
00:18:11,080 --> 00:18:14,040
Speaker 0: То есть именно тех политиков, которые являются менеджерами.

208
00:18:14,200 --> 00:18:17,660
Speaker 0: Отдельная тема того, что политика не должна была быть менеджерством.

209
00:18:17,780 --> 00:18:20,220
Speaker 0: То есть политика и управление — это все-таки разные вещи.

210
00:18:20,500 --> 00:18:31,220
Speaker 0: Проблема в том, что в мире позднего капитализма эти вещи практически слились воедино, потому что политики обсуждают все угодное, кроме стратегически важных, сакральных вопросов, сложных вопросов и так далее.

211
00:18:31,440 --> 00:18:31,980
Speaker 0: Ценностных.

212
00:18:32,140 --> 00:18:38,320
Speaker 1: И это вопросов выборов, вопросов тогда, когда нужно определить свою позицию и сказать там, кто ты, какой ты.

213
00:18:38,760 --> 00:18:45,740
Speaker 1: Они будут повторять какой-то набор общепринятых формулировок, но ничего не скажут о себе для того, чтобы их не уличили.

214
00:18:45,980 --> 00:18:49,140
Speaker 0: И именно политика такого типа нейросеть заменяет в разы лучше.

215
00:18:49,820 --> 00:19:07,920
Speaker 0: А вот политика настоящего, политика, которая либо играет во власти, либо занимается проблемами власти, либо политика, которая является производным от некой миссии, вот эту нейросеть будет заменить опять-таки крайне сложно, потому что такие люди опять-таки делают что-то новое и способны на то, что нейросеть, наверное, не до конца способна.

216
00:19:08,000 --> 00:19:12,020
Speaker 0: в полном объеме на ответственность личную за принятое решение.

217
00:19:12,300 --> 00:19:14,660
Speaker 0: Вот это очень важный момент, с чего нейросеть не может.

218
00:19:14,880 --> 00:19:22,580
Speaker 1: И от чего всячески уклоняется сегодня, я говорю ответственно, как человек, который провел внутри корпорации не один десяток лет.

219
00:19:22,680 --> 00:19:34,480
Speaker 1: Профессиональный менеджер профессионально уклоняется от того, чтобы взять на себя ответственность за что-либо и артикулировать ясно какой-то не общепринятый тезис, сделать какой-то выбор.

220
00:19:34,660 --> 00:19:37,480
Speaker 1: И, видимо, нейросеть не оставит таким людям шанса.

221
00:19:38,000 --> 00:19:41,340
Speaker 1: То, что не может сделать нейросеть, она не может сказать, это хорошо, это плохо.

222
00:19:41,400 --> 00:19:46,320
Speaker 1: что добро – это зло, я выбираю это и несу ответственность за такой поступок.

223
00:19:46,740 --> 00:19:48,280
Speaker 0: Вот это нейросеть как раз сделать не может.

224
00:19:48,320 --> 00:19:51,100
Speaker 0: Все остальное нейросеть сделать может прекрасно, в разы лучше.

225
00:19:51,180 --> 00:19:53,860
Speaker 0: Причем вот здесь чисто капиталистически, чисто рыночно.

226
00:19:53,920 --> 00:20:03,500
Speaker 0: Я гарантирую, что с точки зрения чисто рыночной капитализации нейросети действительно в нескольких итерациях будут показывать результаты в разы лучше, чем огромное большинство современных наемных менеджеров.

227
00:20:03,560 --> 00:20:08,620
Speaker 0: Только нейросети не надо платить денег и не нужны бонусы и опционы в вашей корпорации.

228
00:20:08,780 --> 00:20:26,060
Speaker 1: Хотелось бы напоследок в заключении вот этого нашего выпуска сказать, что на мой взгляд... Очень мало кто сегодня осознаёт масштаб изменений, которые стоят перед нами, связанные с появлением настолько хорошо построенного интерфейса к нейросети.

229
00:20:26,360 --> 00:20:32,120
Speaker 1: То есть нейросеть — это изобретение не этого года, но выход нейросети к потребителю, к широким массам.

230
00:20:32,540 --> 00:20:39,220
Speaker 1: Только сейчас в сфере программирования появляются восхищенные возгласы о том, что, посмотрите, 11 центов в 3 часа.

231
00:20:39,600 --> 00:20:43,900
Speaker 1: У меня есть код, который бы потребовал двух недель и 5000 долларов.

232
00:20:44,080 --> 00:20:46,920
Speaker 1: А это будет внедряться, внедряться и внедряться.

233
00:20:47,080 --> 00:20:51,840
Speaker 1: И скоро приложений на основе искусственного интеллекта будет не десятки, а тысячи.

234
00:20:52,060 --> 00:20:56,260
Speaker 1: Скоро это все будет внутри игр, внутри любых наших бытовых помощников.

235
00:20:56,520 --> 00:20:57,240
Speaker 0: Да, это абсолютно.

236
00:20:57,600 --> 00:21:01,680
Speaker 0: Ты провел пример программистов, потому что это очень важный пример именно программирования.

237
00:21:01,800 --> 00:21:05,160
Speaker 0: воспринимала с большинством людей как некая удобная гавань.

238
00:21:05,300 --> 00:21:14,800
Speaker 0: То есть буквально последние вот пять лет среди моих друзей и знакомых, извините за личные примеры, человек 10-15 сменила профессию, ушел в программист и стал быстро получать огромные деньги.

239
00:21:14,980 --> 00:21:18,720
Speaker 0: Но проблема в том, что тот тип работ, которыми они занимаются, как раз нейросеть заменит.

240
00:21:18,780 --> 00:21:20,920
Speaker 0: В большем случае заменит очень быстро.

241
00:21:20,980 --> 00:21:22,980
Speaker 0: То есть даже это не становится удобной гаванью.

242
00:21:23,280 --> 00:21:25,240
Speaker 0: Зона академической деятельности.

243
00:21:25,700 --> 00:21:27,580
Speaker 0: Нейросеть заменит большую часть академиков.

244
00:21:27,840 --> 00:21:29,320
Speaker 0: Вот это я тоже абсолютно уверен.

245
00:21:29,420 --> 00:21:34,620
Speaker 0: Просто потому то, что большую часть академиков Академика сегодня академической науки не занимается производством нового.

246
00:21:34,940 --> 00:21:48,720
Speaker 1: То есть вся неэффективность, весь интеллектуальный чепуха, вся эта имитация, она должна будет неизбежно сметена, потому что будет простой, трезвый, неэмоциональный инструмент, который жёстко эффективнее будет.

247
00:21:49,040 --> 00:21:50,980
Speaker 0: Давай даже так, сметена в каком смысле?

248
00:21:51,180 --> 00:21:52,660
Speaker 0: Объем текста не изменится.

249
00:21:52,860 --> 00:21:54,600
Speaker 0: Ну, просто человеку его писать не будет.

250
00:21:54,640 --> 00:21:56,680
Speaker 0: То есть она будет сметена именно на человеческом уровне.

251
00:21:56,740 --> 00:22:08,340
Speaker 0: Но, опять-таки, мне очень нравится пример, что, условно говоря, за последние 60 лет написана большая часть научных работ в гуманитарном знании, чем за весь предыдущий период истории человечества.

252
00:22:08,580 --> 00:22:15,180
Speaker 0: Но любой человек, который хоть немного занимался этими вопросами, он понимает, что ценности всех этих работ, они обратные.

253
00:22:15,580 --> 00:22:17,880
Speaker 0: Работы такого рода заменят нейросеть.

254
00:22:18,100 --> 00:22:18,900
Speaker 0: Вообще, без вопросов.

255
00:22:18,960 --> 00:22:25,200
Speaker 0: Потому что, условно говоря, простите, если кого оскорблю, но стандартную работу по gender studies нейросеть вам запишет вот так вот.

256
00:22:25,400 --> 00:22:30,360
Speaker 0: Просто сводными данными на основе уже миллиона работ, которые вы написали, подставя переменные.

257
00:22:30,520 --> 00:22:31,480
Speaker 0: Это вообще не проблема.

258
00:22:32,020 --> 00:22:33,400
Speaker 0: А это считается вершиной.

259
00:22:33,440 --> 00:22:41,180
Speaker 0: То есть общество ценило эти работы настолько, что человек вкладывал 25 лет успешного труда, огромных денежных инвестиций, чтобы получить эту компетенцию.

260
00:22:41,460 --> 00:22:43,620
Speaker 0: А теперь это будет делать машина за бесплатно.

261
00:22:43,880 --> 00:22:44,680
Speaker 0: Вот просто пример.

262
00:22:44,800 --> 00:22:53,920
Speaker 0: Это мы только копаем, показываем, вскрываем малюсенькие некоторые аспекты вот этих перемен, которые только нам в голову приходят через наш опыт.

263
00:22:53,960 --> 00:23:10,360
Speaker 0: Я подозреваю, что в вашей роде деятельности, уважаемые слушатели, вы легко придумываете свой вариант нейросетичного вызова, краш-теста для вашей деятельности, для юриспруденции, для маркетологов, для любого customer-facing job, что называется, то есть для любого общения с клиентами и так далее и тому подобное.

264
00:23:10,400 --> 00:23:22,100
Speaker 1: Ну да, то есть можно представить себе, что вот появление смартфонов с камерами и с удобными интерфейсами для взаимодействия со временем привело к просто радикальному изменению повседневной жизни.

265
00:23:22,400 --> 00:23:28,740
Speaker 1: Изменения, которые нас ждут, связанные с GPT, очевидно, будут гораздо более драматическими.

266
00:23:28,820 --> 00:23:34,460
Speaker 1: Они коснутся всех областей и непосредственно связаны с темой нашего разговора с поисками смысла.

267
00:23:34,760 --> 00:23:50,520
Speaker 1: Вполне возможно, что не обретя вот этот самый смысл своей жизни или не понимая смысл происходящего, человек скукожится до уровня личинки в какой-то скорлупе, которая им приготовит этот искусственный интеллект, и он там потихонечку, да.

268
00:23:51,160 --> 00:23:53,320
Speaker 0: Очень комфортный скорлупе, очень важно подчеркнуть.

269
00:23:53,400 --> 00:23:55,660
Speaker 0: Эта скорлупа может быть предельно комфортная.

270
00:23:56,160 --> 00:24:00,340
Speaker 1: И стлеет там, и в конце концов вот такого рода личинка и останется.

271
00:24:00,500 --> 00:24:03,600
Speaker 1: А те, кто выберут другой путь, вылупятся во что-то большее.

272
00:24:03,920 --> 00:24:16,120
Speaker 1: Но этот выбор будет безжалостным, беспощадным, и от него будет не уйти, и пропетлять уже не удастся, как петляют сегодня множество людей в псевдо-творческой, псевдо-интеллектуальной деятельности.

273
00:24:16,600 --> 00:24:18,640
Speaker 0: Это приведет к социальным потрясениям.

274
00:24:18,740 --> 00:24:22,300
Speaker 0: Это уже начнет приводить к социальным потрясениям в этом году такими темпами.

275
00:24:22,520 --> 00:24:31,920
Speaker 0: Мы видим волну увольнений в Фейсбуке, мы видим волну увольнения в Гугле, мы увидим еще гораздо больше волну увольнения в тех областях, которые считались защищенными.

276
00:24:32,140 --> 00:24:43,440
Speaker 0: Социальные потрясения неизбежно приведут к политическим потрясениям, полноценным, потому что придется входить в новый мир, придется создавать новые системы политических отношений для регуляции новых социальных отношений.

277
00:24:43,740 --> 00:24:45,900
Speaker 0: И это, повторюсь, все будет происходить очень быстро.

278
00:24:46,200 --> 00:24:47,300
Speaker 1: На наших глазах произойдет.

279
00:24:47,660 --> 00:24:51,940
Speaker 0: В этом смысле можно сказать, наверное, что час GPT является одним из садников апокалипсиса.

280
00:24:52,440 --> 00:24:52,680
Speaker 1: Ух ты.

281
00:24:53,120 --> 00:25:03,480
Speaker 0: Ну смотри, если мы рассматриваем апокалипсис просто как мета-символическую модель любого качественного эпохального перемены из одного этапа, условно говоря, в некий другой этап.

282
00:25:04,100 --> 00:25:14,220
Speaker 0: то всадники в этой метафоре являются просто теми факторами, которые обстоятельствами непреодолимой силы, которые делают старый образ жизни невозможным и заставляют переходить в новый образ жизни.

283
00:25:14,540 --> 00:25:19,600
Speaker 0: Обычно в истории такими вещами как раз и являются болезнь, война, технология, революция.

284
00:25:19,980 --> 00:25:27,600
Speaker 1: Попробуем завершить всё-таки на несколько более позитивной ноте, потому что всё-таки есть надежда, что эта технология будет принуждать.

285
00:25:28,080 --> 00:25:36,020
Speaker 1: стимулировать человека быть более человеком, чем предыдущие технологии, которые, казалось, его превращают в какой-то предаток.

286
00:25:36,300 --> 00:25:38,260
Speaker 0: Она будет одновременно делать обе вещи.

287
00:25:38,460 --> 00:25:40,160
Speaker 0: Это к вопросу свободы, воли и выбора.

288
00:25:40,220 --> 00:25:52,140
Speaker 0: То есть она одновременно будет делать максимально комфортный выбор в пользу вещи, потому что именно я тебе скидывал книгу Несмелого «Абсолютное счастье как достижение точки абсолютного комфорта».

289
00:25:52,360 --> 00:25:57,160
Speaker 0: Ничто лучше не продвигнет в сторону вот этого материалистического понимания счастья, чем Чаджи.

290
00:25:57,760 --> 00:26:06,660
Speaker 0: Но именно же он одновременно делает предельно ценным то, что раньше удавалось симулировать или заглушать спамом в цивилизации позднего капитализма.

291
00:26:06,940 --> 00:26:09,100
Speaker 0: То есть в этом, наверное, вообще истинная суть вызова.

292
00:26:09,180 --> 00:26:16,020
Speaker 0: То, что оба этих процесса происходят одновременно, и вопрос, в какую сторону двигаться лично вам, решаете лично вы.

293
00:26:16,260 --> 00:26:18,220
Speaker 0: И ответственность на вас, ваш выбор.

294
00:26:18,500 --> 00:26:20,280
Speaker 1: И свалить ни на кого не удастся.

295
00:26:20,580 --> 00:26:39,360
Speaker 0: Да, свалить ни на кого не удастся, поэтому к позитивной программе срочно начинаем все коллективно инвестировать в человеческое в человеке, в наши духовные способности, в наши духовные практики, в нашу способность различать добро и зло, хорошо или плохо, в нашу способность нести ответственность за этот выбор, в нашу способность думать, в нашу способность говорить.

296
00:26:39,480 --> 00:26:43,680
Speaker 0: То есть именно беседа становится ключевым навыком в мире чата GTP.

297
00:26:43,960 --> 00:26:47,640
Speaker 0: А этот навык мы практически потеряли, потому что мы разговаривали с другом-то особо-то.

298
00:26:47,720 --> 00:26:48,060
Speaker 0: не любим.

299
00:26:48,180 --> 00:26:58,920
Speaker 0: Все смс-ками и голосовыми сообщениями обмениваемся, вместо того, чтобы встретиться лицом к лицу с человеком и провести с ним вот это глубинное погружение, создать в среду, в которой может родиться что-то новое.

300
00:26:59,060 --> 00:27:00,760
Speaker 0: А именно это становится самым ценным.

301
00:27:00,920 --> 00:27:05,580
Speaker 0: Соответственно, ищите людей подобных вокруг себя, создавайте людей подобных вокруг себя.

302
00:27:06,220 --> 00:27:07,700
Speaker 0: Объединяйтесь по этому признаку.

303
00:27:08,040 --> 00:27:27,960
Speaker 1: И как один из признаков этого будущей комьюнити, мне бы хотелось видеть наших слушателей в сообществе тех, с кем можно общаться свободно, открыто и продуктивно, где целью будет совместный поиск истины, а не улетворение каких-то нарциссических потребностей.

304
00:27:28,240 --> 00:27:32,540
Speaker 0: Не нарративная война, не нарративное погружение, а поиск истины.

305
00:27:32,800 --> 00:27:34,080
Speaker 0: Здесь будущее есть.

306
00:27:34,660 --> 00:27:34,880
Speaker 0: Аминь.

307
00:27:35,100 --> 00:27:55,820
Speaker 0: До новых встреч, уважаемые слушатели.

