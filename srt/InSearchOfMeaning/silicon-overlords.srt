1
00:00:09,020 --> 00:00:10,140
Speaker 1: Здравствуйте, друзья!

2
00:00:10,580 --> 00:00:14,000
Speaker 1: В эфире очередной выпуск подкаста «В поисках смысла».

3
00:00:14,380 --> 00:00:15,860
Speaker 1: У микрофона Евгений Голуб.

4
00:00:16,460 --> 00:00:17,080
Speaker 0: И Павел Шерин.

5
00:00:17,160 --> 00:00:17,660
Speaker 0: Здравствуйте!

6
00:00:17,800 --> 00:00:30,440
Speaker 1: Итак, сегодня мы решили поговорить о технофеодалах, о тех людях, которые не стесняются заявлять о том, что они понимают, как будет выглядеть мир в будущем.

7
00:00:30,920 --> 00:00:40,440
Speaker 1: Они не только об этом заявляют, но они ещё фактически формируют эту картинку будущего, в которой… у них есть определённая ведущая роль.

8
00:00:40,880 --> 00:00:44,660
Speaker 1: Я хочу передать слово Павлу, чтобы он поразмышлял на эту тему.

9
00:00:44,880 --> 00:00:47,620
Speaker 0: Ну давайте краткое содержание предыдущих серий.

10
00:00:47,660 --> 00:00:55,520
Speaker 0: Вообще основная тема этого сезона, как вы могли заметить, искусственный интеллект, человек, человечность и вообще дивный новый мир во всех его проявлениях.

11
00:00:55,900 --> 00:01:09,360
Speaker 0: И вот интересная часть этого дивного нового мира, конечно, сегодня — это те люди, которые де-факто реально приобретают статус новой как бы... с очень большими оговорками.

12
00:01:09,520 --> 00:01:10,300
Speaker 0: Почему новый?

13
00:01:10,740 --> 00:01:12,840
Speaker 0: Во-первых, потому что есть аристократия старая.

14
00:01:13,420 --> 00:01:14,780
Speaker 0: И она никуда не уходила.

15
00:01:15,180 --> 00:01:24,340
Speaker 0: И вопрос, насколько новые аристократы по-настоящему являются самостоятельными субъектами, в какой они являются болванчиками-исполнителями, вопрос открытый.

16
00:01:24,560 --> 00:01:35,920
Speaker 0: Напоминаю всем легендарный вызов Цукерберга на ковер в Сенат, где очень было видно, скажем так, пределы вот этой внутренней смелости, честности и силы этих самых новых аристократов.

17
00:01:36,300 --> 00:01:41,380
Speaker 0: То есть насколько они вытянут эту планку, для меня вопрос является пока не самоочевидным.

18
00:01:42,620 --> 00:01:49,320
Speaker 0: То, что их внутреннее мироощущение, как новых аристократов, с этим спорить мне крайне сложно.

19
00:01:49,560 --> 00:01:51,020
Speaker 0: Какие его характерные признаки?

20
00:01:51,080 --> 00:01:53,000
Speaker 0: Ну, давайте, во-первых, сразу о каком идет речь.

21
00:01:53,120 --> 00:01:55,980
Speaker 0: Речь идет о техно-олигархах.

22
00:01:56,280 --> 00:02:00,620
Speaker 0: Сэм Альтман, Безос, Цукерберг, Маск и Ижи Тиль с ними.

23
00:02:00,960 --> 00:02:03,680
Speaker 0: В общем, открывайте список всех этих корпоративных стратегов.

24
00:02:04,060 --> 00:02:05,100
Speaker 0: Что мне интересно?

25
00:02:05,220 --> 00:02:13,320
Speaker 0: Первое, вот на фоне общего... скажем так, напряжение, а мы в целом живем в очень, мне кажется, напряженную эпоху, эти люди прям фонтанируют оптимизмом.

26
00:02:13,640 --> 00:02:19,260
Speaker 0: Вот, конечно, их поразительный оптимизм вызывает ощущение иногда даже легкой придурковатости.

27
00:02:19,620 --> 00:02:25,720
Speaker 0: Можно было бы возникнуть, можно было бы даже поставиться вопросом, а не понимают ли они чего-то, чего не понимаю я?

28
00:02:26,100 --> 00:02:28,280
Speaker 0: Порадоваться, так сказать, за будущее человечества.

29
00:02:28,560 --> 00:02:32,860
Speaker 0: Но после прочтения тех текстов, которые они пишут, за редкими исключениями.

30
00:02:33,380 --> 00:02:34,600
Speaker 0: Выводы приводят в самый обратный.

31
00:02:34,640 --> 00:02:41,980
Speaker 0: К сожалению, это они не понимают базовых вещей относительно человеческой природы и демонстрируют вопиющую философскую неграмотность.

32
00:02:42,140 --> 00:02:45,780
Speaker 0: Так что, когда, если вы думали, кто достигнет нас, одна постучались.

33
00:02:46,180 --> 00:03:00,340
Speaker 0: А те философские работы, которые демонстрируют именно глубокую философскую подготовку, я говорю здесь прежде всего о Питере Тиле, ну, скажем так, это тот вектор, от которого тоже волосы уже встают дыбом, потому что там становится совсем страшно.

34
00:03:00,620 --> 00:03:02,100
Speaker 1: Так, минуточку, минуточку.

35
00:03:02,200 --> 00:03:09,240
Speaker 1: Тут нас любой технооптимист остановит и скажет, ну, если ты такой умный, Павел Щелин, где твои деньги?

36
00:03:09,600 --> 00:03:17,780
Speaker 1: Ты смотри, ребята же, наверное, как-то умеют деньги зарабатывать, что-то соображают, и критиковать их за то, что они как будто бы что-то не понимают.

37
00:03:17,900 --> 00:03:18,920
Speaker 1: Ну и что они понимают?

38
00:03:19,000 --> 00:03:23,860
Speaker 1: Это им не мешает хорошо жить, много очень денег зарабатывать, продвигать технологии.

39
00:03:24,060 --> 00:03:30,140
Speaker 1: Вот это различие между... интеллектом и умом, мудростью, для многих не очевидно.

40
00:03:30,460 --> 00:03:42,140
Speaker 1: Давайте сделаем паузу, потому что нас уже по первым выпускам начинают осуждать лёгкими такими комментариями относительно того, что вот и тот вам не нравится, и этого вы, видите ли, критикуете высокомерно.

41
00:03:42,400 --> 00:03:45,900
Speaker 1: Но ведь это совершенно разные домины знаний и умений.

42
00:03:46,120 --> 00:03:48,120
Speaker 0: Тут есть несколько принципиальных вопросов.

43
00:03:48,240 --> 00:03:52,800
Speaker 0: Первое, разумеется, я и близко не ставлю под сомнение.

44
00:03:53,720 --> 00:04:01,700
Speaker 0: талант и компетенции этих людей в максимизации прибыли их корпораций и осуществлении успешных бизнес-решений.

45
00:04:01,960 --> 00:04:09,380
Speaker 0: Проблема заключается в том, что современный мир построен на гиперспециализации.

46
00:04:09,740 --> 00:04:22,720
Speaker 0: Другими словами... В отличие от античности, в отличие от страшного, мрачного в кавычках средневековья, цельного человека сегодня найти практически невозможно, и я тоже к ним не принадлежу.

47
00:04:22,860 --> 00:04:27,620
Speaker 0: Мои знания в вопросах физики, химии, биологии весьма и весьма ограничены.

48
00:04:28,040 --> 00:04:29,680
Speaker 0: Скажем так, эту вещь признаю.

49
00:04:29,980 --> 00:04:32,220
Speaker 0: В себе понимаю зоны своих ограничений.

50
00:04:32,560 --> 00:04:47,160
Speaker 0: Проблема с уважаемыми бизнесменами в том, что они в 90% случаях позволяют себе рассуждать о вопросах философских, де-факто, этических, моральных, простите, не владея ни малейшими знаниями об этой сфере.

51
00:04:47,680 --> 00:05:01,740
Speaker 1: Обычная ситуация, когда глава успешной корпорации, не обязательно IT, начинает представлять, что его успех позволяет ему содержательно рассуждать о разного рода вещах за пределами его непосредственной компетенции.

52
00:05:02,040 --> 00:05:12,540
Speaker 1: А я вспоминаю случай из относительно недавнего прошлого, когда один из владельцев крупной компании «Новая почта» в Украине полемизировал с Андреем Олеговичем Боумейстером.

53
00:05:12,980 --> 00:05:17,360
Speaker 1: Это кто-то метко назвал диалогом профессора с Википедией.

54
00:05:17,700 --> 00:05:21,400
Speaker 1: Но при этом это никак не повлияло на самооценку этого человека.

55
00:05:21,480 --> 00:05:26,280
Speaker 1: Вот это эффект больших денег, эффект значимости богатых людей в обществе.

56
00:05:26,640 --> 00:05:28,700
Speaker 1: Вот он в том числе имеет и такую сторону.

57
00:05:28,800 --> 00:05:42,620
Speaker 1: То есть если я сумел организовать успешный бизнес, инновационный, еще тем более высокотехнологичный, конечно же, я становлюсь лидером мнений, меня слушают, и как же тут не изречь какую-нибудь там мудрость, так сказать, чтобы люди внимали.

58
00:05:43,040 --> 00:05:47,340
Speaker 1: Это, к сожалению, я не знаю, было ли такое там несколько столетий назад, кажется, нет.

59
00:05:47,680 --> 00:05:49,460
Speaker 0: Я, собственно, хотел рассказать, как это было.

60
00:05:49,620 --> 00:05:54,240
Speaker 0: На самом деле точно такой же процесс происходил в то время Перехода.

61
00:05:54,820 --> 00:05:58,360
Speaker 0: от мира Средневековья к миру Просвещения.

62
00:05:58,740 --> 00:06:05,020
Speaker 0: И, собственно, ты сказал ключевое слово, диалог философа с Википедией, но вспомни, кто создал первую энциклопедию.

63
00:06:05,380 --> 00:06:05,960
Speaker 0: Просвещенцы.

64
00:06:06,500 --> 00:06:07,940
Speaker 1: Но они создают такую иллюзию знати.

65
00:06:08,240 --> 00:06:09,680
Speaker 0: Ее ради этого и создавали.

66
00:06:09,740 --> 00:06:20,160
Speaker 0: То есть масштабы-то, конечно, там были заявки, то, что мы создаем компедиум всех знаний человечества, который все это делает на просвещенном базовом фундаменте, свободном от предрассудков.

67
00:06:20,260 --> 00:06:23,100
Speaker 0: Но в итоге качество этой энциклопедии объективное.

68
00:06:23,380 --> 00:06:26,500
Speaker 0: Это вот аналог Википедии в худших ее проявлениях.

69
00:06:26,560 --> 00:06:35,060
Speaker 0: То есть это передергивание, некачественная информация, откровенно говоря, искажение, переворот, ну или просто высказывание банальностей с видом знания.

70
00:06:35,400 --> 00:06:41,540
Speaker 0: при полной потере неких фундаментальных даже философских принципов мышления и уж тем более всякой глубины.

71
00:06:41,780 --> 00:06:42,960
Speaker 0: Но очень важный момент.

72
00:06:43,020 --> 00:06:51,580
Speaker 0: Точно так же, как сегодня, владение, чтение этой Википедии позволяло ее носителю ощущать себя очень образованным.

73
00:06:52,100 --> 00:06:53,620
Speaker 1: Эта же штука дорогая была, да?

74
00:06:53,840 --> 00:06:56,940
Speaker 1: Книги в прекрасных переплетах, много.

75
00:06:57,100 --> 00:07:07,320
Speaker 1: Вот богатый купец покупал себе этот набор книг, ставил на полочку и выглядел, в общем, компетентным, образованным человеком в глазах окружающих.

76
00:07:07,460 --> 00:07:08,380
Speaker 0: В глазах окружающих.

77
00:07:08,600 --> 00:07:12,020
Speaker 0: Эти люди с условными университетскими схоластами не спорили.

78
00:07:12,360 --> 00:07:14,660
Speaker 0: Они не ходили на споры туда, ну или в твоей метафоре.

79
00:07:14,700 --> 00:07:16,260
Speaker 0: Почему я так зацепил этот сюжет?

80
00:07:16,640 --> 00:07:17,480
Speaker 0: Вот идеальный пример.

81
00:07:17,680 --> 00:07:27,900
Speaker 0: То есть если бы, когда они иногда оказывались в одной гостиной с человеком, который получил допросвященческое образование, они выглядели, откровенно говоря, некомпетентными идиотами.

82
00:07:28,180 --> 00:07:29,540
Speaker 0: Но поэтому они с ними не спорили.

83
00:07:29,680 --> 00:07:36,740
Speaker 0: Чаще всего они выбирали для самоутверждения какого-то действительно менее образованного, условный паттерн сельского священника.

84
00:07:36,980 --> 00:07:44,020
Speaker 0: Вот начитавшись в Википедии, действительно можно очень хорошо оттоптаться на сельском священнике каком-нибудь или на не очень образованном папе.

85
00:07:44,280 --> 00:07:46,580
Speaker 0: Не имею в виду не римский, а твой обычный там отец, дядя.

86
00:07:46,640 --> 00:07:53,800
Speaker 0: Вот он там всю жизнь прослужил в полку, ходит по воскресеньям в церковь и, скажем так, при этом... Особо книжек не читал.

87
00:07:53,880 --> 00:08:00,640
Speaker 0: У него есть четкое мировоззрение, он в нем уверен, но объяснить тебе на уровне силологизмов от него, в принципе, это никогда не требовалось.

88
00:08:00,700 --> 00:08:10,700
Speaker 0: И тут ты приходишь, начитавшись этой энциклопедией, особенно если ты приехал из города Парижа в какой-нибудь провинциальный город Лион, и все, и ты на коне, и ты лидер мнений.

89
00:08:10,940 --> 00:08:15,280
Speaker 0: И вот эта фраза «лидер мнений» современная, она корнями уходит именно туда.

90
00:08:15,380 --> 00:08:18,640
Speaker 0: Это то, что называется в историографии «салонная культура».

91
00:08:18,900 --> 00:08:26,540
Speaker 0: То есть салонная культура — это на самом деле вопиющая необразованность, преподнесённая как моральное превосходство.

92
00:08:26,940 --> 00:08:32,820
Speaker 1: Фактически с появлением социальных сетей эта салонная культура, в общем-то, утысячерилась.

93
00:08:33,039 --> 00:08:34,299
Speaker 1: Именно в таком же формате.

94
00:08:34,700 --> 00:08:38,179
Speaker 1: Начитались в Википедии, пошли спорить и отстаивать свое мнение.

95
00:08:38,440 --> 00:08:49,980
Speaker 1: А тут получается какой-нибудь Альтман, который грамотно, последовательно развивал технологию, непосредственно относящуюся к экспертизе и к умению, может быть, в кавычках.

96
00:08:50,380 --> 00:08:58,480
Speaker 1: мыслить, менять полностью парадигму исследований научных и так далее, ну, конечно, он должен считать себя человеком, который компетентен просто во всем.

97
00:08:58,840 --> 00:09:12,480
Speaker 1: Если мы задаем вопрос через веб-интерфейс чата GPT, то он-то может задать вопрос гораздо с меньшими ограничениями на любую тему, и это наверняка у него уже давно родило такое внутреннее ощущение своего превосходства интеллектуального.

98
00:09:12,980 --> 00:09:13,500
Speaker 1: Ну, плюс деньги.

99
00:09:13,700 --> 00:09:14,520
Speaker 0: Плюс деньги, конечно.

100
00:09:14,660 --> 00:09:16,520
Speaker 0: Повторюсь, принцип такой, и мне понравилось.

101
00:09:16,860 --> 00:09:18,520
Speaker 0: Это метафора, давай ее зафиксируем.

102
00:09:18,580 --> 00:09:24,720
Speaker 0: То, что действительно вот эта салонная культура, социальные сети действительно просто стали такой же культурой, но в тотальном масштабе.

103
00:09:24,980 --> 00:09:33,820
Speaker 0: И у салонной культуры есть несколько объективных признаков, которые мы можем наблюдать в прямом эфире и, скажем так, пожинать их плоды.

104
00:09:34,100 --> 00:09:37,440
Speaker 0: Первое — это действительно, как ни крути, размытие компетенции.

105
00:09:37,720 --> 00:09:48,840
Speaker 0: Пойми правильно, ты сам знаешь, мы с тобой... в наших предыдущих сезонах постоянно критикуем, скажем так, вульгарный рационализм, типа наука превыше всего, бог науки, поэтому я не с этой позиции говорю.

106
00:09:49,120 --> 00:09:57,060
Speaker 0: Но смысл в том, что действительно в условиях салона для неподготовленной аудитории крайне сложно оценить качество высказывания.

107
00:09:57,360 --> 00:10:01,620
Speaker 0: На длинной дистанции это может всегда решаться феноменом репутации.

108
00:10:01,860 --> 00:10:04,460
Speaker 0: Ну, репутация создается, потом репутация гибнет.

109
00:10:04,860 --> 00:10:12,060
Speaker 0: Наша современная салонная культура, она же от предыдущей отличается еще тем, по форме, что она еще более быстрая.

110
00:10:12,380 --> 00:10:19,220
Speaker 0: Первая версия салонной культуры была на порядок более быстрее, чем, если тебе угодно, предшествующая эпоха.

111
00:10:19,300 --> 00:10:19,780
Speaker 0: Ну, условно.

112
00:10:20,060 --> 00:10:25,840
Speaker 0: До салонной культуры тебе надо не фильетон было писать и не красивую полемическую заметку, а трактат.

113
00:10:26,200 --> 00:10:28,020
Speaker 0: Хочешь о чем-то поспорить, напиши трактат.

114
00:10:28,380 --> 00:10:30,580
Speaker 0: Даже если ты споришь с какой-то позицией.

115
00:10:31,020 --> 00:10:34,840
Speaker 0: Тот же Эразм Роттердамский, ему приходилось писать условно «Корабль дураков».

116
00:10:35,140 --> 00:10:35,760
Speaker 0: Но это текст.

117
00:10:36,000 --> 00:10:37,700
Speaker 0: Это большой такой текст и прочее.

118
00:10:37,900 --> 00:10:41,720
Speaker 0: В XVIII веке уже можно писать саркастические сфильетонские заметки.

119
00:10:41,980 --> 00:10:44,300
Speaker 0: То есть это все равно ускорение текстового жанра.

120
00:10:44,520 --> 00:10:52,280
Speaker 0: Пусть сегодня ускорение достигло такой степени, что, в принципе, отрефлексировать позицию человека практически невозможно.

121
00:10:52,520 --> 00:11:04,660
Speaker 1: Ты имеешь в виду, что просто остановиться и попытаться понять, логику и аргументацию, обоснованность высказывания, не хватает времени, надо уже отвечать на следующий тезис.

122
00:11:04,980 --> 00:11:10,720
Speaker 1: И вообще люди разговаривают тезисами, утверждениями и забывают о том, что любое утверждение надо как-то обосновывать.

123
00:11:10,960 --> 00:11:17,840
Speaker 1: Если оно звучит и подано хорошим, глубоким, правильным голосом, быстро и авторитетно, оно сразу считается как доказанное.

124
00:11:18,060 --> 00:11:20,100
Speaker 0: Если говорить по-простому, это прикладная магия.

125
00:11:20,180 --> 00:11:24,360
Speaker 1: Ну, я бы назвал это риторикой, ну, как бы такое, да, ну, допустим.

126
00:11:24,860 --> 00:11:32,640
Speaker 0: Давай прямо говорить, есть много спикеров, которые, собственно, свою аудиторию собирают не содержание высказывания, а формы высказывания.

127
00:11:32,940 --> 00:11:41,120
Speaker 0: Базовый смысл в том, что у людей идет запрос на определенное, скажем так, эмоционально-психологическое воздействие от той или иной речи.

128
00:11:41,200 --> 00:11:45,340
Speaker 0: И давай не скрывать, часть нашей аудитории тоже к нам относится примерно точно так же.

129
00:11:45,680 --> 00:11:55,560
Speaker 0: Это всегда было, и есть объективная издержка, есть вопрос только в том, понимаешь ли ты этот риск и пытаешься ли ты условно не злоупотреблять, скажем так, этими особенностями современной культуры.

130
00:11:55,680 --> 00:11:56,480
Speaker 0: Ну с чего мы начали?

131
00:11:56,760 --> 00:12:07,280
Speaker 0: То, что в современной среде очень легко выглядеть образованным, очень легко выглядеть авторитетным, особенно если тому есть сопутствующие внешние признаки.

132
00:12:07,660 --> 00:12:13,760
Speaker 0: Проблема в том, что на качество аргументов и на качество компетенции само по себе это все равно никак не влияет.

133
00:12:14,040 --> 00:12:30,060
Speaker 0: Конкретно то, что мы обсуждали за кадром, можем немножко этого коснуться, это была вот эта эссе знаменитая, написанная Сэмом Альтманом после его блога, где он показывает абсолютную некритическую... в хорошем смысле этого слова, рефлексию относительно базовых вопросов.

134
00:12:30,300 --> 00:12:44,260
Speaker 0: Ну, допустим, вопрос о том, что категория пользы, о которой много говорит, что вот мы сейчас создаем дополнительную категорию пользы, дополнительную категорию счастья, невозможно осознать вне этического контекста, который он вообще не задает.

135
00:12:44,560 --> 00:12:46,300
Speaker 1: Что значит категория пользы?

136
00:12:46,660 --> 00:12:50,220
Speaker 0: К развитию искусственного интеллекта принесет огромную пользу человечеству.

137
00:12:50,440 --> 00:12:51,580
Speaker 0: Высказывание такого рода.

138
00:12:51,960 --> 00:12:53,500
Speaker 0: Понятное, нормальное высказывание.

139
00:12:53,700 --> 00:13:06,480
Speaker 0: Проблема в том, что с философской точки зрения, если мы об этом пишем, то есть это нормальный текст для маркетолога, но с философской точки зрения утверждение является бессмысленным до тех пор, пока ты не определила, в какой системе координат ты свою пользу оцениваешь.

140
00:13:06,820 --> 00:13:11,200
Speaker 1: Ну, конечно, потому что любое хорошо или плохо имеет смысл только в системе координат.

141
00:13:11,260 --> 00:13:14,040
Speaker 1: А польза — это априори что-то хорошее.

142
00:13:14,260 --> 00:13:17,380
Speaker 1: Значит, нужно определить, что такое хорошо в этой системе координат.

143
00:13:17,440 --> 00:13:19,960
Speaker 1: Например, больше денег — хорошо, меньше денег — плохо.

144
00:13:20,320 --> 00:13:22,800
Speaker 1: Быть здоровым — хорошо, больным — плохо.

145
00:13:22,940 --> 00:13:26,940
Speaker 1: Вот, например, если больше денег, но меньше здоровья, то как это сопоставить?

146
00:13:27,060 --> 00:13:28,780
Speaker 1: Это разные сеточки координационные.

147
00:13:29,020 --> 00:13:36,680
Speaker 0: Да, и те, кто так или иначе связан с философскими рассуждениями, они с такими категориями работают.

148
00:13:36,720 --> 00:13:38,800
Speaker 0: То есть это та самая философская работа.

149
00:13:38,980 --> 00:13:45,460
Speaker 0: Там есть разные школы, разные традиции, но, тем не менее, эти вопросы задаются, эти вопросы ставятся на повестку.

150
00:13:45,700 --> 00:13:48,220
Speaker 1: Да, он такой вопрос не ставит вообще, если я правильно понимаю.

151
00:13:48,280 --> 00:13:49,640
Speaker 1: То есть у него все просто будет хорошо.

152
00:13:49,820 --> 00:13:56,620
Speaker 0: Там есть очень много посылок, которые должны быть приняты, по сути, на веру, без малейшей формулировки этих посылок.

153
00:13:56,720 --> 00:14:00,380
Speaker 0: Ну, допустим, то, что рост сам по себе является абсолютным бланком.

154
00:14:03,980 --> 00:14:07,620
Speaker 0: Хочу зафиксировать наше внимание не на том, что мы тут ругаем Альтмана.

155
00:14:08,060 --> 00:14:11,560
Speaker 0: Мне как раз кажется, текст написанный очень искренний.

156
00:14:11,960 --> 00:14:13,760
Speaker 0: И это меня по-настоящему и пугает.

157
00:14:14,180 --> 00:14:18,800
Speaker 0: Не то, что он злонамеренно не подевает и не задается философскими, этическими.

158
00:14:19,120 --> 00:14:20,800
Speaker 0: В целом такой вопрос у него не стоит.

159
00:14:21,260 --> 00:14:37,300
Speaker 1: Ты склоняешься к тому, что он искренний, а я думаю, что он... Я не верю, но человек, написавший такой блог... для которого проблема регулирования стоит в этой сфере очень острая, он пытается убедить, что не надо регулировать, что всё пойдёт само собой хорошо и так далее.

160
00:14:37,580 --> 00:14:46,520
Speaker 0: Тут может быть небольшой этот элемент, о котором ты говоришь, но корневой, скажем так, опять у тебя больше корпоративный взгляд, что, я думаю, имеет место и то, и другое.

161
00:14:46,560 --> 00:14:49,540
Speaker 0: У меня философский, я, повторюсь, с такими людьми очень общался.

162
00:14:49,660 --> 00:14:57,580
Speaker 0: Ты сам прекрасно, наверное, знаешь, что чем выше поднимаешься на уровень этих техногигантов, тем поражаешься узости реальной их компетенции.

163
00:14:57,700 --> 00:15:01,000
Speaker 0: при ощущении почти божественности собственных возможностей.

164
00:15:01,240 --> 00:15:06,300
Speaker 1: Техноаристократы с новой религией такого технологического детерминизма.

165
00:15:06,680 --> 00:15:15,060
Speaker 0: Ну слушай, давай прямо сказать, кто умно об этом правильное название дал, как и ты к нему не относись, это не клан с его тёмным просвещением, просто параллели буквально.

166
00:15:15,120 --> 00:15:20,720
Speaker 0: Если ты вспомнишь, мы с тобой довольно много обсуждали просвещенцев первой итерации, то есть XVIII века.

167
00:15:21,040 --> 00:15:26,740
Speaker 0: С философской точки зрения тексты просвещения на фоне предшествующей эпохи не выдерживают никакой критики.

168
00:15:27,100 --> 00:15:32,740
Speaker 0: Я даже не говорю про восточных святых отцов, но даже с холастой любого просвещенца могут размазать по хлебушку.

169
00:15:32,900 --> 00:15:34,820
Speaker 0: Просто по уровню качества аргументации.

170
00:15:34,880 --> 00:15:43,060
Speaker 0: Что не мешало просвещенцам воплощать ровно такой же дух уверенности в том, что они несут общее благо, и они вообще знают, как весь мир устроен.

171
00:15:43,120 --> 00:15:45,460
Speaker 0: Вот это страшное слово знаешь энциклопедист.

172
00:15:45,820 --> 00:15:46,700
Speaker 0: Она же в чем родилась?

173
00:15:46,840 --> 00:15:56,440
Speaker 0: То есть если раньше была ставка на цельное глубокое образование, то энциклопедист — это, по сути, была форма Википедии, где я прочитал пару статей в Википедии, и я эксперт по этому вопросу.

174
00:15:56,640 --> 00:15:58,560
Speaker 0: Знаменитая энциклопедия Дидро и Вольтера.

175
00:15:58,980 --> 00:15:59,520
Speaker 1: Интересно.

176
00:16:00,020 --> 00:16:01,480
Speaker 0: Здесь есть какая-то параллелизм.

177
00:16:01,620 --> 00:16:05,460
Speaker 0: То есть он употребляет слова, используя, по сути, некий философский аппарат.

178
00:16:05,700 --> 00:16:08,040
Speaker 0: Но откуда этот философский аппарат взялся вообще?

179
00:16:08,240 --> 00:16:11,000
Speaker 0: На каких метафизических посылках он существует?

180
00:16:11,180 --> 00:16:13,980
Speaker 0: Текст демонстрирует, что этот человек даже слова такого не слышал.

181
00:16:14,020 --> 00:16:20,820
Speaker 1: Слушай, но я все-таки попытаюсь аргументировать свою позицию или свое впечатление, что он действовал намеренно.

182
00:16:21,140 --> 00:16:26,220
Speaker 1: Давай проанализируем качество текста и логически попробуем сделать анализ.

183
00:16:26,660 --> 00:16:27,840
Speaker 1: Смотри, вот он что пишет.

184
00:16:28,280 --> 00:16:31,420
Speaker 1: Он использует сознательно пассивные конструкции.

185
00:16:31,660 --> 00:16:37,040
Speaker 1: Появятся системы, могут появиться роботы, будет достигнут прогресс.

186
00:16:37,180 --> 00:16:39,620
Speaker 1: Вопрос — а кто создаст эти системы?

187
00:16:39,960 --> 00:16:41,640
Speaker 1: Кто построит роботов?

188
00:16:41,760 --> 00:16:43,640
Speaker 1: Кто определит направление прогресса?

189
00:16:43,840 --> 00:16:45,880
Speaker 1: Пассивный залог скрывает субъекта.

190
00:16:46,180 --> 00:16:47,880
Speaker 1: Потом временная фальсификация.

191
00:16:48,120 --> 00:16:49,740
Speaker 1: Есть такое понимание в риторике.

192
00:16:50,660 --> 00:16:54,540
Speaker 1: 25-й год увидел, 26-й, вероятно, увидит, 27-й может увидеть.

193
00:16:54,820 --> 00:16:59,420
Speaker 1: Прошлое представлено как факт, настоящее как вероятность, будущее как возможность.

194
00:17:00,020 --> 00:17:05,740
Speaker 1: Но к концу читатель уже не различает, что произошло, что происходит.

195
00:17:05,780 --> 00:17:08,040
Speaker 1: То есть это такое вот заклинание фактически.

196
00:17:08,720 --> 00:17:10,579
Speaker 1: Вот есть такое слово, я поискал.

197
00:17:10,940 --> 00:17:14,579
Speaker 1: Называется эфемистический новояз.

198
00:17:15,619 --> 00:17:19,900
Speaker 1: Применяют критики товарища Альтмана и говорят, что это же прекрасно.

199
00:17:20,220 --> 00:17:21,819
Speaker 1: Мягкая сингулярность.

200
00:17:22,280 --> 00:17:27,060
Speaker 1: широкий доступ к суперинтеллекту, новые политические идеи.

201
00:17:27,660 --> 00:17:37,080
Speaker 1: Другими словами, потеря человеческого контроля над цивилизацией, монополия на познание и фактически отмены того, что так любят называть либералы, демократии.

202
00:17:37,140 --> 00:17:42,920
Speaker 1: Потому что вот этот технофеодализм, о котором все так говорят, он здесь просто проступает.

203
00:17:43,480 --> 00:17:44,120
Speaker 1: Что ты думаешь?

204
00:17:44,880 --> 00:17:47,360
Speaker 0: Да, я думаю, он проступает, и ты правильно заметил.

205
00:17:48,040 --> 00:17:50,280
Speaker 0: Вот этот пассивный залог действительно очень важный.

206
00:17:50,340 --> 00:17:54,520
Speaker 0: Это вот то, что в наших славянских языках, знаешь, вот такое выражение «ося».

207
00:17:54,820 --> 00:17:56,220
Speaker 0: Оно как-то само сталося.

208
00:17:56,720 --> 00:17:58,080
Speaker 0: Или вот что-то оно само.

209
00:17:58,340 --> 00:18:03,900
Speaker 0: И ты прав в том, что в случае корпоративных форм этот пассивный залог действительно скрывает субъекта, скрывает намерения.

210
00:18:04,120 --> 00:18:05,460
Speaker 0: А кто все будет это делать?

211
00:18:05,520 --> 00:18:07,400
Speaker 0: Ну смотри, он же прямо об этом пишет в конце.

212
00:18:07,500 --> 00:18:08,700
Speaker 0: А делать это будем мы.

213
00:18:09,100 --> 00:18:17,400
Speaker 0: Вот он прямо об этом пишет, что OpenAI — это множество, но сегодня, прежде всего, мы компания по созданию сверхинтеллекта.

214
00:18:17,620 --> 00:18:25,240
Speaker 0: Нам предстоит много работы, но основа пути перед нами сейчас освещена, и темные стороны быстренько уходят.

215
00:18:25,420 --> 00:18:28,520
Speaker 0: И мы очень благодарны за то, что мы можем это все сделать.

216
00:18:29,380 --> 00:18:32,180
Speaker 0: А, да, мы строим мозг для всего мира.

217
00:18:32,460 --> 00:18:37,020
Speaker 0: Он будет очень персонализирован и легок в использовании для всех.

218
00:18:37,300 --> 00:18:38,380
Speaker 0: Нас будут ограничивать.

219
00:18:38,500 --> 00:18:39,900
Speaker 0: только наличие классных идей.

220
00:18:40,120 --> 00:18:43,000
Speaker 0: Так что, видишь, он в конце-то говорит о том, что делать будем мы.

221
00:18:43,420 --> 00:18:44,880
Speaker 1: Да, дальше действовать будем мы.

222
00:18:45,320 --> 00:18:47,420
Speaker 1: Слушай, ну страшное дело, понимаешь, просто страшно.

223
00:18:47,520 --> 00:18:54,440
Speaker 1: А страшнее для меня всего то, что это вот эта вот пустышка, она будет очень привлекательна.

224
00:18:54,660 --> 00:19:01,320
Speaker 1: Люди готовы будут поверить в эту сказку, она удобная, снимает ответственность и за будущее, и за настоящее.

225
00:19:01,780 --> 00:19:03,140
Speaker 1: Должен быть выбор у людей.

226
00:19:03,380 --> 00:19:05,900
Speaker 1: А товарищ говорит, у нас это неизбежность.

227
00:19:06,320 --> 00:19:11,860
Speaker 0: И этот опыт очень правильно поставил, то, что текст построен на том, что у вас как бы и нет выбора.

228
00:19:11,940 --> 00:19:13,080
Speaker 0: Мы все равно это сделаем.

229
00:19:13,400 --> 00:19:15,040
Speaker 0: Очень важная постановка вопроса.

230
00:19:15,420 --> 00:19:29,600
Speaker 0: Понятное дело, единственная надежда моя, это, собственно, позитивная новость, то, что степень именно философской некомпетенции, собственно, и создает предпосылки, почему нас ждет впереди множество интересных сюрпризов, почему этот проект не реализуется так, как он мог бы реализоваться.

231
00:19:29,920 --> 00:19:39,720
Speaker 1: А я здесь более пессимистично настроен, потому что в эпоху вот таких постмодернистского отсутствия каких-либо больших нарративов, вот вам нарратив.

232
00:19:40,100 --> 00:19:41,460
Speaker 0: Он вообще нифига не новый.

233
00:19:41,500 --> 00:19:43,540
Speaker 0: Это тоже просвещение в старой оболочке.

234
00:19:43,840 --> 00:19:44,640
Speaker 0: Ничего не изменилось.

235
00:19:44,780 --> 00:19:59,800
Speaker 0: Даже его финальная метафора «Front path is now lit, and the dark areas are receding fast» — это же самая сбитая, базовая, классическая, агностическая просвещенческая метафора, пародия на первую главу Евангелия Атаана «То, что был свет, и тьма не овладела им».

236
00:20:00,160 --> 00:20:01,220
Speaker 0: Это же тоже самая пародия.

237
00:20:01,280 --> 00:20:03,120
Speaker 1: Ну, видишь, я вот этого как раз не заметил.

238
00:20:03,320 --> 00:20:04,620
Speaker 1: Хорошо, что мы с тобой вдвоем.

239
00:20:05,200 --> 00:20:07,900
Speaker 1: Я считаю, что он абсолютно искренний фанатик.

240
00:20:08,360 --> 00:20:22,680
Speaker 1: Но фанатик, который понимает, что для того, чтобы завербовать людей в свою веру, ему нужно, ну, немного, как бы, там, сгладить, срезать углы, там, сгладить противоречия, акцентировать на прекрасном новом будущем.

241
00:20:22,960 --> 00:20:24,680
Speaker 1: Вот он прям вот верит в свою миссию.

242
00:20:24,960 --> 00:20:29,880
Speaker 1: Вот что самое страшное, то, что он не захватывает власть, да, как фанатике.

243
00:20:30,040 --> 00:20:31,720
Speaker 1: Он, в общем, хочет получить её в подарок.

244
00:20:32,060 --> 00:20:40,140
Speaker 1: То есть общество говорит, о, классно, да, давайте мы откажемся от способности думать о собственном будущем, потому что у нас есть супер-и, да?

245
00:20:40,400 --> 00:20:44,020
Speaker 0: На самом деле мне не до конца понятно, где selling point.

246
00:20:44,080 --> 00:20:49,060
Speaker 0: Ну, по сути, он прямо не пишет, но единственное, что могу понять, это иллюзия бессмертия для избранных.

247
00:20:49,160 --> 00:20:51,620
Speaker 0: Потому что все остальное, в принципе, и так непонятно.

248
00:20:51,780 --> 00:20:55,440
Speaker 0: Что он продает-то этим людям, которые должны на это все с радостью согласиться?

249
00:20:55,740 --> 00:20:56,800
Speaker 0: Свободу творчества?

250
00:20:56,960 --> 00:21:06,420
Speaker 0: Ну, ты же сам пишешь в женском тексте, по сути, эту свободу творчества и надеяться, что в перспективе творчества, по сути, будет не то чтобы ненужным, но оно тоже будет вот это сталося.

251
00:21:06,580 --> 00:21:09,600
Speaker 0: То есть вот интеллект сам будет генерировать творческие идеи.

252
00:21:09,860 --> 00:21:18,520
Speaker 0: Мне как раз очень, ты прав, фигура умолчания, которая скрывается за этим классическим новоязом, тоже классическая схема, работает со времен, читайте, Левиафан Гоббса.

253
00:21:18,740 --> 00:21:23,260
Speaker 0: Он прямо пишет инструкции, как надо создавать в университетах комиссаров, чтобы это работало.

254
00:21:23,440 --> 00:21:24,660
Speaker 0: Так что схема тоже старая.

255
00:21:25,000 --> 00:21:26,560
Speaker 0: Но там хоть понятно, что он им обещает.

256
00:21:26,640 --> 00:21:28,980
Speaker 0: Гоббс хоть прямо пишет, я обещаю вам безопасность.

257
00:21:29,460 --> 00:21:33,580
Speaker 0: Мне интересно, что Альтман даже четко не формулирует, а что, собственно, он продает.

258
00:21:33,680 --> 00:21:34,840
Speaker 1: Нет, он изобилие продает.

259
00:21:35,020 --> 00:21:38,560
Speaker 0: Будет очень сложно, потому что исчезнут целые слои работ.

260
00:21:38,860 --> 00:21:45,000
Speaker 0: Но, с другой стороны, мы будем становиться богаче настолько быстрее, что мы сможем все это позволить.

261
00:21:45,400 --> 00:21:46,780
Speaker 0: Короче, безусловно, базовый доход.

262
00:21:47,180 --> 00:21:49,100
Speaker 1: Я люблю приводить классический пример.

263
00:21:49,320 --> 00:21:52,820
Speaker 1: Мы с тобой уже говорили об этом, что я уже в третий или в четвёртый раз повторюсь.

264
00:21:53,220 --> 00:22:10,140
Speaker 1: Освобождение человека от рутины домашнего труда не привело к взрывному росту поэтов, художников и появлению каких-то хобби у тех, кто теперь может часы проводить не моя посуду и убирая, а заниматься творчеством.

265
00:22:10,440 --> 00:22:12,520
Speaker 1: Люди смотрят сериалы, играют в игры.

266
00:22:12,720 --> 00:22:16,280
Speaker 1: Ничего в этом самом по себе плохого нет, но просто это время уходит в никуда.

267
00:22:16,540 --> 00:22:18,140
Speaker 1: То есть фактически они отказываются жить.

268
00:22:18,420 --> 00:22:20,160
Speaker 0: Как всегда, добавлю немного теологии.

269
00:22:20,260 --> 00:22:25,120
Speaker 0: Можно сказать, что, конечно, любое наше подобное действие — это акт принесения жертвы.

270
00:22:25,480 --> 00:22:30,180
Speaker 0: То есть главная жертва, которую мы приносим, — это пресловутое наше внимание, силы, ресурсы.

271
00:22:30,680 --> 00:22:36,560
Speaker 0: И когда мы убиваем время в буквальном смысле, мы, конечно, осуществляем по сути идолопоклоннический акт.

272
00:22:36,840 --> 00:22:51,900
Speaker 0: По сути, на самом деле, что Сэм Альтман намеренно или ненамеренно описывает будущее как такая тотальная машина жертвоприношения людей во всем широком смысле вот этому новому суперинтеллекту и тем, кто будет стоять за ним.

273
00:22:52,160 --> 00:22:52,360
Speaker 1: Ну да.

274
00:22:52,900 --> 00:22:57,180
Speaker 0: Это очень такая классическая библейская демоническая история, на самом деле.

275
00:22:57,340 --> 00:23:01,920
Speaker 1: Каждый раз, когда вам обещают утопию, нужно поискать, а кто будет ее администрировать.

276
00:23:02,200 --> 00:23:05,840
Speaker 0: Если суммировать, наверное... То, что говорит Сэм Альтман, это...

277
00:23:06,180 --> 00:23:07,180
Speaker 1: Мир и безопасность, да?

278
00:23:07,640 --> 00:23:16,160
Speaker 0: А вот нет, он пока говорит изобилие, пока предлагает изобилие и процветание, но мир и безопасность там очевидны.

279
00:23:16,360 --> 00:23:18,200
Speaker 0: Просто, наверное, нужна вторая часть эссе.

280
00:23:18,480 --> 00:23:23,320
Speaker 0: Я прям все ждал, когда он будет написать, что ИИ будет прекрасно разрешать международные конфликты.

281
00:23:23,520 --> 00:23:27,240
Speaker 0: Пока не написал, но явно предполагается, типа, все конфликты одни на понимание.

282
00:23:27,280 --> 00:23:32,940
Speaker 0: Вот если у нас будет правильный искусственный интеллект в начальных, как над всеми, то вот мы тогда установим глобальный мир.

283
00:23:33,100 --> 00:23:36,740
Speaker 0: Ну, сразу скажу, вот это, кстати, причина, почему это не срастется, по крайней мере.

284
00:23:36,840 --> 00:23:42,740
Speaker 0: Я этого не ожидаю, потому что для того, чтобы это срослось, нужно доверие между начальниками, а они друг другу доверять не могут.

285
00:23:42,880 --> 00:23:45,380
Speaker 1: В данном случае это недоверие будет предохранителем.

286
00:23:46,460 --> 00:23:47,180
Speaker 0: Да, да, да.

287
00:23:47,280 --> 00:23:48,220
Speaker 0: Я же не говорю, что это плохо.

288
00:23:48,260 --> 00:23:51,120
Speaker 0: Я просто показываю, скажем так, дырки в аггументе.

289
00:23:51,220 --> 00:23:53,460
Speaker 0: Но да, текст действительно впечатляет.

290
00:23:53,520 --> 00:23:55,460
Speaker 0: Это сочетание детской наивности.

291
00:23:56,080 --> 00:24:00,040
Speaker 0: Если бы мне не сказали, кто этот текст написал, то действительно я бы сказал, что он написан подросткам.

292
00:24:00,340 --> 00:24:08,000
Speaker 0: который открыл первый закон физики или узнал, как это работает, и преобладает в полной уверенности, что он сейчас изменит мир, и он всё знает.

293
00:24:08,420 --> 00:24:09,560
Speaker 1: Всё будет телевидение.

294
00:24:09,940 --> 00:24:11,700
Speaker 0: Да, да, слушай, Москва слезам не верит.

295
00:24:11,760 --> 00:24:16,200
Speaker 0: В общем, да, вот этот разговор на кухне их, когда они молодые, вот очень тот же самый вайб.

296
00:24:16,280 --> 00:24:17,340
Speaker 0: Всё будет телевидение.

297
00:24:17,700 --> 00:24:20,240
Speaker 1: Казалось бы, но уже столько этого телевидения было.

298
00:24:20,500 --> 00:24:28,460
Speaker 1: И сначала всех обучим, потом электричество, потом радио, потом телевидение, потом тебе социальные сети.

299
00:24:28,800 --> 00:24:33,480
Speaker 1: И всегда... есть громадный побочный эффект всех этих прелестей.

300
00:24:33,700 --> 00:24:37,380
Speaker 1: Человек, тупо уставившийся в телевизор, прекращает читать.

301
00:24:37,780 --> 00:24:41,040
Speaker 1: Он свою скуку убивает пассивность таким наблюдением.

302
00:24:41,360 --> 00:24:45,280
Speaker 1: А тут нам говорят, ну вот это было все не то, но сейчас точно сработает.

303
00:24:45,420 --> 00:24:46,700
Speaker 0: Ну смотри, давай просто суммируем.

304
00:24:46,740 --> 00:24:53,240
Speaker 0: На самом деле здесь есть большая проблема, которую не только Альтман не понимает, а это вообще, кажется, тема всего нашего сезона.

305
00:24:53,520 --> 00:24:54,880
Speaker 0: Потому что это философская проблема.

306
00:24:55,100 --> 00:24:56,540
Speaker 0: Какое у нас есть заблуждение?

307
00:24:57,060 --> 00:25:07,800
Speaker 0: Что... Мы не разделяем проблемы, то, что можно сказать, антропологические, они же этические, они же философские, они же проблемы человеческого сердца, и технологические.

308
00:25:08,200 --> 00:25:11,020
Speaker 0: И находятся они не в равной зависимости.

309
00:25:11,300 --> 00:25:15,340
Speaker 0: Другими словами, как мне представляется, существует обратный закон.

310
00:25:15,580 --> 00:25:20,260
Speaker 0: Иначе говоря, любую технологическую проблему можно дать антропологический ответ.

311
00:25:20,660 --> 00:25:25,480
Speaker 0: переводя на простой язык, нет той технологии, которая остановит святого к его пути к святости.

312
00:25:25,780 --> 00:25:31,520
Speaker 0: Хоть 15 искусственных интеллектов изобретай, если сердце святого направлено к Богу, то он преодолеет эти вещи.

313
00:25:31,560 --> 00:25:37,000
Speaker 0: То есть потенциал антропологического ответа, этического ответа духа бесконечно больше любой технологии.

314
00:25:37,560 --> 00:25:43,440
Speaker 0: И наоборот, на любую проблему сердца, духа, невозможно дать технический ответ.

315
00:25:43,740 --> 00:25:50,080
Speaker 0: А все, что они предлагают, это, в принципе, вся наша цивилизация, это попытка дать технический ответ на антропологические проблемы.

316
00:25:53,920 --> 00:25:55,380
Speaker 1: Всё-таки не могу не задать вопрос.

317
00:25:55,760 --> 00:26:13,500
Speaker 1: Скажи, пожалуйста, я вот до сих пор колеблюсь, потому что то ли Сэм Альтман действительно настолько ограничен в понимании каких-то базовых, даже не философских, просто здравомыслия, что всё имеет обратную сторону, что нужно оценивать явления в комплексе.

318
00:26:13,980 --> 00:26:16,260
Speaker 1: Кажется, это маловероятно мне, по крайней мере.

319
00:26:16,640 --> 00:26:27,800
Speaker 1: Или же он ведет сознательную агитацию, имея в виду свои пряники, которые он должен получить в результате расширения влияния за счет владения технологией развития искусственного интеллекта.

320
00:26:28,140 --> 00:26:30,940
Speaker 0: Мой опыт показывает, что скорее первый.

321
00:26:31,260 --> 00:26:38,180
Speaker 0: По крайней мере, в любой непонятной ситуации я вместо злонамеренности выберу искреннее, скажем так, вот это мировоззрение.

322
00:26:38,520 --> 00:26:39,380
Speaker 0: Заблуждение, да-да.

323
00:26:39,460 --> 00:26:41,620
Speaker 0: Но это нисколько даже заблуждение, понимаешь?

324
00:26:41,700 --> 00:26:42,760
Speaker 0: Это мироощущение.

325
00:26:42,860 --> 00:26:45,420
Speaker 0: Вот это вот есть мировоззрение, а есть мироощущение, да?

326
00:26:45,680 --> 00:26:57,280
Speaker 0: Вот повторюсь, мироощущение — ну вот представь, ты молодой человек, относительно молодой, и ты один из богатейших людей планеты, причем... Действительно создаешь вещи, которые, ну, под твоим руководством.

327
00:26:57,320 --> 00:27:06,100
Speaker 0: Лучше ты понимаешь, что не ты лично их создаешь, но под твоим руководством создаются вещи, которые ни много ни мало, ну, еще там 50 лет назад выглядели бы как магия.

328
00:27:06,520 --> 00:27:09,560
Speaker 0: Психологически тот же самый Фенолин, что накануне Просвещения.

329
00:27:09,740 --> 00:27:12,500
Speaker 0: Мы там, условно, научились паровую машину создать.

330
00:27:13,000 --> 00:27:14,940
Speaker 0: Или новые технологии каналов рыть.

331
00:27:15,140 --> 00:27:15,900
Speaker 0: Вау!

332
00:27:16,180 --> 00:27:16,980
Speaker 0: Или еще чего-то.

333
00:27:17,120 --> 00:27:18,160
Speaker 1: Ну, боги, боги, да.

334
00:27:18,600 --> 00:27:23,820
Speaker 0: Как тут не поверить, что мы можем решить вообще все проблемы человечества и сразу.

335
00:27:23,880 --> 00:27:27,460
Speaker 0: Тем более, раньше они были... Тем более, мы-то красивые люди.

336
00:27:27,680 --> 00:27:29,740
Speaker 0: Там вот эти странные мракобесы сидят.

337
00:27:29,820 --> 00:27:30,960
Speaker 0: Там они что-то ноют.

338
00:27:31,000 --> 00:27:32,020
Speaker 0: Там они что-то ходят.

339
00:27:32,060 --> 00:27:34,940
Speaker 0: Там они о чем-то говорят на своем высокоумном непонятном.

340
00:27:35,060 --> 00:27:37,120
Speaker 1: А мы тут конкретные пацаны.

341
00:27:37,260 --> 00:27:38,740
Speaker 1: Решаем конкретные задачи.

342
00:27:38,800 --> 00:27:40,140
Speaker 1: У нас все получается.

343
00:27:41,520 --> 00:27:47,120
Speaker 1: Поэтому весь XIX век — это, может быть, первый или, может быть, не первый, но второй эпоха технооптимизма.

344
00:27:48,140 --> 00:27:49,820
Speaker 1: Ничего же нового в технооптимизме нет.

345
00:27:49,960 --> 00:27:51,300
Speaker 0: Да, самому по себе нет.

346
00:27:51,360 --> 00:27:55,040
Speaker 0: То есть был технооптимизм XIX век, он закончился Первой мировой.

347
00:27:55,300 --> 00:27:59,200
Speaker 0: Первая версия технооптимизма очень умерла, а потом был технооптимизм.

348
00:28:00,040 --> 00:28:01,460
Speaker 0: 50-х, 60-х, 70-х.

349
00:28:01,580 --> 00:28:04,820
Speaker 0: Волна технооптимизма, которая сменилась киберпанком в 80-х.

350
00:28:04,860 --> 00:28:06,880
Speaker 0: Ну, сейчас просто очередная волна технооптимизма.

351
00:28:07,100 --> 00:28:10,900
Speaker 0: Структурно-то ничего этой прав абсолютно не изменилось, но масштаб другой, да?

352
00:28:11,240 --> 00:28:12,100
Speaker 0: Но что пугает?

353
00:28:12,380 --> 00:28:17,560
Speaker 0: Есть несколько признаков, которые все-таки именно эффект масштаба заставляют задуматься.

354
00:28:17,920 --> 00:28:27,120
Speaker 0: Первый – масштаб потенциально метафоричный джина, который эти товарищи выпускают из бутылки, не задаваясь ни на секунду вопросом.

355
00:28:27,400 --> 00:28:43,700
Speaker 0: Я хочу здесь снова процитировать знаменитого нашего великого физиолога Бехтерева, который, изучая, правда, шизофреников, но, мне кажется, сказал гениальную фразу, которую каждому из нас просто жизненно необходимо выучить в эпоху социальных сетей, а именно, что власть — это торможение.

356
00:28:44,440 --> 00:28:49,000
Speaker 1: Мы говорим о том, что в человеческой психике есть две функции — возбуждение и торможение.

357
00:28:49,140 --> 00:28:59,740
Speaker 1: И именно в этом контексте Бехтерев и сказал, что для того, чтобы обладать некоторой властью, нужно, чтобы функция торможения была развита по крайней мере, не меньше, чем функция возбуждения.

358
00:29:00,000 --> 00:29:03,340
Speaker 1: Потому что только так мы можем контролировать свои импульсы.

359
00:29:04,000 --> 00:29:05,040
Speaker 1: В этом смысле ты имеешь в виду?

360
00:29:05,340 --> 00:29:09,520
Speaker 0: В этом смысле говорил Бехтерев, а я применяю на, допустим, условно, социальной сети.

361
00:29:09,860 --> 00:29:11,780
Speaker 0: Нас все время провоцируют на ускорение.

362
00:29:11,920 --> 00:29:16,300
Speaker 0: Ну, это не то, что кто-то злой, а алгоритм социальных сетей построен на постоянном ускорении.

363
00:29:16,560 --> 00:29:20,880
Speaker 0: То есть постоянное ускорение обмена информации, постоянное ускорение потребляемого контента.

364
00:29:21,100 --> 00:29:22,820
Speaker 0: Не длинная медиа, а рилсы.

365
00:29:23,040 --> 00:29:26,720
Speaker 0: Твиттер, а не длинный пост, не книга, а длинный пост и так далее.

366
00:29:26,760 --> 00:29:28,080
Speaker 0: Все идет по логике ускорения.

367
00:29:28,620 --> 00:29:37,480
Speaker 0: Субъектность в эпоху социальных сетей, она во многом именно в способности отложить, закрыть, не смотреть, не слушать, не реагировать, не взаимодействовать.

368
00:29:37,740 --> 00:29:39,520
Speaker 0: Вот это проявляется настоящая субъектность.

369
00:29:39,680 --> 00:29:42,340
Speaker 0: Какое это отношение имеет к нашим новым технофеандалам?

370
00:29:42,400 --> 00:29:57,360
Speaker 0: Наверное, самый радикальный момент, который как бы находится в слепом пятне во время чтения всех этих текстов, это то, что нигде не в тексте на секунду не ставится вопрос, что возможность что-то сделать не означает необходимость это сделать.

371
00:29:57,760 --> 00:30:04,740
Speaker 0: Другими словами, раз мы можем сделать эту технологию, раз мы можем сделать те или иные решения, значит, мы будем это делать.

372
00:30:04,940 --> 00:30:17,300
Speaker 0: То есть ни на секунду не задается вопросом, с какой скоростью, с какой последовательностью, с каким там интенсивностью, на каком этапе, с какими подготовительными периодами для общества нужно интегрировать ту или иную технологию такого масштаба.

373
00:30:17,460 --> 00:30:19,140
Speaker 0: Вот этот вопрос не ставится вообще.

374
00:30:19,500 --> 00:30:24,080
Speaker 0: Просто общество ставится перед фактом, мы вам это сделаем, и вы будете счастливы.

375
00:30:24,500 --> 00:30:35,040
Speaker 1: Мы с тобой часто приходим к тому, что вся логика развития событий, весь этот старт, который мы относим к XVI веку, просто неутомимо и неумолимо прибежал к нас к этой точке.

376
00:30:35,320 --> 00:30:49,920
Speaker 1: Когда технологии развились настолько, что человек не в состоянии с ними конкурировать, когда технологические оптимисты нажимают на эту педальку ещё больше и больше, имея в виду, что они управляют этим транспортным средством.

377
00:30:50,400 --> 00:30:56,280
Speaker 1: И они ведут куда-то, куда, по их разумению, мы обязаны прийти и получить там счастье.

378
00:30:56,560 --> 00:30:57,760
Speaker 1: Ну и чем это закончится?

379
00:30:58,080 --> 00:31:04,420
Speaker 1: Ну, как бы не хочется быть негативным, но похоже, что это должно закончиться неизбежно столкновением с какой-то стенкой.

380
00:31:04,780 --> 00:31:14,560
Speaker 0: Здесь вот этот момент, о котором, думаю, стоит будет сделать отдельный эфир, потому что вот самые умные из наших технооптимистов — это вот те самые акселерационисты.

381
00:31:14,860 --> 00:31:17,940
Speaker 0: Вот как раз те люди, которые уже получили философскую подготовку.

382
00:31:18,260 --> 00:31:24,560
Speaker 0: Они поддерживают твой тезис, на самом деле, но вывод этически делают из него совсем другой.

383
00:31:24,780 --> 00:31:27,320
Speaker 0: Другими словами, то, что это не стенка, это сингулярность.

384
00:31:27,820 --> 00:31:42,860
Speaker 0: То, что, скажем так, кто-то намеренно, а кто-то ненамеренно из наших технофеодалов действительно приближает момент, который является, как они хотят верить, некой точкой качественного скачка или перехода, или катастрофы, это объективный процесс, в котором мы живем.

385
00:31:43,060 --> 00:31:56,440
Speaker 0: Там, где мы говорим на протяжении наших эфиров то, что у технологических вопросов должно быть антропологическое решение, они продолжают, как это даблдаун, удваивать ставки на то, что на человеческую проблему мы дадим технологическое решение.

386
00:31:56,660 --> 00:31:57,920
Speaker 0: Это как раз разные этики.

387
00:31:58,380 --> 00:32:11,340
Speaker 1: Но мы и пытались с тобой до сих пор убедить наших слушателей в том, что ответа на технологический вызов в рамках технологий не может быть, что этот ответ должен быть за пределами.

388
00:32:11,800 --> 00:32:14,760
Speaker 1: этой системы технологических решений.

389
00:32:15,300 --> 00:32:23,460
Speaker 1: Я думаю, что буквально, может быть, и следующий выпуск мы посвятим больше не технологическому вызову, а антропологическому ответу.

390
00:32:23,800 --> 00:32:32,080
Speaker 1: Потому что для многих становится вопросом, ну и мы с тобой пытались ответить на этот вопрос, а что же значит этот антропологический ответ?

391
00:32:32,440 --> 00:32:37,260
Speaker 1: Проще говоря, что значит быть человеком в эту эпоху технологических вызовов.

392
00:32:37,620 --> 00:32:42,800
Speaker 1: И здесь, я думаю, мы пригласим человека, который… Ну, я не думаю, я лукавлю, конечно.

393
00:32:42,900 --> 00:32:54,400
Speaker 1: Я знаю, что мы собираемся пригласить человека, который согласился помочь нам рассмотреть вот этот вопрос о человечности с философской точки зрения, с точки зрения академической философии, скажем так.

394
00:32:54,520 --> 00:32:58,240
Speaker 1: И я уверен, что этот разговор будет нам небездолезен.

395
00:33:02,460 --> 00:33:03,860
Speaker 1: Ты знаешь, я о чём думаю?

396
00:33:04,280 --> 00:33:19,620
Speaker 1: Вот этот антропологический ответ и в целом наша цивилизационная травма, попытка при помощи интеллекта, то есть технология — это же продукт интеллектуальных усилий, решить задачи, которые не решаются таким способом.

397
00:33:19,980 --> 00:33:28,060
Speaker 1: Попытка умом построить рай на земле, попытка… В конечном итоге ведь человек — это же далеко не только интеллект и ум.

398
00:33:28,360 --> 00:33:36,120
Speaker 1: Ведь очень многие… Наверное, мы знаем людей, которые не обладают выдающимися интеллектуальными способностями, но они прекрасные люди.

399
00:33:36,500 --> 00:33:38,420
Speaker 1: И в этом не надо смеяться, человек хороший.

400
00:33:38,500 --> 00:33:52,520
Speaker 1: Бывают в каких-то далёких деревнях крестьяне, которые, в общем, имеют смутное представление о технологиях и, наверное, не смогут тебе рассказать ничего о законах физики, математики и чего-то ещё.

401
00:33:52,940 --> 00:33:56,980
Speaker 1: Их кругозор узок, но они как люди великолепны, они добры.

402
00:33:57,480 --> 00:34:02,600
Speaker 1: Они открыты, они искренни, они доверчивы, но при этом очень просты.

403
00:34:03,140 --> 00:34:05,780
Speaker 1: Не могу найти слов, наверное, каждый понимает, о чём я говорю.

404
00:34:05,960 --> 00:34:11,219
Speaker 1: То есть человечность — это далеко не только и, может быть, не столько интеллект.

405
00:34:11,820 --> 00:34:13,880
Speaker 0: Вот-вот-вот, это ты абсолютно прав.

406
00:34:14,239 --> 00:34:15,679
Speaker 0: Это точно не интеллект.

407
00:34:15,920 --> 00:34:22,179
Speaker 0: Можно сказать, что это Логос как имя, но тут вот надо разбирать, чем Логос отличается от интеллекта.

408
00:34:22,260 --> 00:34:23,639
Speaker 0: Но я с тобой абсолютно прав.

409
00:34:23,960 --> 00:34:25,540
Speaker 0: Я с тобой здесь абсолютно согласен.

410
00:34:25,580 --> 00:34:34,800
Speaker 0: И это как раз, если мы смотрим философскую предпредпредпредпосылку всего этого безумия, то, что мы сегодня обсуждали, это именно убеждение, что мы прежде всего интеллект.

411
00:34:35,159 --> 00:34:37,860
Speaker 0: А на этом стоит ни много ни мало вся наша цивилизация.

412
00:34:38,340 --> 00:34:40,219
Speaker 1: Да, мы же меряем людей по IQ.

413
00:34:40,760 --> 00:34:42,420
Speaker 1: Да, у нас культ интеллекта.

414
00:34:42,880 --> 00:34:46,880
Speaker 1: Принято восхищаться людьми незаурядных интеллигентных способностей.

415
00:34:47,239 --> 00:34:49,540
Speaker 0: Причем, смотри, это же двойная итерация.

416
00:34:49,699 --> 00:35:06,280
Speaker 0: У нас же как было сначала, у них был интеллект, поскольку там natural intellect и natural philosophy, это вот как раз просвещение первой итерации, но дальше пришла проблема, они сами себя, по сути говоря, опровергли постмодернизмом, а именно Дарвином, а именно Фрейдом, а именно проблемой предопределенности, проблемой предкоза.

417
00:35:06,340 --> 00:35:13,240
Speaker 0: Другими словами, их же, в кавычках, наука показала им, что, в общем, интеллект human intellect, то есть человеческий интеллект faulty.

418
00:35:13,500 --> 00:35:16,600
Speaker 0: У него там эмоции, тоната, смортида, либидо.

419
00:35:16,900 --> 00:35:22,780
Speaker 0: То есть вместо того, чтобы переосмыслить базовую посылку, что может быть не в интеллекте только дело, условно-исамальную нам предлагать.

420
00:35:22,820 --> 00:35:24,340
Speaker 1: Технологии добавим, да.

421
00:35:24,600 --> 00:35:34,660
Speaker 1: Так вот получается, что искусственный интеллект или вот этот будущий генерализированный интеллект, он фактически как раз и доводит до логического конца это рассуждение.

422
00:35:35,020 --> 00:35:36,220
Speaker 1: Ну вот вам интеллект.

423
00:35:36,460 --> 00:35:42,260
Speaker 1: То, что они называют искусственным интеллектом, оно должно настолько превосходить человеческий интеллект.

424
00:35:42,460 --> 00:35:48,580
Speaker 1: Измеряемый вот этот интеллект будет измеряться... теми мерками, которые подходили к интеллекту человеческому.

425
00:35:48,740 --> 00:35:51,880
Speaker 1: Окажется, что самый большой интеллектуал — это не человек.

426
00:35:51,940 --> 00:35:53,100
Speaker 1: А что остаётся человеку?

427
00:35:53,580 --> 00:35:54,980
Speaker 1: Что остаётся от человека?

428
00:35:55,340 --> 00:35:56,780
Speaker 1: Вот возникают такие вопросы.

429
00:35:57,220 --> 00:36:02,540
Speaker 0: То есть, по сути, он предлагает, пока мы ещё не до конца отдавайте нам свои идеи, но вопрос остаётся открытым.

430
00:36:02,580 --> 00:36:03,820
Speaker 0: А что остаётся от человека?

431
00:36:03,860 --> 00:36:05,360
Speaker 1: То есть мы достигаем предела.

432
00:36:05,560 --> 00:36:17,420
Speaker 1: И мы с тобой в каждом из выпусков приходим к одному и тому же выводу, что вот это технологическое развитие, эта сингулярность... Это и катастрофа нового времени, катастрофа ставки на интеллект.

433
00:36:18,180 --> 00:36:25,960
Speaker 1: Технология, которая призвана была при помощи интеллектуальных усилий, изобретений решать проблемы человека, создает новые проблемы.

434
00:36:26,340 --> 00:36:36,720
Speaker 0: Но тут возникает уже проблема следующего порядка, что цена философского сначала отката к базовой проблеме с каждым технологическим витком возрастает.

435
00:36:37,120 --> 00:36:39,460
Speaker 0: Называется sunken cost fallacy уж, если по-простому.

436
00:36:39,800 --> 00:36:40,960
Speaker 0: Это классическая проблема.

437
00:36:41,140 --> 00:36:46,320
Speaker 1: То есть теперь, отказавшись от этого, мы столько потеряем, что мы уже не можем от этого отказаться?

438
00:36:46,860 --> 00:36:48,380
Speaker 0: Ну, мы думаем, что мы не можем, да.

439
00:36:48,480 --> 00:36:49,860
Speaker 0: И поэтому лучше double down.

440
00:36:50,060 --> 00:36:54,100
Speaker 0: Удвоить ставку и попытаться решить проблему за счет количественного скачка.

441
00:36:54,340 --> 00:36:54,520
Speaker 1: Ага.

442
00:36:55,120 --> 00:36:57,300
Speaker 1: Попробовали при помощи пара.

443
00:36:57,900 --> 00:37:00,100
Speaker 1: Ну, что-то получилось, что-то нет.

444
00:37:00,140 --> 00:37:01,960
Speaker 1: Там детский труд, то все, 5-10.

445
00:37:02,480 --> 00:37:04,140
Speaker 1: Попробовали с помощью электричества.

446
00:37:04,860 --> 00:37:06,660
Speaker 1: Улучшилось что-то, но все же не то.

447
00:37:06,780 --> 00:37:08,580
Speaker 1: Попробовали при помощи атома.

448
00:37:09,700 --> 00:37:12,420
Speaker 0: Но при этом каждый раз среда улучшалась.

449
00:37:12,480 --> 00:37:15,300
Speaker 0: То есть все равно материальная среда радикально менялась.

450
00:37:15,340 --> 00:37:19,000
Speaker 0: Потому что, называй, комфорт, возможности, власть росла.

451
00:37:19,380 --> 00:37:20,040
Speaker 0: То есть все время.

452
00:37:20,580 --> 00:37:25,580
Speaker 1: Теперь получается, что этот комфорт, он фактически делает человека заложником.

453
00:37:26,580 --> 00:37:38,080
Speaker 1: Мы становимся заложниками комфорта и тех решений, которые уже обеспечивают нам минимальные затраты энергии на решение тех задач, которые у наших предков требовало невероятного количества энергии.

454
00:37:38,240 --> 00:37:47,220
Speaker 1: Теперь мы вынуждены наращивать мышцы искусственными упражнениями в зала, что, наверное, было бы странно для многих из наших прадедушек и прабабушек.

455
00:37:47,500 --> 00:37:51,660
Speaker 1: Хотя нет, наверное, уже тогда для аристократии физкультура была не в новинку.

456
00:37:51,720 --> 00:37:52,320
Speaker 1: Но тем не менее.

457
00:37:53,660 --> 00:38:01,040
Speaker 1: фактически обречены на вот этот антропологический ответ, если не хотим исчезнуть как люди.

458
00:38:01,460 --> 00:38:09,880
Speaker 0: Да, и в каком-то смысле, если искать эсхатологический оптимизм во всех этих историях, повторюсь, то, что искусственный интеллект просто радикально нас ставит перед этим вызовом.

459
00:38:10,420 --> 00:38:18,840
Speaker 0: Ты хотел прятаться за этим, но вот теперь мы действительно технологически создаём такую иллюзию, которая создаёт вот этого искушения твоей полной отмены.

460
00:38:19,020 --> 00:38:19,620
Speaker 0: Что выберешь?

461
00:38:20,200 --> 00:38:21,180
Speaker 1: И хочу сделать шаг назад.

462
00:38:21,260 --> 00:38:23,520
Speaker 1: Многие говорят, слушайте, чего это вы тут паникуете?

463
00:38:23,620 --> 00:38:25,340
Speaker 1: Ну, инструмент новый изобрели.

464
00:38:25,720 --> 00:38:28,620
Speaker 1: Просто берём этот инструмент и пользуемся им.

465
00:38:28,720 --> 00:38:42,120
Speaker 1: Но фундаментальное отличие ИИ или АИ от лопаты или молотка, что лопата и молоток сами себя не совершенствуют, не развивают сами себя, они не делают тебя менее субъектным.

466
00:38:42,720 --> 00:38:45,260
Speaker 0: Как мы говорили ранее, книга сама себя не напишет.

467
00:38:45,320 --> 00:38:49,800
Speaker 1: А вот теперь, да, напишет, снимет, нарисует и еще как.

468
00:38:50,040 --> 00:38:55,940
Speaker 1: И еще за тебя, как вот мы в первом выпуске говорили, за тебя твоим же голосом расскажет то, что ты и не думал говорить.

469
00:38:56,100 --> 00:38:58,080
Speaker 1: И пойди отличи, что тебя от фальшивки.

470
00:38:58,460 --> 00:39:02,480
Speaker 1: Ну, в общем, да, веселые времена, на что надеемся, Павел Александрович.

471
00:39:02,960 --> 00:39:06,800
Speaker 0: на антропологию, на ту самую, на этику, на вечные истины, на то самое.

472
00:39:07,180 --> 00:39:07,580
Speaker 0: Всё туда.

473
00:39:07,840 --> 00:39:13,320
Speaker 0: То есть лишний повод посмотреть честно себе в зеркало и спросить, а всё-таки что делает меня человеком?

474
00:39:13,460 --> 00:39:16,680
Speaker 0: Что есть человеческое в человеке?

475
00:39:16,860 --> 00:39:23,460
Speaker 0: Повторюсь, для меня главный парадокс в том, что чем больше мы развиваем технологию, тем более остро встаёт антропологический вопрос.

476
00:39:23,880 --> 00:39:24,220
Speaker 1: Хорошо.

477
00:39:24,460 --> 00:39:29,100
Speaker 1: А я, ты знаешь, скажу немножко иначе, не так возвышенным, и о другом.

478
00:39:29,560 --> 00:39:38,140
Speaker 1: Всё время... что человечество движется по этому пути технического прогресса и утопического мышления.

479
00:39:38,480 --> 00:39:42,640
Speaker 1: Любая утопия противоречит самому характеру человека.

480
00:39:42,980 --> 00:39:45,720
Speaker 1: «Ну не хочу, я хочу, как я хочу», — вот моя дочка говорила.

481
00:39:45,940 --> 00:39:47,940
Speaker 1: «Ну давай так, нет, я хочу, как я хочу».

482
00:39:48,040 --> 00:39:49,000
Speaker 1: Вот этот дух.

483
00:39:49,320 --> 00:39:55,500
Speaker 1: противоречия, которое может быть очень деструктивным, но он может в то же самое время быть таким предохранителем.

484
00:39:55,720 --> 00:39:58,000
Speaker 0: Все время напоминаю вот этот двойной перевод.

485
00:39:58,080 --> 00:39:59,800
Speaker 0: У нас, понимаешь, все-таки нужно понимать.

486
00:39:59,900 --> 00:40:03,140
Speaker 0: Это, кстати, интересно, что косвенно отражено в разных культурах.

487
00:40:03,440 --> 00:40:06,960
Speaker 0: Мне ближе православная, она же с греческим подтекстом.

488
00:40:07,260 --> 00:40:10,540
Speaker 0: Апокалипсис у нас воспринимается синонимом к слову «катастрофа».

489
00:40:10,940 --> 00:40:13,080
Speaker 0: Но его дословный перевод — «откровение».

490
00:40:13,320 --> 00:40:18,400
Speaker 0: То есть открытие истину, то есть более честный взгляд на мир, какой он есть, да, и на себя.

491
00:40:18,700 --> 00:40:26,440
Speaker 0: То есть с одной стороны, это, конечно, катастрофа, и можно делать акцент на катастрофу, но с другой стороны, это возможность для честности и правды.

492
00:40:26,720 --> 00:40:27,580
Speaker 0: А это самоценность.

493
00:40:28,180 --> 00:40:38,200
Speaker 1: Мне кажется неизбежным то, о чём говорит Альтман, и точно так же мне кажется неизбежным вот эта вот катастрофа-апокалипсис в хорошем смысле слова.

494
00:40:38,360 --> 00:40:40,120
Speaker 1: Апокалипсис в хорошем смысле слова.

495
00:40:40,340 --> 00:40:45,020
Speaker 1: Те, кто... останутся людьми, они, в общем, не смогут ими не быть.

496
00:40:45,360 --> 00:40:46,960
Speaker 1: Или те, кто захотят остаться людьми.

497
00:40:47,280 --> 00:40:52,120
Speaker 1: Те, кто предпочут тут жить в резервации, ну, в общем, исчезнут.

498
00:40:52,260 --> 00:40:53,020
Speaker 1: Просто исчезнут.

499
00:40:53,560 --> 00:40:57,860
Speaker 1: Повторюсь, я рад, что в силу естественных причин я этого уже не увижу.

500
00:40:58,160 --> 00:41:00,200
Speaker 1: Тебе вообще, возможно, ещё и придётся.

501
00:41:00,560 --> 00:41:08,420
Speaker 0: Ну, во-первых, Альтман нам предрекает это уже к 30-м, а я надеюсь, с тобой платформа еще долго работает, так что ты тут не зарекайся особенно.

502
00:41:08,540 --> 00:41:19,840
Speaker 1: Слушай, ну вот Алиса, наша собеседница из предыдущего подкаста, она часто говорит, ну, ребята, посмотрите, насколько вот это распространение технологий на самом деле идет медленно.

503
00:41:20,140 --> 00:41:29,640
Speaker 1: Да, там на кончике иглы или на вершине пирамиды все там уже в Сан-Франциско уже беспилотные такси приезжают, как совершенно обычное явление.

504
00:41:30,020 --> 00:41:39,740
Speaker 1: А в остальном мире многие ещё воду из колодца набирают и не понимают, зачем им нужно то или другое.

505
00:41:40,220 --> 00:41:41,880
Speaker 1: Ну вот не знаю, поглядим.

506
00:41:42,240 --> 00:41:43,300
Speaker 0: Пока есть горы и лес.

507
00:41:43,820 --> 00:41:46,320
Speaker 1: В общем, так хочу сказать вам, дорогие друзья.

508
00:41:46,480 --> 00:41:50,520
Speaker 1: Если вам кажется, что мир сходит с ума, в общем, это не паранойя.

509
00:41:50,980 --> 00:41:54,320
Speaker 1: Это довольно адекватная реакция на то, что происходит вокруг.

510
00:41:54,720 --> 00:41:57,200
Speaker 1: на, в общем, несколько неадекватную реальность.

511
00:41:57,960 --> 00:42:03,080
Speaker 1: И выход не в том, чтобы становиться умнее, а в том, чтобы становиться человечнее.

512
00:42:03,760 --> 00:42:04,640
Speaker 0: Прекрасно сказано.

513
00:42:05,260 --> 00:42:05,560
Speaker 1: Хорошо.

514
00:42:05,880 --> 00:42:06,780
Speaker 1: Спасибо, друзья.

515
00:42:07,120 --> 00:42:07,820
Speaker 1: До новых встреч.

516
00:42:07,940 --> 00:42:14,840
Speaker 1: Пишите, ругайте, находите несоответствия, делайте нас лучше, полируйте, шлифуйте наши несовершенства.

517
00:42:15,180 --> 00:42:16,880
Speaker 1: Мы вам будем за это только благодарны.

518
00:42:16,940 --> 00:42:27,840
Speaker 1: Всего вам доброго.

