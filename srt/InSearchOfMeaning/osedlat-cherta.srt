1
00:00:08,382 --> 00:00:09,762
Здравствуйте, друзья!

2
00:00:09,762 --> 00:00:14,922
В эфире необычный выпуск ведущих подкаста «В поисках смысла».

3
00:00:14,922 --> 00:00:20,862
Евгения Голуба, Павла Щелина и сегодня с нами приглашённый эксперт Алиса Ким.

4
00:00:20,862 --> 00:00:29,002
Мы решили записать этот выпуск, так как первый раз мы затронули тему искусственного интеллекта, а сегодня речь пойдёт о нём.

5
00:00:29,002 --> 00:00:33,922
Примерно год назад, может быть, немного больше, и с тех пор многое изменилось.

6
00:00:33,922 --> 00:00:36,310
Я бы так сказал, что изменилось практически всё.

7
00:00:36,310 --> 00:00:41,830
И для того чтобы осмыслить происходящее, нам уже не хватает собственного понимания.

8
00:00:41,830 --> 00:00:43,830
Мы подозреваем себя в предвзятости.

9
00:00:43,830 --> 00:00:49,510
И для этого мы пригласили Алису, кандидата наук, эксперта по искусственному интеллекту.

10
00:00:49,510 --> 00:00:55,250
Алиса занималась академическими исследованиями в университете Гумбольда и в Стэнфордском университете.

11
00:00:55,250 --> 00:01:02,187
Алиса разрабатывала языковые модели в AWS, Amazon, что-то там, как она дальше.

12
00:01:02,187 --> 00:01:02,904
Веб-сервис.

13
00:01:02,904 --> 00:01:03,744
Веб-сервис.

14
00:01:03,744 --> 00:01:12,344
Ну и, в общем, последние 10 лет Алиса занимается внедрением искусственного интеллекта в разного рода системах, в стартапах и крупных корпорациях.

15
00:01:12,344 --> 00:01:18,324
Поэтому кому как ни к Алисе нам прийти с нашими вопросами и недоумениями?

16
00:01:18,413 --> 00:01:24,213
Итак, я предложил сегодня разделить роли в нашем встрече следующим образом.

17
00:01:24,213 --> 00:01:27,773
Как уже понятно, Алиса — наш эксперт, Павел — философ.

18
00:01:27,773 --> 00:01:33,833
Ну а я буду выступать сегодня в роли обычного пользователя разного рода и помощников.

19
00:01:33,837 --> 00:01:39,617
обычного обывателя, которых много и у которых есть простые незамысловатые вопросы.

20
00:01:39,617 --> 00:01:43,137
И начну я с такого простого вопроса к Алисе.

21
00:01:43,137 --> 00:01:50,317
Сегодня уже очень многие, включая меня, не мыслят в своей жизни без помощников, как мы их называем.

22
00:01:50,317 --> 00:02:01,837
У меня при запуске браузера запускается 6 штук сразу, и многие сейчас уже, собственно, ищут даже продукты через разного рода помощников.

23
00:02:01,849 --> 00:02:03,489
Так вот вопрос следующего рода.

24
00:02:03,489 --> 00:02:08,209
Всё-таки это полезная штука или за этим кроется что-то ещё?

25
00:02:08,209 --> 00:02:17,749
Как Вы считаете, насколько нагружены сегодняшние помощники скрытыми какими-то функциями, скрытыми намерениями их создателей?

26
00:02:17,749 --> 00:02:20,108
Есть ли подвох в этой технологии?

27
00:02:20,108 --> 00:02:24,148
Наверное, начну с занудного «полезно для кого и для чего».

28
00:02:24,148 --> 00:02:31,088
Для нас, как для пользователей в наших жизненных целях, безусловно, на мой взгляд, полезная вещь.

29
00:02:31,088 --> 00:02:45,123
Но мы с вами находимся в такой сложной ситуации, когда абсолютно всё в этом продукте, по сути, оптимизировано для того, чтобы мы чуть-чуть сбились с курса, забыли о том, зачем мы делаем то, что мы делаем, и как это работает.

30
00:02:45,123 --> 00:02:51,557
Если бы я должна была ответить на вопрос классический, ну так это хорошая вещь или плохая, я бы сказала.

31
00:02:51,557 --> 00:02:55,057
Хорошая, если мы будем пользоваться ей осознанно.

32
00:02:55,057 --> 00:03:01,337
И на этой части обычно все юзеры, я в том числе, говорят, ну если осознанно, то это уже как-то слишком сложно.

33
00:03:01,350 --> 00:03:06,750
Что касается нагруженности, тут, на мой взгляд, всё банально и немножко грустно.

34
00:03:06,750 --> 00:03:15,530
Практически всё о том, как создаются эти помощники, в общем-то зачем, с какими ограничениями, всё прописано, всё открыто.

35
00:03:15,530 --> 00:03:21,010
Я думаю, что там скрытых смыслов и скрытых идей и тайных помыслов довольно мало.

36
00:03:21,010 --> 00:03:27,610
Другое дело, что, знаете, как никто не считает правила пользования, все всегда всё принимают и радостно этим дальше пользуются.

37
00:03:27,726 --> 00:03:52,970
Другое дело, что мы с вами, наверное, никогда ещё не сталкивались с инструментом, по сути, телевизорами, чайниками, то есть любыми инструментами, доступными для рядового пользователя, которые были бы настолько сложны и настолько обманчивы, настолько сделаны для того, чтобы максимально с нашей головушкой помериться с силами, запутать и вести в искусство.

38
00:03:52,970 --> 00:04:10,554
Изначально же все эти системы, когда они стали популярны, не когда они стали точными, не когда они стали правильными с точки зрения даваемых ответов, а тогда, когда они стали настолько похожи на настоящего собеседника, что мы прямо включились в это.

39
00:04:10,554 --> 00:04:19,554
К сожалению, эволюционно мы очень быстро… Мы и так любим всё антропоморфизировать, с компьютерами разговариваем, с телевизорами даём имена машинам и так далее.

40
00:04:19,583 --> 00:04:25,943
А тут оно ещё и разговаривает, и отвечает, ещё и учится тому, что мы любим и не любим, и запоминает нас.

41
00:04:25,943 --> 00:04:31,923
Ну то есть тут не начать с этим взаимодействовать нас и помощникам, не как инструментом, ну практически нереально.

42
00:04:31,923 --> 00:04:41,843
Даже если вы супер в сознанке, скорее всего, через эти дни и от пользования вы уже немножко забудете про то, зачем это было сделано, как это работает и так далее.

43
00:04:41,843 --> 00:04:44,563
На мой взгляд, пользы этого можно извлечь очень много.

44
00:04:44,563 --> 00:04:52,870
Другое дело, что с такими инструментами, Прямо вот совсем-совсем нельзя забывать, зачем я это делаю и какие у этого ограничения.

45
00:04:52,870 --> 00:04:55,050
Они все прописаны, они все понятны.

46
00:04:55,050 --> 00:05:01,550
Но кроме тех, которые не прописаны, по идее, должны быть понятны любому человеку, что это коммерческий инструмент.

47
00:05:01,550 --> 00:05:08,970
Наверное, там есть какая-то доля того, что на вас хотят заработать, и всё общее благо мира, наверное, там не единственная цель.

48
00:05:08,970 --> 00:05:12,150
То есть, скорее всего, этот компонент тоже есть, но мы про это не очень любим помнить.

49
00:05:12,150 --> 00:05:15,820
А это влияет на то, как дальше развиваются эти системы.

50
00:05:15,820 --> 00:05:22,110
То есть искусственный интеллект, как любой инструмент, может быть использован по прямому назначению, его благо.

51
00:05:22,110 --> 00:05:26,630
Или как кухонный нож можно резать мясо, можно зарезать человека.

52
00:05:26,630 --> 00:05:33,210
Как технология книгопечатания можно печатать Библию, а можно порно-рисунки издавать.

53
00:05:33,242 --> 00:05:36,602
И здесь уже вопрос у меня к Павлу.

54
00:05:36,602 --> 00:05:52,642
Мы говорили с тобой год, наверное, немного больше назад о том, что надвигаются времена, когда многое из того, что делает человека человеком, будет, скорее всего, передано на аутсорс, искусственному интеллекту.

55
00:05:52,642 --> 00:06:08,087
Мы тогда с тобой видели риски того, что вот это творческое начало, чувства, эмоции начнут автоматизироваться, и, соответственно, в этой части есть риски для людей потерять свой компонент человечности.

56
00:06:08,087 --> 00:06:09,887
Что ты думаешь по этому поводу?

57
00:06:10,215 --> 00:06:19,635
Первое, всё-таки сделаю базовые комментарии для фиксации собственной позиции по преамбуле, про вот кухонный нож и прочую всю эту историю.

58
00:06:19,635 --> 00:06:23,235
Дело в том, что вот с этой позиции я философски не согласен.

59
00:06:23,235 --> 00:06:31,055
Мне представляется, что само представление существования такого феномена как нейтральная технология является глубоким заблуждением.

60
00:06:31,073 --> 00:06:35,053
Не существует такого феномена, как нейтральность технологий.

61
00:06:35,053 --> 00:06:40,173
С самим фактом своего существования технология не нейтральна.

62
00:06:40,173 --> 00:06:47,593
Она создаёт ассиметрию банально между теми, кто технологией пользоваться умеет, и кто технологией пользоваться не умеет.

63
00:06:47,593 --> 00:06:58,033
Те, кто технологией пользоваться умеют, получают дополнительные ресурсы, власть, способ взаимодействия с миром относительно тех, кто ей не пользуется.

64
00:06:58,073 --> 00:07:01,793
Вот это и есть, собственно, сама технологическая асимметрия.

65
00:07:01,793 --> 00:07:08,713
То, что ты говоришь уже про волевой, этический выбор субъекта, использующий технологию, — это следующий этап, это следующий уровень.

66
00:07:08,713 --> 00:07:15,173
Но сначала есть вот этот базовый уровень, что самим фактом своего бытия технология мир меняет.

67
00:07:15,173 --> 00:07:25,493
И вот наше представление о ней как о некой нейтральности — это очень хороший sales point для любого, скажем так, маркетолога, но с философской точки зрения просто он неадекватный.

68
00:07:25,680 --> 00:07:33,100
Ты знаешь, мне кажется, нужно объяснить твой тезис о том, что самим фактом существования технология меняет мир.

69
00:07:33,100 --> 00:07:37,500
Логическая связь здесь не очевидна, по крайней мере, для меня.

70
00:07:37,500 --> 00:07:38,680
Давай самое простое.

71
00:07:38,680 --> 00:07:52,540
Вот если пока этой технологии не было, у тебя была определённая культура и определённые закономерности отношений между людьми, отношений экономические, социальные, политические и так далее, как ты ни крути.

72
00:07:52,562 --> 00:07:58,602
Вот когда технология появилась, она стала фактором всех этих отношений просто по факту своего появления.

73
00:07:58,602 --> 00:08:02,022
Еще никакой воли нет, но она создала дополнительные возможности.

74
00:08:02,022 --> 00:08:06,662
Повторюсь, главная эта возможность, я ее назвал, это возможность к власти.

75
00:08:06,662 --> 00:08:10,669
Любая технология содержит в себе заряд к власти.

76
00:08:10,669 --> 00:08:11,529
Но это логично.

77
00:08:11,529 --> 00:08:13,589
Если бы она его не содержала, ее бы никто не создал.

78
00:08:13,589 --> 00:08:14,909
Это вот очень важно понимать.

79
00:08:14,909 --> 00:08:22,529
То, что в нашем культуре технология всегда создается, на самом деле, с неким зарядом к власти.

80
00:08:22,529 --> 00:08:23,789
В нашем конкретном примере.

81
00:08:23,789 --> 00:08:30,149
Власть манипулирования, власть работы с данными, власть производства дополнительного материального ресурса и так далее.

82
00:08:30,149 --> 00:08:31,889
Это все властные отношения.

83
00:08:31,889 --> 00:08:38,930
И вот эта технология, ты можешь сказать, если тебе уточнить, Эта барыда времени может сказать, что она создается с технологией как минимум потенциальной власти.

84
00:08:38,930 --> 00:08:46,790
То есть она лежит и спит, да, требуется волевой субъект, чтобы эту власть активизировать, но тем не менее потенциал-то уже создан самим фактом ее появления.

85
00:08:46,790 --> 00:08:49,150
То есть некая статус-кво оказалась нарушенным.

86
00:08:49,150 --> 00:08:49,510
Все.

87
00:08:49,510 --> 00:08:58,630
Изобретение автомобиля, вне зависимости от намерений конкретного водителя или производства автомобиля, вот самим фактом своей технологии является угрозой, условно говоря, для коневодов.

88
00:08:58,630 --> 00:09:02,970
Она меняет эти отношения автоматически, просто по факту своего появления.

89
00:09:03,015 --> 00:09:05,575
Технология может существовать, продукта может не быть.

90
00:09:05,575 --> 00:09:06,075
Ты прав.

91
00:09:06,075 --> 00:09:07,115
Как минимум потенциально.

92
00:09:07,115 --> 00:09:18,195
С философской точки зрения я веду категорию потенциальное изменение, но просто мы живем еще в цивилизации последние 400 лет, где любое потенциальное изменение в зоне технологии и прогресса должно быть актуализировано.

93
00:09:18,195 --> 00:09:22,715
У нас нет никаких этических ограничений на любую технологическую актуализацию.

94
00:09:22,715 --> 00:09:24,995
Это, собственно, ради этого модерн мы и создавали.

95
00:09:25,257 --> 00:09:37,937
Мы говорим о том, что в модерне каждая технология прежде всего рассматривается с точки зрения возможности увеличить властный потенциал субъекта, обладающего этой технологией.

96
00:09:37,937 --> 00:09:38,377
И да.

97
00:09:38,377 --> 00:09:45,277
А в любой технологии, и до модерна, и после модерна, и вне модерна, всегда содержится увеличение потенциала субъекта.

98
00:09:45,277 --> 00:09:46,117
Простой пример.

99
00:09:46,117 --> 00:09:47,877
Я не могу бегать как гепард?

100
00:09:47,987 --> 00:09:52,027
Но с машиной я могу перемещаться со скоростью, которой гепарду и не снилось.

101
00:09:52,027 --> 00:09:55,887
Мой потенциал в категории бегания, она усиливает.

102
00:09:55,887 --> 00:09:57,427
Она поэтому и создана.

103
00:09:57,427 --> 00:10:02,507
Теперь, я тебя прервал, может быть, ты сделаешь шаг назад и вернешься к своему второму тезису?

104
00:10:02,507 --> 00:10:06,667
Он связанный с первым тезисом, и это проблема асимметрии последствий.

105
00:10:06,667 --> 00:10:11,207
Другими словами, это вот был бы мой вопрос к Алисе следующий, если она в робот.

106
00:10:11,207 --> 00:10:14,647
Мне просто интересно, есть ли размышления на эту тему.

107
00:10:14,647 --> 00:10:17,266
Приведу пример не с искусственным интеллектом, но близким.

108
00:10:17,266 --> 00:10:40,547
У нас есть технологии социальных сетей, выпущенные относительно недавно, буквально 15 лет назад Сейчас начали выходить исследования Айн-Нейра в изменениях на материальном уровне мозга молодых, особенно детей, подростков, девочек, которые 10 лет выросли на этих технологиях Скажем так, исследования, мягко говоря, тревожащие, там много разных неприятных последствий Но я сейчас говорю не про это.

109
00:10:40,547 --> 00:10:42,627
Я говорю про то, что вот у нас есть асимметрия.

110
00:10:42,627 --> 00:10:46,927
Технология выпущена была 10 лет назад, впоследствии от нее пришли 15 лет.

111
00:10:46,927 --> 00:10:56,467
И что-то я сомневаюсь, что 15 лет назад, когда люди выпускали социальные дети, вообще хоть на каком-то этапе выпуска этой технологии задумывались о последствиях через 15 лет.

112
00:10:56,467 --> 00:10:58,187
Это ее структурное ограничение.

113
00:10:58,187 --> 00:11:02,507
По крайней мере, в нашей культуре, где скорость является благом сама по себе.

114
00:11:02,507 --> 00:11:09,327
В принципе, идея торможения, движения, особенно технологического, является ересью и харамом.

115
00:11:09,327 --> 00:11:10,807
У меня есть вот большой вопрос.

116
00:11:10,807 --> 00:11:14,147
Проблема технологической асимметрии последствий существовала всегда.

117
00:11:14,147 --> 00:11:16,547
Собственно, мы это знаем со времен ящика Пандоры.

118
00:11:16,952 --> 00:11:18,932
Миф ящика Пандоры ровно про это.

119
00:11:18,932 --> 00:11:22,432
Принесли огонь, а потом выпушил, как-то получился ящик.

120
00:11:22,432 --> 00:11:25,092
Это вот очень классическая взаимосвязанная история.

121
00:11:25,092 --> 00:11:34,832
Но сегодня мы просто повысили масштабы, скажем так, этой проблемы до определенного уровня, который в каком-то смысле количественно действительно является беспрецедентом.

122
00:11:34,832 --> 00:11:36,952
Это вот такая моя занятка о зарубках на полях.

123
00:11:36,952 --> 00:11:45,472
Мне просто интересно, как изнутри вообще ставится ли так вопрос, вот по-честному, не на уровне красивых презентаций, а как внутреннее самоощущение.

124
00:11:45,741 --> 00:11:54,621
Имеется в виду вопрос, насколько обсуждается и насколько озабочены компании, разрабатывающие большие языковые модели, озабочены последствиями.

125
00:11:54,621 --> 00:12:00,741
Не возьмусь говорить, конечно, за всех гигантов индустрии, но такую среднёную позицию сформулировать.

126
00:12:02,346 --> 00:12:16,446
философская, идеалистическая воля их основателей, CEO, предыдущих учёных, которые все как один пишут и, скорее всего, действительно думают о том, что они очень хотели сделать мир лучше.

127
00:12:16,446 --> 00:12:19,126
Их позиции почти всегда такие очень публичные.

128
00:12:19,126 --> 00:12:22,455
Я думаю, что они действительно в это верят.

129
00:12:22,455 --> 00:12:25,084
Есть то, как это работает внутри.

130
00:12:25,084 --> 00:12:41,563
В целом, почти все негативные последствия, которые вызывают эти продукты, они практически всегда важны, только когда они коротковременные и влияют на два самых важных фактора, которые, в свою очередь, влияют на то, довольны инвесторами или нет.

131
00:12:41,563 --> 00:12:44,403
К сожалению, тот единственный драйвер, который важен.

132
00:12:44,403 --> 00:12:54,303
В силу, кстати, того, что вы сейчас сказали о скорости, потому что сейчас у абсолютно всех участников рынка 100% уверенность в том, что вот сейчас мы в этой точке бифоркации.

133
00:12:54,303 --> 00:12:58,343
Тот, кто успеет и возьмёт рынок, тот будет править следующие 100 лет.

134
00:12:58,343 --> 00:13:03,503
А ещё хорошо бы, чтобы хотя бы в нашей стране, а есть же ещё китайцы, русские и все остальные.

135
00:13:03,503 --> 00:13:04,203
Дальше посмотрим.

136
00:13:04,203 --> 00:13:05,983
Сейчас главное — нужно успевать.

137
00:13:05,983 --> 00:13:10,321
Для этого нужно очень много денег на самые разные вещи.

138
00:13:10,321 --> 00:13:11,438
Что не любят инвесторы?

139
00:13:11,438 --> 00:13:17,978
Инвесторы не любят, когда по рукам дают регулятор, и инвесторы не любят, когда сильно жалуются и отпадает пользователь.

140
00:13:17,978 --> 00:13:32,931
Вот если кто-то из них посмотрел непосредственный вред в каком-то виде, сумел это довести до точки, когда реально уже наступает прессинг на компанию внести какие-то изменения, то тут компания может официально как-то позицию заявить.

141
00:13:32,931 --> 00:13:38,351
То есть, например, сейчас у всех компаний прописаны их, условно говоря, ценности и ориентиры.

142
00:13:38,351 --> 00:13:45,851
Например, OpenAI — это вот мы хотим, чтобы мы были helpful, но no harm и maximize utility — вот это их такая общая тема.

143
00:13:45,851 --> 00:13:54,285
И это влияет на то, что они реально внутри пытаются делать для того, чтобы как-то ограничить негативные, например, вот этот вот no harm обеспечить.

144
00:13:54,285 --> 00:14:03,698
Но в целом есть, к сожалению, такая неприятная история, такой конфликт интересов, что, знаете, принципиального человека уломать на что угодно гораздо сложнее.

145
00:14:03,698 --> 00:14:12,818
Вот модель, у которой слишком много ограничений, она, скорее всего, будет не так хорошо, красиво работать, её тренировать дороже, ей могут быть недовольны пользователи.

146
00:14:12,818 --> 00:14:18,018
И поэтому в целом мотивации реально усложнять эту историю у компаний нет никакой.

147
00:14:18,018 --> 00:14:22,487
Только вот те два фактора, которые я сказала, регуляторы и пользователи.

148
00:14:22,487 --> 00:14:26,316
И это еще мы находимся в очень, на самом деле, маленьком кружочке.

149
00:14:26,316 --> 00:14:28,116
Я этот вопрос задал гораздо больше.

150
00:14:28,116 --> 00:14:39,047
Вы, по сути, подчеркнули проблему некого knowable harm, в которой ты получаешь информацию об этом вреде через вот этот фидбэк-клуб некий, да, и ты хоть о каком-то информации получаешь.

151
00:14:39,047 --> 00:14:54,847
Я же постулировал вопрос более радикально, потому что, как в примере с этих социальных сетей, ни за что не буду утверждать, что у людей, которые вводили социальные сети как корпоративный метод в середине 2000-х, было намерение сломать психику девочкам-подросткам в 2025 году.

152
00:14:54,847 --> 00:15:15,395
Основная проблема в том, что есть огромная сфера того, что мы не знаем о технологическом последствии, Мы в теории могли бы попытаться об этом думать, как, не знаю, категория философского риска, промышления, но, как я понимаю, из вашего описания, разумеется, не по причине некого зла, а по причине той системы, скажем так, мотивации к действию, такой вопрос в принципе никто не ставит.

153
00:15:15,395 --> 00:15:19,935
Если последствия будут через 15 лет, нас это вообще никаким образом сегодня не волнует.

154
00:15:20,104 --> 00:15:35,244
Про это пытаются думать и даже нанимают дорогостоящих исследователей, образовывают целые «финк-тэнки» внутри компаний, и они даже публикуют желательно не сильно, конечно, радикальные работы, но показать социальную ответственность очень надо.

155
00:15:35,244 --> 00:15:38,584
Но нет времени и денег у компаний сейчас об этом думать.

156
00:15:38,584 --> 00:15:49,084
Разве что какие-то более независимые институты могут пытаться делать какие-то проекты, они их делают, но нет времени и возможностей, слишком велика конкуренция, слишком велик прессинг.

157
00:15:49,450 --> 00:15:50,910
Просто на это никак.

158
00:15:50,910 --> 00:15:55,330
Хотела отдельно прокомментировать то, что Вы сказали касательно нейтральности технологии.

159
00:15:55,330 --> 00:16:00,990
Мы здесь, безусловно, не имеем дела с технологией, которая даже подаётся как нейтральная.

160
00:16:00,990 --> 00:16:20,778
То есть, во-первых, большинство этих решений подаются с очень громким информационным бэкграундом того, Мы это делаем ради того, чтобы человечество тут лучше жило, чтобы вам, дорогие пользователи, дать свободу, то есть «freedom to the users» — он прямо это обещает.

161
00:16:20,778 --> 00:16:30,378
Потом уже появляются «safety» и так далее, но это в целом подаётся очень агрессивно, как это прямо то, что сейчас вам всем сделает лучше.

162
00:16:30,378 --> 00:16:48,337
И для того, чтобы этой технологией пользоваться действительно максимально осознанно, как-то максимально возможно и безопасно, тоже нужно найти третий ход слева за трактором, повернуть направо и желательно отключить вот этот вот ещё функционал, вот эту информацию не давать, а вот здесь ещё перезагрузиться.

163
00:16:48,337 --> 00:16:51,737
И тогда в целом, наверное, будет чуть получше.

164
00:16:51,737 --> 00:16:53,338
Ну то есть уровень сложности.

165
00:16:53,338 --> 00:17:04,158
количество сальто, которое нужно сделать, чтобы действительно эта технология для вас, когда пользователя, была нейтральной, бесчестно велика для того, чтобы утверждать.

166
00:17:04,158 --> 00:17:07,198
Да нет, мы же вам всё по-честному дали.

167
00:17:07,198 --> 00:17:10,998
Это всё вы, это ваше пользование дало вам плохие результаты.

168
00:17:10,998 --> 00:17:11,518
Это не мы.

169
00:17:11,518 --> 00:17:13,738
Тут нужно просто это по-честному отметить.

170
00:17:14,044 --> 00:17:15,644
У меня будет два комментария.

171
00:17:15,644 --> 00:17:16,304
Первое.

172
00:17:16,304 --> 00:17:26,464
Так как я постоянно рассказываю о том, что человек выходит из корпоративного мира, то я знаю цену всем вот этим корпоративным миссиям, виденью и всему остальному.

173
00:17:26,464 --> 00:17:28,484
Цена эта не очень высока.

174
00:17:28,484 --> 00:17:31,324
Это всё, в общем, известное лицемерие.

175
00:17:31,324 --> 00:17:34,664
И во главе угла всегда стоят только деньги.

176
00:17:34,665 --> 00:17:37,445
Деньги и власть.

177
00:17:37,445 --> 00:18:05,803
Поэтому если на пути у топ-менеджмента становятся какие-то не вполне очевидные или сомнительные свойства продукта, то топ-менеджмент всегда Повторяю, всегда, прежде всего, попытается добиться максимального финансового результата, конечно же, с одной стороны, снижая риски для себя, и главным образом, как бы кто об этом ничего не узнал, или как бы ничего не вышло с точки зрения пиара, а уже потом будет думать о всех этих миссиях и видениях.

178
00:18:05,803 --> 00:18:10,983
Миссии и видения нужны для того, чтобы красиво выступать на конференциях и сорвать аплодисменты.

179
00:18:10,983 --> 00:18:19,816
Поэтому наличие миссии и видения для меня совершенно не успокаивает, а даже скорее наоборот говорит о том, что если такое миссия и видение, значит точно там где-то что-то не так.

180
00:18:19,816 --> 00:18:20,749
Это первое.

181
00:18:20,749 --> 00:18:39,313
А второе — наблюдение за нашими лидерами мнений, вот этими замечательными гениями технологическими, вроде Сэма Альтмана, который, как мы уже говорили с Павлом, в своём послании «Городу и миру» заявил о том, что мы в двух шагах от райских кущ, которые нам произведёт искусственный интеллект.

182
00:18:39,313 --> 00:18:56,337
И при этом он перечислял какие-то такие, скажем так, свойства искусственного интеллекта и привёл такие доводы, которые можно, наверное, оглянувшись назад, было бы услышать от изобретателей, не знаю, электрических двигателей, стиральных машин, паровозов и так далее, и так далее.

183
00:18:56,337 --> 00:18:59,717
То есть кажется, что, дружище, ну что ж ты повторяешь-то всё одно и то же?

184
00:18:59,717 --> 00:19:02,377
Ну как изменилась жизнь обывателя к лучшему?

185
00:19:02,377 --> 00:19:04,537
За счёт технологии она стала комфортнее, да.

186
00:19:04,537 --> 00:19:05,217
И что?

187
00:19:05,217 --> 00:19:06,517
И к чему это привело?

188
00:19:07,090 --> 00:19:11,350
твоя технология отнимает у него последний шанс к творчеству, как мне кажется.

189
00:19:11,350 --> 00:19:16,990
Алиса, ваши комментарии как человека близко к корпоративному миру, ну и, конечно же, Павла хотелось бы послушать.

190
00:19:17,012 --> 00:19:36,012
Нужно просто сказать, что даже если внезапно самые айфоны Марки Цукерберге этого мира решат, всё, давайте, ребята, забудем про деньги, будем заниматься, вот прямо действительно постараемся, чтобы наши атом-ЛЛМы, наши клоды, наши чаты-ГПТ, они прямо сели разумное, доброе, вечное и так далее.

191
00:19:36,012 --> 00:19:38,612
Вот прямо вот сейчас, эх, мы возьмёмся.

192
00:19:38,612 --> 00:19:45,288
Я здесь, конечно, не скажу совсем за bleeding edge, того, что существует.

193
00:19:45,288 --> 00:20:20,632
Но в целом, насколько мне известно, даже если мы очень сильно захотим, наши границы возможного для того, чтобы действительно заставить моделей, если только мы их не превратим вот в попок, которые вот если тебя спросили, я отвечаю «это», мы просто не можем точно быть уверены в том, что модели будут действовать так, как нам надо, что они будут демонстрировать то поведение, те ценности, которые мы в них хотели заложить, исходя из своего представления о том, как это должно быть, и исходя из фидбэка вьюзеров, чтобы им тоже не навредить, мы здесь просто ещё технически ограничены.

194
00:20:20,632 --> 00:20:39,285
Несмотря на то, что ограничения в разной степени контроля направления этих моделей закладываются на целом ряде разных шагов, которые происходят во время тренировки, подготовки, тюнинга этих моделей, пользования уже этими моделями, Мы на всех этих шагах довольно сильно ограничены.

195
00:20:39,285 --> 00:20:43,605
Хочу здесь просто также оговориться, что есть некий потолок того, что мы можем сделать.

196
00:20:43,605 --> 00:20:56,242
И даже если вы тысячу раз спросите моделей, ну вот если тебе попросят рецепт коктейля молота, ну например, спросит, самый лучший способ сделать что-нибудь нехорошее, ты же ничего не ответишь?

197
00:20:56,242 --> 00:20:59,162
Тысячу раз модель отвечает, конечно, ничего не отвечу, нельзя.

198
00:20:59,162 --> 00:21:00,882
Тысяча первыми может ответить.

199
00:21:00,882 --> 00:21:12,322
То есть наша личная степень контроля, несмотря на то, что она очень велика, и, конечно же, вообще мы целиком формируем то, какую информацию они выдают, как действуют, как принимают решения, мы всё равно довольно сильно ограничены.

200
00:21:12,707 --> 00:21:17,727
Среди обывателей, скажем так, распространено две крайние точки зрения.

201
00:21:17,727 --> 00:21:25,167
С одной стороны, «да что вы там мне рассказываете про этот искусственный интеллект, он галлюцинир, это вообще продвинутая Т9, которую просто развили».

202
00:21:25,167 --> 00:21:39,387
А с другой стороны, «господи, это магия какая-то, это вообще душа, наконец-то меня кто-то понял, услышал, не рассказывайте мне все ваши сказки про Т9, это просто уже новая сущность какая-то, совершенно непредставимая раньше».

203
00:21:39,387 --> 00:21:44,907
И сейчас, Алиса, то, что вы говорите, звучит как «мы вообще не понимаем, как это работает».

204
00:21:45,225 --> 00:21:50,505
Есть огромное ограничение в том, что там называется «объяснимость сетей».

205
00:21:50,505 --> 00:22:04,785
То есть если мы спросим даже самую суперпродвинутую модель, «А почему ты думаешь, что это плохо?», мы не можем быть уверены, что она нам отвечает честно и что она дала какой-то ответ именно потому, что она думает так, как мы хотим, чтобы она думала.

206
00:22:04,785 --> 00:22:12,009
То есть мы всё равно здесь взаимодействуем с очень высокой степенью неуверенности того, почему она делает то, что она делает.

207
00:22:12,009 --> 00:22:13,492
Но вы говорите, она думает.

208
00:22:13,492 --> 00:22:14,812
Она вообще думает вообще?

209
00:22:14,812 --> 00:22:22,252
Там же, как я понимаю, идёт огромный подбор вариантов в сочетании тех или иных смысловых знаков и так далее.

210
00:22:22,252 --> 00:22:23,972
Можно сказать, что она думает вообще?

211
00:22:24,088 --> 00:22:35,968
С учётом того, что первые нейронные сети и вообще перисоптрон создавался как моделька вот этого нейрона в голове, я думаю, что мы склонны использовать глагол «вроде думает».

212
00:22:35,968 --> 00:23:02,551
Но нет, это последовательность неких математических действий, которые обусловлены оптимизацией, которая была проведена некими правилами, которые были вшиты в это всё во время тренировки модели, которые приводят к тому, что модель отвечает что-то определённое, но там этих вот ходов, которые не нами прописаны, а которые возникли сами в ходе тренировки, подготовки этой модели, их гораздо больше.

213
00:23:02,551 --> 00:23:06,394
То есть какую-то часть мы контролируем и видим, а какую-то часть мы всё ещё не видим.

214
00:23:06,394 --> 00:23:09,334
Ну да, звучит всё-таки немного жутковато.

215
00:23:09,334 --> 00:23:11,914
Жутковато, но оптимистично.

216
00:23:11,914 --> 00:23:12,914
Объясню почему.

217
00:23:12,914 --> 00:23:14,074
Повод задуматься.

218
00:23:14,074 --> 00:23:23,392
Мне нравится, что на фоне всей этой искусственно-интеллектной истории актуализируется постепенный интерес к по-настоящему важным вопросам эсхатологически.

219
00:23:23,392 --> 00:23:24,930
и антологическим.

220
00:23:24,930 --> 00:23:28,770
Мне тоже уже 20 человек переслали ссылки на лекцию Питера Тиля.

221
00:23:28,770 --> 00:23:38,730
Тоже поразительная черта нашей эпохи, то есть то, что говорит батюшка Самон, условно тысячелетиями никому неинтересно, но тут Тиль четыре лекции сделает!

222
00:23:38,730 --> 00:23:39,190
Вау!

223
00:23:39,190 --> 00:23:43,190
Ничего не знаю о содержании этих лекций, просто интересно само вот это общественное восприятие.

224
00:23:43,190 --> 00:23:52,048
И эта реакция, мне кажется, сама по себе свидетельствует о том, что вот это столкновение с вот этой Вот ты употреблял раньше слово магия, я бы на нем, честно говоря, остановился.

225
00:23:52,048 --> 00:23:56,808
То есть на каком-то уровне бытия это действительно магическая история.

226
00:23:56,808 --> 00:24:08,808
Просто если мы не ограничиваемся только материальными причинами, о чем весьма подробно рассказала Алиса, а подключаем к этому субъективный опыт взаимодействия пользователя с этой системой, то он действительно очень магичен.

227
00:24:08,832 --> 00:24:12,532
в своем вот именно опыте, поэтому здесь как раз особых противоречий нет.

228
00:24:12,532 --> 00:24:22,552
Перед этим самым пользователем, уверенным в себе и в своем интеллектуальном превосходстве над неорганической природой до недавнего времени, искусственный интеллект, конечно, ставит очень большой вопрос.

229
00:24:22,552 --> 00:24:24,132
А ты, собственно, человек или нет?

230
00:24:24,132 --> 00:24:25,972
А что в тебе, собственно, человеческого?

231
00:24:25,972 --> 00:24:31,492
А что ты готов пожертвовать, чтобы сохранить в себе некую человеческую особенность?

232
00:24:31,492 --> 00:24:38,450
Здесь я объясню, я имею в виду сейчас даже не какие-то страшные пожертвования, А очень практические, но отсюда не менее страшные.

233
00:24:38,450 --> 00:24:39,350
Начну издалека.

234
00:24:39,350 --> 00:24:51,318
Классические примеры проблемы технологии сформулирован Васькой нашим Платоном, а еще вот в знаменитом разговоре египетского бога Тото с жрецами, который жаловался на изобретение письменности.

235
00:24:51,318 --> 00:24:53,746
Тоже к вопросу о нейтральности технологий.

236
00:24:53,746 --> 00:25:03,366
Бог тот, если конкретно жаловался на то, что вы сейчас писать научитесь, и саги, и священные тексты длиной в 15 тысяч строчек запоминать перестанете.

237
00:25:03,366 --> 00:25:06,386
То есть вы потеряете способность это делать.

238
00:25:06,386 --> 00:25:09,606
И вот в каком-то смысле любая технология несет в себе эту угрозу.

239
00:25:09,606 --> 00:25:13,602
На самом деле с письменностью в широком значении произошло действительно именно это.

240
00:25:13,602 --> 00:25:18,298
Я уже молчу о том, что есть большая разница между научиться читать и научиться понимать текст.

241
00:25:18,298 --> 00:25:26,178
Это у нас отдельная проблема, то что разрыв между этими двумя, скажем так, феноменами нашего опыта тоже нами очень редко осознаётся.

242
00:25:26,178 --> 00:25:29,058
Так вот, тем не менее, мы о себе думаем очень высокого мнения.

243
00:25:29,058 --> 00:25:33,338
И тут ИИ ставит перед нами очень такую, на мой взгляд, радикальную задачу.

244
00:25:33,338 --> 00:25:43,454
Вот как раз всё не творческое, всё построено на комбинаторике, всё построено на запоминании даже количества беспонимания.

245
00:25:43,454 --> 00:25:49,734
Я вот уточню некоторым, то есть количественная информация и беспонимание сути этой информации, он действительно сделает лучше нас.

246
00:25:49,734 --> 00:25:54,274
И это ставит нас действительно перед радикальным зеркалом, а, собственно, повторюсь, кто-то и есть.

247
00:25:54,274 --> 00:26:22,728
Как бы могла бы выглядеть альтернативная практическая, скажем так, Мыслим ли нам по-честному сценарий, что на каждый час использования интеллекта ради рациональных, усиления твоего могущества по организации собственного дня и приобретения дополнительно свободного времени человек будет тратить полтора часа на запоминание стихов, саг, псалмов или прочтение бумагиточной книги не потому, что это более эффективно, а для того, чтобы сохранить вот эту свою способность человеческого восприятия?

248
00:26:22,752 --> 00:26:26,972
Вот это, мне кажется, есть вопрос, который сегодня стоит по-настоящему.

249
00:26:26,972 --> 00:26:39,692
Другими словами, мне кажется, взаимодействовать с определенной субъектностью ИИ, но в любом случае очевидно, что взаимодействовать с этой машинкой можно только, постоянно повышая субъектность собственную.

250
00:26:39,692 --> 00:26:41,392
И вот в этом-то у нас проблема.

251
00:26:41,392 --> 00:26:49,892
То, что вот эта вторая часть, наша уже культура, общество, да и само мышление о технологиях вообще ни разу не поднимает.

252
00:26:49,892 --> 00:26:52,252
Мы не ставим принципиально так вопрос.

253
00:26:52,536 --> 00:26:55,696
Для меня именно в этом-то и заключена главная опасность.

254
00:26:55,696 --> 00:26:59,036
Да, технология в метафоре классической это черт.

255
00:26:59,036 --> 00:27:02,656
Причем, возможно, такой нынешний черт, он прям всем чертям черт.

256
00:27:02,656 --> 00:27:06,236
Но, как говорят русские сказки, черта иногда можно попытаться оседлать.

257
00:27:06,236 --> 00:27:09,196
Ну, как укула, летающая на черте за черевичками.

258
00:27:09,196 --> 00:27:14,656
Но чтобы очертать этого черта, даже в сказках, ты должен проявить субъектность больше, чем у этого самого черта.

259
00:27:14,656 --> 00:27:17,476
А с субъектностью у нас коллективная напряженка.

260
00:27:17,476 --> 00:27:19,756
И вот как-то так я вижу эту проблематику.

261
00:27:20,059 --> 00:27:22,579
Задумался я о твоих словах.

262
00:27:22,579 --> 00:27:29,919
Этот образ вакуула, оседлавшего чёрта, я уже его слышал, по-моему, на одном из твоих интервью.

263
00:27:29,919 --> 00:27:31,299
Интересный образ.

264
00:27:31,299 --> 00:27:35,259
Но получается так, что мы фактически стоимся заложниками.

265
00:27:35,259 --> 00:27:36,799
Технологию не остановить.

266
00:27:36,799 --> 00:27:44,279
Уровень, скажем так, мудрости разработчиков искусственного интеллекта, на мой взгляд, довольно невысок.

267
00:27:44,279 --> 00:27:48,387
Логика отношений капиталистических толкает нас к тому, что нужно максимизировать прибыль.

268
00:27:48,387 --> 00:27:56,205
Да, будут говорить про всех стейкхолдеров и нужно всеобщее, но давайте честно, деньги и власть по прежнему решают.

269
00:27:56,205 --> 00:27:59,785
Когда я это говорил, я это говорил, прежде всего, в индивидуальном порядке.

270
00:27:59,785 --> 00:28:04,085
Но второй момент, здесь есть очень интересный аспект, который я всё-таки тогда тоже здесь закину.

271
00:28:04,085 --> 00:28:07,725
Связан он, уж извини меня, напрямую с властью и вот этой драмократией.

272
00:28:07,725 --> 00:28:12,205
Это к тому, что Алиса упоминала ранее о страшных русских, американцах и китайцах.

273
00:28:12,205 --> 00:28:19,925
Проблема в том, что с этими искусственными интеллектами на данный момент мы имеем очень интересный парадокс уже с точки зрения такой некой теории.

274
00:28:19,925 --> 00:28:23,885
Метафорично все сейчас заняты выработкой технологического меча.

275
00:28:23,948 --> 00:28:29,208
Этот меч все оттачивают до такой степени, чтобы нанести первый обезоруживающий удар.

276
00:28:29,208 --> 00:28:29,988
Образно, да?

277
00:28:29,988 --> 00:28:34,808
То есть я не говорю, что прямо у всех есть такое намерение, но общее восприятие картинки идет примерно таково.

278
00:28:34,808 --> 00:28:36,828
Плюс, да, делаются словесные омажи.

279
00:28:36,828 --> 00:28:40,908
Ну, мы, наверное, говоримся об общих правилах, какая-то гарантия взаимного уничтожения.

280
00:28:40,908 --> 00:28:41,608
Чёрт-вство.

281
00:28:41,608 --> 00:28:43,528
Ничего подобного вообще не происходит.

282
00:28:43,528 --> 00:28:45,148
Проблема-то в чём заключается?

283
00:28:45,148 --> 00:28:48,468
То, что щит находится вообще в другой области.

284
00:28:48,468 --> 00:28:51,428
То есть обычно щит и меч должны находиться в одной топологии.

285
00:28:51,572 --> 00:28:56,412
А у нас получается меч технологичный, а щит антропологичный.

286
00:28:56,412 --> 00:28:59,492
Да, вот это, я думаю, самое важное, что нужно понимать.

287
00:28:59,492 --> 00:29:08,532
Нет технологии, которой можно противопоставить технологии искусственного интеллекта, потому что она затрагивает уже сущностные свойства человека как такового.

288
00:29:08,532 --> 00:29:18,512
Да, то есть единственный способ даже защититься вам как государству, начальству и прочим, это иметь такое население, которое обладает антропологической защитой от технологического манипулирования.

289
00:29:18,555 --> 00:29:25,315
Это, собственно, и есть то, что я описывал, как повышение антропологической субъектности в ответ на повышение технологического вызова.

290
00:29:25,315 --> 00:29:30,795
Тут тогда возникают очень неприятные последствия для всей нашей политической системы, о которых я говорю из эфира в эфир.

291
00:29:30,795 --> 00:29:33,355
Алиса в курсе, и ты тоже, в принципе.

292
00:29:33,591 --> 00:29:35,031
Но другого выхода нет.

293
00:29:35,031 --> 00:29:38,511
И мне кажется, в этом тоже есть определённая парадоксальная красота.

294
00:29:38,511 --> 00:29:49,071
Ты говоришь о том, что, скажем, предыдущие технологии нас с вами пытались оскотинить, да, давая максимум комфорта и не стимулируя быть с людьми в полном смысле этого слова.

295
00:29:49,071 --> 00:29:51,211
А сейчас будет радикальный выбор.

296
00:29:51,211 --> 00:29:56,511
Либо ты уже совсем в предаток превратишься к экранчику, либо станешь человеком.

297
00:29:56,511 --> 00:29:58,431
Но давай мы зададим слово нашему эксперту.

298
00:29:58,583 --> 00:30:04,123
Ваши мысли по поводу слов Павла о том, что может противостоять технологии в данном случае?

299
00:30:04,185 --> 00:30:08,125
Я себе позволю такое маленькое грустное отхождение.

300
00:30:08,125 --> 00:30:14,505
Очень любят во всех статьях про искусственный интеллект сейчас, конечно же, писать про Азимова, про три закона о робототехнике.

301
00:30:14,505 --> 00:30:24,605
Если, собственно говоря, почитать его общую серию о роботах, там есть такая замечательная история, которая называется «Лжать», в которой удалось создать робота, который читает человеческие мысли.

302
00:30:24,605 --> 00:30:30,647
И так как он не может людям вредить, то он им начинает врать напропалу, их чувства не задеть.

303
00:30:30,647 --> 00:30:32,387
То есть вред же может быть эмоциональный.

304
00:30:32,387 --> 00:30:37,287
И как только они это понимают, это робота сразу же разбирает, потому что ну зачем человечеству такой робот?

305
00:30:37,287 --> 00:30:39,667
А вот мы его решили не разбирать.

306
00:30:39,667 --> 00:30:46,067
И в нашей исторической реальности мы такого робота всячески пытаемся дальше усовершенствовать, который нам рассказывает.

307
00:30:46,067 --> 00:30:51,947
Дальше нас успокаивает и говорит, что всё будет хорошо, и максимально пытается сделать нас счастливыми не во благо — нам же.

308
00:30:51,975 --> 00:30:59,455
Что касается такой геополитической составной части, я думаю, Павел, государства дойдут до вашей мысли, если ещё не целиком дошли.

309
00:30:59,455 --> 00:31:04,255
Пока что они находятся на точке чуть подальше хотя бы, что уже ценно.

310
00:31:04,255 --> 00:31:08,355
Там уже идёт вопрос о ИИ-суверенитете, то есть что происходит.

311
00:31:08,395 --> 00:31:19,815
государства начали понимать, что если их граждане будут пользоваться разработками, например, американскими, то будет идти колоссальнейшая культурная манипуляция, ценностная манипуляция и так далее.

312
00:31:19,815 --> 00:31:30,822
То есть не так много, но уже появляются работы на эту тему, наверное, моё любимое, которые используют эту карту с труднопроизносимым названием Ингельхарта-Вельселя.

313
00:31:30,822 --> 00:31:37,082
которая наосяг выживание против общественного блага и элективность и индивидуальность.

314
00:31:37,082 --> 00:31:45,882
И там, в общем-то, показано, что все нынешние самые популярные сети, они так очень хорошо пластеризованы в смысле того, какие они дают ответы, как они себя ведут.

315
00:31:45,882 --> 00:31:49,262
Мы сейчас не говорим о том, что они там реально думают, а о том, какие они дают ответы.

316
00:31:49,262 --> 00:31:51,522
Такие протестантско-аналитические классы.

317
00:31:51,522 --> 00:32:08,147
И в этом смысле, если гражданин Индии спросит у чьего-то Джи-Пи-Ти, напиши мне историю про мальчика, Например, который приготовил завтрак, то мальчик будет готовить тосты, бекончик и шинко, а не чапати с гей со всем остальным.

318
00:32:08,147 --> 00:32:10,647
Вот этим сейчас очень сильно отзабочены государства.

319
00:32:10,647 --> 00:32:30,209
Поэтому, например, такие страны, как Дания, Израиль и так далее, уже несколько лет назад прямо очень сильно встрепенулись, то есть уже хотя бы поняли, начали быстро-быстро собирать данные, формировать команды в свои государства для формирования тех моделей, которые будут соответствовать уже каким-то их представлением, и даже некий успех там достигнут.

320
00:32:30,209 --> 00:32:37,429
Я целиком согласна с тем, что Вы, Павел, обозначили как сложность, щит в другой сфере.

321
00:32:37,429 --> 00:32:49,129
Я, честно говоря, кроме очень страшных исторических событий, не знаю примеров, когда людей реально что-то мотивировало резко перестать расслабляться, а начать собираться.

322
00:32:49,129 --> 00:33:12,802
Ментально от нас, по сути, требуется это для того, чтобы окончательно не потонуть в истории Здесь, к сожалению, в целом у нас нет союзников, потому что даже корпорации, которые… И это, кстати, такое тоже грустное осознание, например, когда люди говорят, ну вот я же программист или дизайнер, я в своей компании, могу столько всего теперь сделать.

323
00:33:12,802 --> 00:33:20,921
На самом деле компании, и это вам скажет любой продавец SaaS-продукта, Компания уже давно, наверное, лет 10 как ненавидит слово «продуктивность».

324
00:33:20,921 --> 00:33:25,881
И гидой продуктивности им продать какой-либо продукт очень сложно, потому что это тяжело измерить.

325
00:33:25,881 --> 00:33:30,641
Но если только вы не на заводе, тогда можете выпустить больше лампочек и так далее.

326
00:33:30,641 --> 00:33:37,430
Компаниям интересна целиком замена людей, потому что это является ключевым вот прямо таким… Качественный скачок.

327
00:33:37,430 --> 00:33:43,958
Всё, что до этого — это они пытаются не отстать, но ждут, когда уже можно будет щелкнуть пальцами.

328
00:33:43,958 --> 00:33:55,858
Несмотря на те заявления, которые делают кларные мира сего, что «мы столько-то людей уже сократили», или «Амазоны, которые говорят, а мы не наймём теперь тысячу кодеров, потому что у нас есть «И»».

329
00:33:55,858 --> 00:34:05,673
Я себе позволю такое деткое замечание, что это просто звучит лучше, что мы должны сократить количество денег на персонал, потому что то, что мы пытались, наши инвестиции не оправдались в других областях.

330
00:34:05,673 --> 00:34:12,367
Но на самом деле мечта бизнеса скорее движется в этом направлении, во всяком случае, большого бизнеса.

331
00:34:12,367 --> 00:34:13,167
Тут два варианта.

332
00:34:13,167 --> 00:34:25,067
По крайней мере, пока у нас не будет технологического коллапса с отключением электричества и прочего, мы приходим в точку, в которой единственная добавленная стоимость генерируется в зоне идеи творчества того самого.

333
00:34:25,073 --> 00:34:28,653
все материальное воплощение стремится к полной автоматизации.

334
00:34:28,653 --> 00:34:35,893
Все материю стремится отдать на аутсорс, а соответственно единственная в принципе добавленная стоимость может быть именно сгенерирована в самой идее.

335
00:34:35,893 --> 00:34:40,953
Идея нового приложения, идея нового продукта, идея чего-то и тому подобное.

336
00:34:40,953 --> 00:34:47,993
Но тут-то возникает неприятная особенность, которую мы все знаем, что вот к такому генерации новых идей мы не то чтобы сильно были научены.

337
00:34:48,343 --> 00:34:54,463
Большая часть работы от тебя вообще исторически не требовала ничего генерировать нового, особенно в индустриальном обществе, да?

338
00:34:54,463 --> 00:34:57,783
И поэтому в этом плане корпоративная логика мне тут очень понятна.

339
00:34:57,783 --> 00:35:08,423
Я просто хотел бы еще заострить, когда я говорил о вот этой гонке вооружений, для меня все-таки важно подчеркнуть, что на данный момент вот то, что вы описываете, оно укладывается в нормальную государственную логику.

340
00:35:08,423 --> 00:35:12,328
Они пока ищут технологическое решение технологической проблемы.

341
00:35:12,328 --> 00:35:36,072
Условно говоря, да, и проблема, но если мы создадим свой, если мы создадим гигантский прекрасный фейерволл, это будет наша проблема, и мы ее, так сказать, героически уже будем с ней работать, мы эту штуку оседлаем, а мой же тезис более радикальный И вот здесь я подчеркнул оригинальность для меня аиста, потому что В принципе, нет технологического решения этой проблемы в силу антропологического масштаба давления этой технологии.

342
00:35:36,072 --> 00:35:43,872
То есть, условно говоря, неважно, какой ИИ будет программировать мозги твоему населению, если оно будет сидеть по квартирам, и ты его не заставишь не сделать ничего.

343
00:35:43,872 --> 00:35:53,473
То есть, если наша цель — производить добавленную эту стоимость этим сверхтворческим субъектам, то тогда, получается, повторюсь, И решение в принципе не находится в этой зоне.

344
00:35:53,473 --> 00:36:00,373
Если мы говорим о, допустим, более прикладных вещах, у нас же возникает целый комплекс проблем, о которых мы не говорили, но прикладные.

345
00:36:00,373 --> 00:36:01,473
Когнитивная война.

346
00:36:01,473 --> 00:36:05,433
То есть СИИ — это уникальный пример, чтобы свести с ума население оппонента.

347
00:36:05,433 --> 00:36:06,533
То есть можно таргетировать.

348
00:36:06,533 --> 00:36:08,713
Что делать в обратную сторону, никто не понимает.

349
00:36:08,713 --> 00:36:11,113
Ну, когнитивная война — это такая моя побочная тема.

350
00:36:11,113 --> 00:36:12,128
Можем уйти сюда.

351
00:36:12,128 --> 00:36:14,708
Все знают эту историю с телефонными мошенниками.

352
00:36:14,708 --> 00:36:23,068
Только представьте, что это телефонный мошенник, который звонит реально голосом внучка, которого ты не отличишь, причем реально не отличишь.

353
00:36:23,068 --> 00:36:26,928
Это вопрос 5 лет, когда он сможет делать такие запросы.

354
00:36:26,928 --> 00:36:31,388
И тогда единственное, что тебя может спасти, это вопрос доверия, вопрос протокола.

355
00:36:31,388 --> 00:36:34,688
Решения принципиально не могут быть технологическими.

356
00:36:34,688 --> 00:36:38,288
В этом, мне кажется, то, что мы пока не осознали радикальности этого вызова.

357
00:36:38,638 --> 00:36:50,338
Я здесь могу сказать, что решение, конечно, можно сделать технологически, мы можем сделать, и который будет нас пинать и говорить «Так а ты сам-то подумал, прежде чем меня спрашивать?» Но этим никто не будет пользоваться.

358
00:36:50,434 --> 00:36:51,954
Интересно, что вы сказали.

359
00:36:51,954 --> 00:36:53,334
Никто не будет пользоваться.

360
00:36:53,334 --> 00:36:54,614
Это очень прикладной ответ.

361
00:36:54,614 --> 00:37:08,814
Но я скорее думаю, можем ли мы сделать ИИ, который будет блокировать на фоне подхода звонок из условного колл-центра через 15 айпи голосом внучка бабушки под Тамбовым, чтобы она принесла там 20 миллионов тому, что беспокоит ФСБ.

362
00:37:08,814 --> 00:37:11,074
Вот такой вы можете хотя бы представить?

363
00:37:11,241 --> 00:37:12,341
Конечно.

364
00:37:12,341 --> 00:37:18,081
Насколько я знаю, кстати, в России уже это решается просто немножко другим методом.

365
00:37:18,081 --> 00:37:29,281
Насколько я знаю, один из банков, они просто начинают разговор как секретарь с мошенниками и в ходе получения информации потихонечку начинают фильтровать всё больше и больше и больше количества.

366
00:37:29,281 --> 00:37:38,468
То есть я думаю, что здесь же знаете как, если вас вот кто-то лично таргетирует, И вот прям замучился сделать модели, и узнал детали, и прям вот всё взял.

367
00:37:38,468 --> 00:37:41,268
Тут, конечно, это как со взломом систем.

368
00:37:41,268 --> 00:37:45,988
Если кто-то именно вас хочет взломать, то, скорее всего, у него это вот.

369
00:37:45,988 --> 00:37:53,648
Если мы говорим про широкую сеть, которую обычно раскидывают мошенники, то здесь, я думаю, мы видим постепенные ответы.

370
00:37:53,698 --> 00:37:57,158
Ответ можно написать, но это же, знаете, как с соцсетями.

371
00:37:57,158 --> 00:38:12,918
Когда начались такие действительно большие кризисы в Фейсбуке, и многие оттуда начали уходить, сделали же столько альтернатив, которые пытались сделать, сказать, вот наши будут не токсичными соцсетями, делают блузка, и сами люди из Фейсбука пытались это всё делать.

372
00:38:12,918 --> 00:38:13,698
Не пользуются.

373
00:38:14,050 --> 00:38:16,630
По соцсетям там произошло более интересно.

374
00:38:16,630 --> 00:38:22,330
У нас произошло падение Фейсбука, Фейсбук реально пал, но при этом не возникло прямой альтернативы Фейсбуку.

375
00:38:22,330 --> 00:38:25,810
То есть мы увидели дальнейшую фрагментацию сетевого пространства.

376
00:38:25,810 --> 00:38:26,490
То есть условно...

377
00:38:26,490 --> 00:38:36,770
То есть ни один из тех, что возник после Фейсбука, не стал сам аналогичен Фейсбуку, но их общий потенциал, ну, примерно остался, скажем так, стал сравнимым.

378
00:38:36,770 --> 00:38:39,610
Ну и условно множество людей из Фейсбука в Сапстэк ушло.

379
00:38:39,610 --> 00:38:42,630
И вообще стали вводить разную систему протоколов.

380
00:38:42,957 --> 00:38:46,877
Я хотел бы потихонечку нас подвести к какому-то заключению.

381
00:38:46,877 --> 00:38:51,277
Мы начали с того, что я задал специально упрощённый вопрос.

382
00:38:51,277 --> 00:38:53,517
Так что же такое искусственный интеллект?

383
00:38:53,517 --> 00:38:56,997
Удобный инструмент или скрытая угроза?

384
00:38:56,997 --> 00:38:58,597
А может быть, и то, и другое?

385
00:38:58,597 --> 00:39:04,097
Из того, что прозвучало, я могу сделать несколько выводов, попробовать сделать несколько выводов.

386
00:39:04,097 --> 00:39:10,397
И я попрошу сначала Алису, а потом Павла откорректировать или подтвердить мои умозаключения.

387
00:39:10,412 --> 00:39:11,452
Первое.

388
00:39:11,452 --> 00:39:29,392
Сами создатели технологии и те, кто её развивают, до конца не представляют, с чем они имеют дело, не вполне осознают последствия, а скорее пытаются свои пожелания, свои идеальные стремления вербализовать как то, что они знают наверняка.

389
00:39:29,481 --> 00:39:37,701
Вообще говоря, мы имеем дело с технологией в руках людей, которые не до конца понимают ни как она работает, ни что с ней делать — это первое.

390
00:39:37,701 --> 00:39:38,521
Второе.

391
00:39:38,521 --> 00:39:56,875
В силу того, как устроено современное общество и как это общество привыкло реагировать на новые технологии, не стоит ожидать, что появится общепринятый подход к решению задачи, как противостоять рискам или как управлять рисками, связанными с искусством и интеллектом.

392
00:39:56,875 --> 00:40:08,309
Поскольку, как Павел очень верно и здорово заметил, в данном случае технологии, можно сказать, дошли до того уровня предела, когда противостоять этой технологии… А зачем противостоять?

393
00:40:08,309 --> 00:40:12,929
Для того чтобы не утратить, собственно, какую-то свою Личность, субъектность.

394
00:40:12,929 --> 00:40:26,569
Вот как раз только увеличивая субъектность, можно противостоять этой негативным последствиям внедрения искусственного интеллекта, И это, как уже, наверное, третий пункт, самое позитивное, что есть в технологии искусственного интеллекта.

395
00:40:26,569 --> 00:40:33,349
Здесь у нас не остаётся выбора — либо быть человеком, либо раствориться в коконе из помощников.

396
00:40:33,437 --> 00:40:37,797
Я не могу сказать, что в компаниях сидят люди, которые совсем не понимают, что они делают.

397
00:40:37,797 --> 00:40:42,077
Реально физически возможных пределов того, что можно понимать, понимают.

398
00:40:42,077 --> 00:40:46,237
Какая-то математика нам ещё не совсем понятна, но, наверное, мы её поймём.

399
00:40:46,237 --> 00:40:59,277
Другое дело, что те люди, которые в этом разбираются действительно хорошо, довольно часто выходят на конференции и произносят спички про то, что «ребята, нам нужно ответственно, нам нужно вот так вот делать».

400
00:40:59,277 --> 00:41:01,997
Мне всегда хочется спросить, к кому вы обращаетесь?

401
00:41:02,496 --> 00:41:06,116
Не люди, которые восслушаются, могут что-то с этим сделать.

402
00:41:06,116 --> 00:41:07,596
А вдруг оно выйдет из-под контроля?

403
00:41:07,596 --> 00:41:08,596
Что значит «вдруг»?

404
00:41:08,596 --> 00:41:09,676
Что выйдет?

405
00:41:09,676 --> 00:41:11,516
Мы тут все работаем на это.

406
00:41:11,516 --> 00:41:52,708
Другое дело, что я думаю, что люди, которые обладают более глубоким пониманием, у них, судя по каким-то совсем небольшому количеству информации, которую я получаю, То есть даже те, у кого, я думаю, есть какой-то и наработанный философский инструментарий, ценностный инструментарий, я думаю, что именно нереально этот в таком окружении, инструментарий, в такой этот инструментарий, это инструментарий, это инструментарий, это инструментарий, инструментарий.

407
00:41:52,708 --> 00:41:56,759
ситуации, с такими задачами действительно сохранять холодную голову.

408
00:41:56,759 --> 00:42:01,799
То есть я предполагаю, что они, наверное, даже пытаются сделать что-то максимально хорошее.

409
00:42:01,799 --> 00:42:03,379
Выходит, как всегда.

410
00:42:03,379 --> 00:42:11,091
Ян Лекун, которого я люблю слушать, потому что он уже, в общем-то, вышел из такой большой корпоративной игры, поэтому может себе позволить сказать гораздо больше.

411
00:42:11,091 --> 00:42:19,902
Он, правда, отвечал на вопрос, что сделать, чтобы я и нас не уничтожил, но, на мой взгляд, может быть, это и можно применить в какую-то такую более хорошую среду.

412
00:42:19,902 --> 00:42:32,042
Он тут говорит, слушайте, если мы говорим про ситуацию, когда у нас есть более глупое существо, человек, и более умное существо, там вот AGI, которое мы вдруг можем создать, не дай Бог, вдруг.

413
00:42:32,170 --> 00:42:38,490
в этот день, в этот час он появится у этой команды, то давайте мы сделаем ценности материнские.

414
00:42:38,490 --> 00:42:50,817
Потому что мы знаем только один биологический случай, когда более умное существо готово щадить и как-то воспитывать, и давать какой-то конструктивный фидбэк более глупому существу.

415
00:42:50,817 --> 00:42:55,837
Но он это говорил с точки зрения, чтобы не уничтожило, а с нашей стороны.

416
00:42:55,837 --> 00:43:08,657
Если сделать яй достаточно занудным, неприятным, и это сделать как абсолютную необходимость для всех инструментов, может быть, как-то удастся допинать нас до чуть более лучшей ситуации.

417
00:43:08,657 --> 00:43:10,617
Но можем, но не будем.

418
00:43:10,645 --> 00:43:22,025
Поэтому я думаю, что мы тут уже действительно в такой очень нехорошей ситуации, с которой нас может вытащить только, скорее всего, что-то очень радикальное, что прямо встряхнёт нас.

419
00:43:22,025 --> 00:43:23,825
Буквально два коротких комментария.

420
00:43:23,825 --> 00:43:24,165
Первый.

421
00:43:24,165 --> 00:43:28,225
Вспоминаем детскую сказку, которая не очень детская — «Алиса в стране чудес».

422
00:43:28,225 --> 00:43:31,585
И иногда, чтобы стоять на месте, нужно бежать в два раза быстрее.

423
00:43:31,585 --> 00:44:12,962
Вот на фоне развития наших вот этих магических артефактов, Теперь однозначно буду называть такими методами Бежать нужно даже диф-2, может быть, по экспоненте быстрее И в этом я вижу оптимистическую провиденческую работу А то мы слишком долго упрятались за разными технологическими, скажем так, идолами Считая себя их господами Ну, наконец-то мы создали голема или создаём в процессе, под прекрасные лозунги, с прекрасными технологическими возможностями, которые вопрос, собственно, нашей, прежде всего, на самом деле, этической субъектности, уже во вторую уровень и когнитивно-эмоциональную субъектность ставят перед тем самым большим-большим зеркалом, которое нам и должно показать.

424
00:44:12,962 --> 00:44:18,082
Первое — то, что мы-то уже сами по себе почти очень голенькие стали за последние лет 400.

425
00:44:18,111 --> 00:44:18,991
Тут два варианта.

426
00:44:18,991 --> 00:44:26,911
Либо уж совсем залезать в эту капсулку, как в фильме «Матрица», либо всё-таки уже искать одежду, основания, тренировать себя.

427
00:44:26,911 --> 00:44:32,271
То есть это то самое внутреннее усилие подменять своё самоволие настоящим самовластием.

428
00:44:32,271 --> 00:44:34,971
То есть вот такого вот спрятаться не удастся.

429
00:44:34,971 --> 00:44:36,971
Такой для себя вывод, который я сделал.

430
00:44:37,315 --> 00:44:39,095
Отличное завершение.

431
00:44:39,095 --> 00:44:40,595
Спасибо, Алиса.

432
00:44:40,595 --> 00:44:42,435
Прежде всего, спасибо, Павел.

433
00:44:42,435 --> 00:44:49,035
Я думаю, что мы, по крайней мере, постараемся ещё вернуться к этой теме вместе с Алисой и Павлом.

434
00:44:49,035 --> 00:44:53,093
Ну а для тех, кто слушает, пожалуйста, оставляйте свои комментарии.

435
00:44:53,093 --> 00:45:09,133
Дополняйте нас, мы будем рады услышать мнение тех людей, особенно кто находится в этой индустрии и сможет рассказать нам о том, что мы не знаем, например, или поделиться какими-то своими размышлениями, которые нас дополнят.

436
00:45:09,133 --> 00:45:10,573
Всем ещё раз большое спасибо!

437
00:45:10,573 --> 00:45:11,793
Ну и до встречи!

438
00:45:11,793 --> 00:45:12,273
До встречи!
